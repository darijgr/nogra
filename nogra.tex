\documentclass[numbers=enddot,12pt,final,onecolumn,notitlepage]{scrartcl}%
\usepackage[headsepline,footsepline,manualmark]{scrlayer-scrpage}
\usepackage[all,cmtip]{xy}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{framed}
\usepackage{comment}
\usepackage{color}
\usepackage{hyperref}
\usepackage{ifthen}
\usepackage[en-GB,showdow]{datetime2}
\DTMlangsetup[en-GB]{ord=raise,monthyearsep={,\space}}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{tikz}
\usepackage{needspace}
\usepackage{tabls}
%TCIDATA{OutputFilter=latex2.dll}
%TCIDATA{Version=5.50.0.2960}
%TCIDATA{LastRevised=Tuesday, December 13, 2016 21:43:59}
%TCIDATA{SuppressPackageManagement}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%TCIDATA{Language=American English}
%BeginMSIPreambleData
\providecommand{\U}[1]{\protect\rule{.1in}{.1in}}
%EndMSIPreambleData
\usetikzlibrary{arrows.meta}
\usetikzlibrary{chains}
\theoremstyle{definition}
\newtheorem{theo}{Theorem}[subsection]
\newenvironment{theorem}[1][]
{\begin{theo}[#1]\begin{leftbar}}
{\end{leftbar}\end{theo}}
\providecommand*\theoremautorefname{Theorem}
\newtheorem{lem}[theo]{Lemma}
\newenvironment{lemma}[1][]
{\begin{lem}[#1]\begin{leftbar}}
{\end{leftbar}\end{lem}}
\providecommand*\lemmaautorefname{Lemma}
\newtheorem{prop}[theo]{Proposition}
\newenvironment{proposition}[1][]
{\begin{prop}[#1]\begin{leftbar}}
{\end{leftbar}\end{prop}}
\providecommand*\propositionautorefname{Proposition}
\newtheorem{defi}[theo]{Definition}
\newenvironment{definition}[1][]
{\begin{defi}[#1]\begin{leftbar}}
{\end{leftbar}\end{defi}}
\providecommand*\definitionautorefname{Definition}
\newtheorem{remk}[theo]{Remark}
\newenvironment{remark}[1][]
{\begin{remk}[#1]\begin{leftbar}}
{\end{leftbar}\end{remk}}
\providecommand*\remarkautorefname{Remark}
\newtheorem{coro}[theo]{Corollary}
\newenvironment{corollary}[1][]
{\begin{coro}[#1]\begin{leftbar}}
{\end{leftbar}\end{coro}}
\providecommand*\corollaryautorefname{Corollary}
\newtheorem{conv}[theo]{Convention}
\newenvironment{condition}[1][]
{\begin{conv}[#1]\begin{leftbar}}
{\end{leftbar}\end{conv}}
\providecommand*\conditionautorefname{Convention}
\newtheorem{alg}[theo]{Algorithm}
\newenvironment{algorithm}[1][]
{\begin{alg}[#1]\begin{leftbar}}
{\end{leftbar}\end{alg}}
\providecommand*\algorithmautorefname{Algorithm}
\newtheorem{warn}[theo]{Warning}
\newenvironment{conclusion}[1][]
{\begin{warn}[#1]\begin{leftbar}}
{\end{leftbar}\end{warn}}
\providecommand*\conclusionautorefname{Warning}
\newtheorem{conj}[theo]{Conjecture}
\newenvironment{conjecture}[1][]
{\begin{conj}[#1]\begin{leftbar}}
{\end{leftbar}\end{conj}}
\providecommand*\conjectureautorefname{Conjecture}
\newtheorem{exam}[theo]{Example}
\newenvironment{example}[1][]
{\begin{exam}[#1]\begin{leftbar}}
{\end{leftbar}\end{exam}}
\providecommand*\exampleautorefname{Example}
\newtheorem{exmp}[theo]{Exercise}
\newenvironment{exercise}[1][]
{\begin{exmp}[#1]\begin{leftbar}}
{\end{leftbar}\end{exmp}}
\providecommand*\exerciseautorefname{Exercise}
\newenvironment{statement}{\begin{quote}}{\end{quote}}
\newtheorem{quest}[theo]{TODO}
\newenvironment{todo}[1][]
{\begin{quest}[#1]\begin{leftbar}}
{\end{leftbar}\end{quest}}
\iffalse
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\fi
\let\sumnonlimits\sum
\let\prodnonlimits\prod
\let\cupnonlimits\bigcup
\let\capnonlimits\bigcap
\renewcommand{\sum}{\sumnonlimits\limits}
\renewcommand{\prod}{\prodnonlimits\limits}
\renewcommand{\bigcup}{\cupnonlimits\limits}
\renewcommand{\bigcap}{\capnonlimits\limits}
\newcommand\arxiv[1]{\href{http://www.arxiv.org/abs/#1}{\texttt{arXiv:#1}}}
% A command for citing arXiv preprints.
% Example syntax: \arxiv{1009.4134v2} and \arxiv{math/0602634v4}.

\setlength\tablinesep{3pt}
\setlength\arraylinesep{3pt}
\setlength\extrarulesep{3pt}
\voffset=0cm
\hoffset=-0.7cm
\setlength\textheight{22.5cm}
\setlength\textwidth{15.5cm}
\newenvironment{verlong}{}{}
\newenvironment{vershort}{}{}
\newenvironment{noncompile}{}{}
\excludecomment{verlong}
\includecomment{vershort}
\excludecomment{noncompile}
\newcommand{\id}{\operatorname{id}}
\newcommand{\rev}{\operatorname{rev}}
\newcommand{\conncomp}{\operatorname{conncomp}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\powset}[2][]{\ifthenelse{\equal{#2}{}}{\mathcal{P}\left(#1\right)}{\mathcal{P}_{#1}\left(#2\right)}}
% $\powset[k]{S}$ stands for the set of all $k$-element subsets of
% $S$. The argument $k$ is optional, and if not provided, the result
% is the whole powerset of $S$.
\newcommand{\set}[1]{\left\{ #1 \right\}}
% $\set{...}$ yields $\left\{ ... \right\}$.
\newcommand{\abs}[1]{\left| #1 \right|}
% $\abs{...}$ yields $\left| ... \right|$.
\newcommand{\tup}[1]{\left( #1 \right)}
% $\tup{...}$ yields $\left( ... \right)$.
\newcommand{\ive}[1]{\left[ #1 \right]}
% $\ive{...}$ yields $\left[ ... \right]$.
\newcommand{\verts}[1]{\operatorname{V}\left( #1 \right)}
% $\verts{...}$ yields $\operatorname{V}\left( ... \right)$.
\newcommand{\edges}[1]{\operatorname{E}\left( #1 \right)}
% $\edges{...}$ yields $\operatorname{E}\left( ... \right)$.
\newcommand{\arcs}[1]{\operatorname{A}\left( #1 \right)}
% $\arcs{...}$ yields $\operatorname{A}\left( ... \right)$.
\newcommand{\underbrack}[2]{\underbrace{#1}_{\substack{#2}}}
% $\underbrack{...1}{...2}$ yields
% $\underbrace{...1}_{\substack{...2}}$. This is useful for doing
% local rewriting transformations on mathematical expressions with
% justifications.
\newcommand{\are}{\ar@{-}}
% In an xymatrix environment, $\are$ gives an arrow without
% arrowhead. I use this to represent edges in graphs.
\ihead{Notes on graph theory (\today, \DTMcurrenttime)}
\ohead{page \thepage}
\cfoot{}
\begin{document}

\title{Notes on graph theory}
\author{Darij Grinberg}
\date{
%TCIMACRO{\TeXButton{today}{\today} }%
%BeginExpansion
\today\ 
%EndExpansion
at \DTMcurrenttime\ 
(unfinished draft!)\\
These notes are \textbf{frozen} in a (very) unfinished state. Currently, only
two chapters (beyond the preface) exist, and they too are incomplete
(although hopefully readable).}
\maketitle
\tableofcontents

\section{Preface}

These are lecture notes on graph theory -- the part of mathematics
involved with graphs. They are currently work in
progress (but the parts that are finished are self-contained!);
once finished, they should contain a semester's worth of
material. I have tried to keep the presentation as self-contained and
elementary as possible; the reader is nevertheless assumed to possess
some mathematical maturity (in particular, to know how to read
combinatorial proofs, filling in simple details)\footnote{I believe
that the MIT lecture notes \cite{LeLeMe16} are a good source for
achieving this mathematical maturity. (Actually, there is some
intersection between \cite[Chapters 10 and 12]{LeLeMe16} and our
notes, but \cite{LeLeMe16} mostly keeps to the basics of graph
theory.)
Two other resources to familiarize oneself with proofs are
\cite{Hammac15} and \cite{Day-proofs}.
Generally, most good books about ``reading and
writing mathematics'' or ``introductions to abstract mathematics''
should convey these skills, although the extent to which they actually
do so may differ.}
and know how to work
with modular arithmetic (i.e., congruences between integers modulo a
positive integer $n$) and summation signs (such as $\sum_{i=1}^n$ and
$\sum_{a \in A}$, where $n$ is an integer and $A$ is a finite
set)\footnote{See \cite[\S 1.4]{detnotes} for a list of important
properties of summation signs.}.
In some chapters, familiarity with matrices (and their products),
permutations (and their signs)\footnote{A summary of the most
fundamental results about signs of permutations can be found in
\cite[\S 8.1]{LaNaSc16}. These results appear with proofs in
\cite[Chapter 6.B]{Day-proofs}. For an even more detailed treatment
(also including proofs),
see \cite[\S 5.1--5.3]{detnotes}. Some of the most important
properties of signs of permutations are also proven in
\cite[Appendix B]{Strick13}, which also gives one of the best
introductions to determinants I have seen in the literature.
Another treatment can be found in
\cite{Conrad-sign}, but this requires some familiarity with group
theory.}
and polynomials will be required.
I hope that the proofs are readable; feel free to contact me (at
\texttt{darijgrinberg@gmail.com}) for any clarifications.

The choice of material surveyed in these notes is idiosyncratic
(sometimes even purposefully trying to wander some seldom trodden
paths). Some standard material (Eulerian walks, Hamiltonian paths,
trees, bipartite matching theory, network flows) is present (at least
in the eventual final form of these notes), whereas
other popular topics (planar graphs, random graphs, adjacency matrices
and spectral graph theory) are missing. Some of these omissions have
specific reasons (e.g., many of the omitted topics would make it much
harder to keep the notes self-contained), whereas others are merely
due to my tastes and lack of time.
% [The following is probably far too optimistic;]
% I am trying to give an elementary
% introduction into the (rather new) theory of sandpiles (also known as
% chip-firing) as well as two applications of combinatorics to linear
% algebra (viz., Gessel's proof of the Vandermonde determinant
% \cite{Gessel-Vand}, and some of the applications of matchings to
% Pfaffians); I also intend to include some properties of Hamiltonian
% paths not commonly exposed in textbooks.

These notes are accompanying
\href{http://www.cip.ifi.lmu.de/~grinberg/t/17s}{a
class on graph theory (Math 5707) I am giving at the University of
Minneapolis in Spring 2017}. They contain both the
material of the class (although with no promise of timeliness!) and the
homework exercises (and possibly some additional exercises).
Sections marked with an asterisk (*) are not part of the Math 5707
course.

Various other texts on graph theory are \cite{Bollob79},
\cite{Bollob98}, \cite{Harary69}, \cite{Harju14}, \cite{Balakr97},
\cite{Jungni13}, \cite{Martin16}, \cite{ThuSwa92},
\cite{BonMur76}, \cite{Ore74}, \cite{BehCha71}, \cite{BeChZh15},
\cite{BonMur08}, \cite{Ruohon13}, \cite{Dieste16}, \cite{Ore90},
\cite{HaHiMo08}, \cite{Berge91}, \cite{ChaLes15}, \cite{Griffi15},
\cite{Wilson96}.
(This is a haphazard list; I have barely touched most of these texts.)
Also, texts on combinatorics and on discrete mathematics (such as
\cite{BenWil12}, \cite{KelTro15}, \cite{PoTaWo83}, \cite{Bona11},
\cite{Guicha16},
or the introductory \cite{LoPeVe03}) often contain
sections on graph theory, since graph theory is considered to be part
of both.
Material on graph theory can also be found in large quantities on
mathematical contests for students (such as the International
Mathematical Olympiad) and, consequently, in collections of problems
from these contests, such as the AoPS collection of IMO Shortlist
problems \cite{AoPS-ISL}.
Finally, some elementary results in graph theory double as puzzles
(or are related to puzzles), which often has the consequence that they
appear on puzzle websites such as Cut-the-Knot \cite{cut-the-knot}.

The notes you are reading are under construction, and will remain so for at
least the whole Spring of 2017. Please let me know of any errors and
unclarities you encounter (my email address is \texttt{darijgrinberg@gmail.com}%
)\footnote{The sourcecode of the notes is also publicly available at
\url{https://github.com/darijgr/nogra} .}. Thank you!

\subsection{Acknowledgments}

Thanks to Victor Reiner and Travis Scrimshaw for helpful
conversations.

[Your name could be in here!]

\section{\label{chp.intro}Introduction}

In this chapter, we shall define a first notion of graphs (``first''
because there are several others to follow) and various basic notions
related to it, and prove some elementary properties thereof. This
chapter is meant to give a taste of the whole theory, although (not
unexpectedly) it is not a representative sample.

\subsection{\label{sect.intro.notations}Notations and conventions}

Before we get to anything interesting, let me get some technicalities
out of the way. Namely, I shall be using the following notations:

\begin{itemize}
\item In the following, we use the symbol $\NN$ to denote the set
$\left\{ 0, 1, 2, \ldots \right\}$. (Be warned that some other authors
use this symbol for $\left\{ 1, 2, 3, \ldots \right\}$ instead.)

\item We let $\QQ$ denote the set of all rational numbers; we let
$\RR$ be the set of all real numbers.

\item If $X$ and $Y$ are two sets, then we shall use the notation
``$X\rightarrow Y,\ x\mapsto E$'' (where $x$
is some symbol which has no specific meaning in the current context,
and where $E$ is some expression which usually involves $x$) for ``the
map from $X$ to $Y$ that sends every $x\in X$ to $E$''. For
example, ``$\NN\rightarrow\NN,\ x\mapsto x^{2}+x+6$'' means the map
from $\NN$ to $\NN$ that sends every $x\in\NN$ to $x^{2}+x+6$. For
another example, ``$\NN\rightarrow\QQ,\ x\mapsto\dfrac{x}{1+x}$''
denotes the map from $\NN$ to $\QQ$ that
sends every $x\in\NN$ to $\dfrac{x}{1+x}$.\ \ \ \ \footnote{A word of
warning: Of course, the notation ``$X\rightarrow Y,\ x\mapsto E$''
does not always make sense; indeed, the map that it
stands for might sometimes not exist. For instance, the notation
``$\NN\rightarrow\QQ,\ x\mapsto\dfrac{x}{1-x}$'' does not actually
define a map, because the map that it
is supposed to define (i.e., the map from $\NN$ to $\QQ$ that
sends every $x\in\NN$ to $\dfrac{x}{1-x}$) does not exist (since
$\dfrac{x}{1-x}$ is not defined for $x=1$). For another example, the
notation ``$\NN\rightarrow\ZZ,\ x\mapsto\dfrac{x}{1+x}$'' does not
define a map, because the map that it is
supposed to define (i.e., the map from $\NN$ to $\ZZ$ that
sends every $x\in\NN$ to $\dfrac{x}{1+x}$) does not exist (for $x=2$,
we have $\dfrac{x}{1+x}=\dfrac{2}{1+2}\notin\ZZ$, which shows that a
map from $\NN$ to $\ZZ$ cannot send this $x$ to this $\dfrac
{x}{1+x}$). Thus, when defining a map from $X$ to $Y$ (using whatever
notation), do not forget to check that it is well-defined (i.e., that
your definition specifies precisely one image for each $x\in X$, and
that these images all lie in $Y$). In many cases, this is obvious or
very easy to check (I will usually not even mention this check), but
in some cases, this is a difficult task.}

\item If $S$ is a set, then the \textit{powerset} of $S$ means the set
of all subsets of $S$. This powerset will be denoted by
$\powset{S}$. % This expands to $\mathcal{P} \left( S \right)$.
For example, the powerset of $\left\{  1,2\right\}  $ is
$\powset{ \left\{  1,2\right\} }
=\left\{  \varnothing,\left\{
1\right\}  ,\left\{  2\right\}  ,\left\{  1,2\right\}  \right\}  $.

Furthermore, if $S$ is a set and $k$ is an integer, then
$\powset[k]{S}$ % This expands to $\mathcal{P}_k\left(S\right)$.
shall mean the set of all $k$-element subsets of $S$. (This is empty
if $k < 0$.)
\end{itemize}

\subsection{\label{sect.intro.simple}Simple graphs}

As already hinted above, there is not one single concept of a
``graph''. Instead, there are several mutually related (but not
equivalent) concepts of ``graph'', which are often kept apart by
adorning them with adjectives (e.g., ``simple graph'', ``directed
graph'', ``loopless graph'', ``loopless weighted undirected
graph'', ``infinite graph'', etc.) or prefixes (``digraph'',
``multigraph'', etc.). Let me first define the simplest one of these:

\begin{definition} \label{def.intro.simple.sg}
A \textit{simple graph} is a pair $\tup{V, E}$, where $V$ is a
finite set, and where $E$ is a subset of $\powset[2]{V}$.
\end{definition}

Let us unpack this definition first. The word ``simple'' in
``simple graph'' (roughly speaking) has the meaning of ``with no
bells and whistles''; i.e., it says that the notion of
``simple graph'' is one of the crudest, most primitive notions of
a graph known.
% i.e., no additional structure.
It does not mean
that everything that can be said about simple graphs is simple (this
is far from the case, as we will see below). The condition
``$E$ is a subset of $\powset[2]{V}$'' in
Definition~\ref{def.intro.simple.sg} can be rewritten as ``$E$ is a
set of $2$-element subsets of $V$'' (since $\powset[2]{V}$ is the set
of all $2$-element subsets of $V$). Thus, a simple graph is a pair
consisting of a finite set $V$, and a set of $2$-element subsets of
$V$.
For example,
$\left(\set{1,2,3}, \set{\set{1,3}, \set{3,2}} \right)$ and
$\left(\set{2,5}, \set{\set{2,5}}\right)$ and
$\left(\varnothing, \varnothing\right)$ are three simple graphs.

\begin{conclusion}
\textbf{(a)}
Our Definition~\ref{def.intro.simple.sg} differs from the definition
of a ``simple graph'' in many sources, in that we are requiring $V$ to
be finite.

\textbf{(b)}
Simple graphs are often just called ``graphs''. But then again, some
other concepts of graphs (such as multigraphs, which we will
encounter further below) are also often just called ``graphs''. Thus,
the precise meaning of the word ``graph'' depends on the context in
which it appears. For example, Bollob\'as (in \cite{Bollob79}) uses
the word ``graph'' for ``simple graph'', whereas Bondy and Murty
(in \cite{BonMur08}) use it for ``multigraph with loops allowed''
(a concept we will define further below).
When reading literature, always check the
definitions (and, if these are missing, try to take an educated guess,
ruling out options that make some of the claims false).
\end{conclusion}

So far, we have not explained how we should intuitively think of
simple graphs, and why they are interesting. We will spend a
significant part of these notes answering the latter question; but let
us first comment on the former.

Simple graphs can be used to model symmetric relations between
different objects. For example, if you have $n$ integers
(for some $n \in \NN$), then you can define a graph
$\tup{V, E}$ for which $V$ is the set of these $n$ integers,
and $E$ is the set of all $2$-element subsets
$\set{u, v}$ of $V$ for which $\abs{u-v} \leq 3$. (Notice that
$\set{u, u}$ does not count as a $2$-element subset.)
For a non-mathematical example, consider an (idealized) group $P$ of
(finitely many) people, each two of which are either mutual friends or
not\footnote{We assume that if a person $u$ is a friend of a person
$v$, then $v$ is a friend of $u$. We also assume that no person $u$ is
a friend of $u$ itself (or, at least, we don't count this as
friendship).}. Then, you can define a graph $\tup{P, E}$, where $E$
is the set of all $2$-element subsets $\set{u, v}$ of $P$ for which
$u$ and $v$ are mutual friends. This graph then models the friendships
between the people in the group $P$; in a sense, it is a social
network (similar to the ones widespread on the Internet, but much more
rudimentary, since it only knows who is a friend of
whom).\footnote{This kind of ``social graphs'' has been used for many
years as a language for stating theorems about graphs without saying
the word ``graph'' (and without using mathematical notation): Just
speak of people and their mutual friendships. This language was in use
long before the Internet and actual social networks came about.}

The following notations provide a quick way to reference the elements
of $V$ and $E$ when given a graph $\tup{V, E}$:

\begin{definition} \label{def.intro.simple.VE}
Let $G = \tup{V, E}$ be a simple graph.

\begin{enumerate}

\item[\textbf{(a)}]
The set $V$ is called the \textit{vertex set} of $G$;
it is denoted by $\verts{G}$. (Notice that the letter
``$\operatorname{V}$'' in ``$\verts{G}$'' is upright, as opposed to
the letter ``$V$'' in ``$\tup{V, E}$'', which is italic.
These are two different symbols, and have different meanings: The
letter $V$ stands for the specific set $V$ which is the first
component of the pair $G$, whereas the letter
$\operatorname{V}$ is part of the notation $\verts{G}$ for the
vertex set of any graph. Thus, if $H = \left(W, F\right)$ is another
graph, then $\verts{H}$ is $W$, not $V$.)

The elements of $V$ are called the \textit{vertices} (or the
\textit{nodes}) of $G$.

\item[\textbf{(b)}]
The set $E$ is called the \textit{edge set} of $G$; it
is denoted by $\edges{G}$. (Again, the letter ``$\operatorname{E}$''
in ``$\edges{G}$'' is upright, and stands for a different thing than
the ``$E$''.)

The elements of $E$ are called the \textit{edges} of $G$. When $u$ and
$v$ are two elements of $V$, we shall often use the notation $uv$ for
$\set{u, v}$; thus, each edge of $G$ has the form $uv$ for two
distinct elements $u$ and $v$ of $V$. Of course, we always have
$uv = vu$.

Notice that each simple graph $G$ satisfies
$G = \tup{\verts{G}, \edges{G}}$.

\item[\textbf{(c)}] Two vertices $u$ and $v$ of $G$ are said to be
\textit{adjacent} (or \textit{connected by an edge}) if $uv \in E$
(that is, if $uv$ is an edge of $G$). In this case, the edge $uv$ is
said to \textit{connect} $u$ and $v$; the vertices $u$ and $v$ are
called the \textit{endpoints} of this edge. When the graph $G$ is not
obvious from the context, we shall often say ``adjacent in $G$''
instead of just ``adjacent''.

Two vertices $u$ and $v$ of $G$
are said to be \textit{non-adjacent} if they are not adjacent (i.e.,
if $uv \notin E$).

We say that a vertex $u$ of $G$ is \textit{adjacent to} a vertex $v$
of $G$ if the vertices $u$ and $v$ are adjacent (i.e., if $uv \in E$).
Similarly, we say that a vertex $u$ of $G$ is \textit{non-adjacent to}
a vertex $v$ of $G$ if the vertices $u$ and $v$ are non-adjacent
(i.e., if $uv \notin E$).

\item[\textbf{(d)}] Let $v$ be a vertex of $G$ (that is, $v \in V$).
Then, the \textit{neighbors} of $v$ are the vertices $u$ of $G$ that
satisfy $vu \in E$.
In other words, the \textit{neighbors} of $v$ are the vertices of
$G$ that are adjacent to $v$.
(Of course, these neighbors depend on both $v$ and
$G$. When $G$ is not clear from the context, we shall call them the
``neighbors of $v$ in $G$'' instead of just ``neighbors of $v$''.)

\end{enumerate}

\end{definition}

Of course, the relation of adjacency is symmetric\footnote{This means
the following: Given two vertices $u$ and $v$ of a simple graph $G$,
the vertex $u$ is adjacent to $v$ if and only if $v$ is adjacent to
$u$.}. The same holds for the relation of non-adjacency.

\begin{example} \label{exa.intro.simple.VE}
Let $U$ be the $5$-element set $\set{1,2,3,4,5}$. Let $E$ be the
subset $\set{\set{u,v} \in \powset[2]{U} \ \mid \ u + v \geq 5 }$
of $\powset[2]{U}$. This set $E$ is well-defined, because the sum
$u + v$ of two integers $u$ and $v$ depends only on the set
$\set{u,v}$ and not on how this set is written (since
$u + v = v + u$). (This is important, because if we had used the
condition $u - v \geq 3$ instead of $u + v \geq 5$, then the set $E$
would not be well-defined, because it would not be clear whether
$\set{1, 5}$ should be inside it or not -- indeed, if we write
$\set{1, 5}$ as $\set{u, v}$ with $u = 5$ and $v = 1$, then
$u - v \geq 3$ is satisfied, but if we write $\set{1, 5}$ as
$\set{u, v}$ with $u = 1$ and $v = 5$, then $u - v \geq 3$ is not
satisfied.)

Let $G$ be the simple graph $\tup{U, E}$. Then, $\verts{G} = U
= \set{1,2,3,4,5}$ and
\begin{align}
\edges{G} &= E
= \set{\set{u,v} \in \powset[2]{U} \ \mid \ u + v \geq 5 }
\nonumber \\
&= \set{\set{1,4}, \set{1,5},
        \set{2,3}, \set{2,4}, \set{2,5},
        \set{3,4}, \set{3,5},
        \set{4,5}} .
\label{eq.exa.intro.simple.VE.edgesG=}
\end{align}
Thus, $G$ has $\abs{\verts{G}} = \abs{U} = 5$ vertices and
$\abs{\edges{G}} = \abs{E} = 8$ edges.

Using the shorthand notation
$uv$ for $\set{u, v}$ (introduced in
Definition~\ref{def.intro.simple.VE} \textbf{(b)}), the equality
\eqref{eq.exa.intro.simple.VE.edgesG=} rewrites as
\[
\edges{G}
= \set{14, 15, 23, 24, 25, 34, 35, 45} .
\]

The vertices $2$ and $4$ of $G$ are adjacent (since $24 \in E$).
In other words, $4$ is a neighbor of $2$. Equivalently, $2$ is a
neighbor of $4$. On the other hand, the vertices $1$ and $3$ of $G$
are not adjacent (since $13 \notin E$); thus, $1$ is not a neighbor
of $3$. The neighbors of $1$ are $4$ and $5$.
\end{example}

\subsection{\label{sect.intro.draw}Drawing graphs}

There is a common method to represent graphs visually: Namely, a graph
can be drawn as a set of points in the plane and a set of curves
connecting some of these points with each other.

More precisely:

\begin{definition} \label{def.intro.draw}
A simple graph $G$ can be visually represented by
\textit{drawing} it on the plane (or on some other surface).
To do so, we represent each vertex of $G$ by a point (at which we
put the name of the vertex),
and then, for each edge $uv$ of $G$, we draw a
curve that connects the point representing $u$ with the point
representing $v$. The positions of the points and the shapes of the
curves can be chosen freely, as long as they allow the reader to
unambigously reconstruct the graph $G$ from the picture.
(Thus, for example:
\begin{itemize}
\item the points should be distinct and spaced
reasonably far apart;
\item the curves should not pass through points other than the ones
they connect;
\item the curves should not be too jagged (and they should certainly
be contiguous);
\item the curves should intersect in such a way that one can easily
see which strand is the continuation of which one.
\end{itemize}
Various examples of graphs being drawn are given below.)
\end{definition}

\begin{example} \label{exa.intro.draw}
Let us draw some simple graphs.

\begin{enumerate}

\item[\textbf{(a)}]
The simple graph $\tup{\set{1,2,3}, \set{12, 23}}$ (where
we are again using the shorthand notation $uv$ for $\set{u,v}$) can be
drawn as follows:
\[
\xymatrix{ 1 \are[r] & 2 \are[r] & 3 }.
\]
This is (in a sense) the simplest way to draw this graph: The edges
are represented by straight lines (rather than tortuous curves).

There are other options as well. For example, we can draw the same
graph as follows:
\begin{align*}
\xymatrix{ 1 \are@/_2pc/[rr] & 3 \are[r] & 2 }. \\
\phantom{a} %empty space to keep the graph from bleeding into the text
\end{align*}
Here, we have placed the points representing the vertices $1,2,3$
differently. As a consequence, we were not able to draw the edge $12$
as a straight line, because it would then have overlapped with the
vertex $3$, which would make the structure of the graph ambiguous
(the edge $12$ could be mistaken for two edges $13$ and $32$).

We can just as well draw both edges as curves:
\begin{align*}
\xymatrix{ 1 \are@/_2pc/[rr] & 3 \are@/^1pc/[r] & 2 }. \\
\phantom{a} %empty space to keep the graph from bleeding into the text
\end{align*}
This drawing still represents the same graph
$\tup{\set{1,2,3}, \set{12, 23}}$.

We do not have to place the three vertices all along the same line.
We can just as well draw the graph $\tup{\set{1,2,3}, \set{12, 23}}$
as follows:
\[
\xymatrix{
& 2 \are[dl] \are[dr] \\
1 & & 3
} .
\]

So far we have drawn our edges in such a way that they did not
intersect. But this is not a requirement. We could just as well have
represented our graph $\tup{\set{1,2,3}, \set{12, 23}}$
as follows:
\[
\xymatrix{
& 2 \are@/^2pc/[dl] \are@/_2pc/[dr] \\
1 & & 3
} .
\]
This last drawing is, of course, needlessly complicated for such a
graph, but it is perfectly legitimate.

\item[\textbf{(b)}] Let $U$, $E$ and $G$ be as in
Example~\ref{exa.intro.simple.VE}. Here is one way to draw the graph
$G$:
\begin{align*}
\xymatrix{
& 2 \are[rr] \are[rrrd] \are[rdd] & & 3 \are[rd] \are[ldd] \\
1 \are[rrrr] \are[rrd] & & & & 4 \are[lld] \\
& & 5
}
\end{align*}
Here is another way to draw the same graph $G$, with fewer
intersections between edges:
\begin{align*}
\xymatrix{
& 2 \are[rr] \are@/^4pc/[rrrd] \are@/_6pc/[rdd] & & 3 \are[rd] \are[ldd] \\
& 1 \are[rrr] \are[rd] & & & 4 \are[lld] \\
& & 5
} .
\end{align*}
By appropriately repositioning the points corresponding to the five
vertices of $G$, we can actually get rid of all intersections; namely,
we can draw the graph $G$ as follows:
\begin{align*}
\phantom{a} \\ %empty space to keep the graph from bleeding into the text
\xymatrix{
& 2 \are[rr] \are@/^4pc/[rrrd] \are[rdd] & & 3 \are[rd] \are[ldd] \\
& & & & 4 \are[lld] \\
& & 5 \are[r] & 1 \are[ru]
} .
\end{align*}
If we reposition the vertices further, we can even achieve a drawing
without intersecting curves that uses only straight lines as curves:
\begin{align*}
\xymatrix{
& 2 \are[rd] \are[rrrd] \are[rdd] \\
& & 3 \are[d] \are[rr] & & 4 \are[lld] \\
& & 5 \are[r] & 1 \are[ru]
} .
\end{align*}
All of these drawings are equally legitimate; some are more convenient
for certain purposes while others are less so.

Let us now show a \textbf{bad} drawing of the graph $G$ (that is, a
drawing that fails the ``allow the reader to
unambigously reconstruct the graph $G$ from the picture'' criterion):
\begin{align*}
\xymatrix{
2 \are[rr] \are@/^3pc/[dd] & & 3 \are@/_3pc/[dd] \are[lldd] \\
& \\
5 \are[rr] \are[rd] & & 4 \are[ld] \\
& 1
} .
\end{align*}
This drawing is bad because of the muddle that happens in the center
of the rectangle formed by the vertices $2$, $3$, $4$ and $5$. Two
curves are touching each other at that center, making the reader
wonder whether they are two curves representing the edges $24$ and
$35$ ``passing through each other'', or two curves representing the
edges $25$ and $34$ ``bouncing off each other'', or both, or maybe
even three curves. (The answer is ``both'': they represent all four
edges $24$, $35$, $25$ and $34$. But this is not clear from the
drawing.)

\item[\textbf{(c)}] Let us draw one further graph: the simple graph
$\tup{\set{1, 2, 3, 4, 5}, \powset[2]{\set{1, 2, 3, 4, 5}}}$. This is
the simple graph whose vertices are $1, 2, 3, 4, 5$, and whose edges
are all possible two-element sets consisting of its vertices (i.e.,
each pair of two distinct vertices is adjacent). We shall later (in
Definition~\ref{def.intro.complete-graph}) refer to this graph as
the ``complete graph $K_5$''. Here is a simple way to draw this graph:
\begin{align*}
\xymatrix{
& 2 \are[ld] \are[rr] \are[rrrd] \are[rdd] & & 3 \are[rd] \are[ldd] \are[llld] \\
1 \are[rrrr] \are[rrd] & & & & 4 \are[lld] \\
& & 5
}
\end{align*}
This drawing is useful for many purposes; for example, it makes the
abstract symmetry of this graph (i.e., the fact that, roughly
speaking, its vertices $1,2,3,4,5$ are ``equal in rights'') obvious.
But sometimes, you might want to draw it differently, to minimize the
number of intersecting curves. Here is a drawing with fewer
intersections:
\begin{align*}
\phantom{a} \\ %empty space to keep the graph from bleeding into the text
\xymatrix{
& 2 \are[rr] \are@/^4pc/[rrrd] \are@/_6pc/[rrdd] \are[dd] & & 3 \are[rd] \are[lldd] \are[dd] \\
& & & & 4 \are[llld] \\
& 5 \are[rr] & & 1 \are[ru]
} . \\
\phantom{a} %empty space to keep the graph from bleeding into the text
\end{align*}
In this drawing, we have only one intersection between two curves
left. Can we get rid of all intersections?

This is a non-combinatorial question, since it really is about curves
in the plane rather than about finite sets and graphs. The answer is
``no''. (That is, no matter how you draw this graph in the plane, you
will always have at least one pair of curves intersect.) This is a
classical result (one of the first theorems in the theory of
\textit{planar graphs}), and proofs of it can be found in various
textbooks on graph theory (e.g., \cite[Corollary 11.1(d)]{Harary69} or
\cite[Theorem 10.2]{BonMur08}). These proofs, however, are usually not
self-contained; they rely on some basic facts from the topology of the
real plane (mostly, the Jordan curve theorem). We shall not study
geometrical questions like this in these notes, but instead refer the
reader to texts such as \cite{FriFri98} for careful and complete
treatments of the topological technicalities.
(If you are willing to take certain intuitively obvious
topological facts about curves for granted, you can also read the
chapters about planar graphs appearing in most books on graph theory.)

\end{enumerate}

\end{example}

We note that some authors prefer to put the labels on the nodes
in little circles when drawing a graph. For example, the graph
from Example~\ref{exa.intro.draw} \textbf{(a)} then looks as
follows:
\[
\begin{tikzpicture}
\begin{scope}[every node/.style={circle,thick,draw=green!60!black}]
\node(A) at (0,0) {$1$};
\node(B) at (2,0) {$2$};
\node(C) at (4,0) {$3$};
\end{scope}
\begin{scope}[every edge/.style={draw=black,very thick}]
\path[-] (B) edge (A) (B) edge (C);
\end{scope}
\end{tikzpicture}
\]

\begin{remark}
When drawing a simple graph $G$, we have so far labelled the points
by the names of the vertices that they represent. However, often, the
names of the vertices will be unimportant. In such cases, we can just
as well label the points by little circles. For example, the simple
graph $G$ from Example~\ref{exa.intro.simple.VE} can thus be drawn
as follows:
\begin{align*}
\xymatrix{
& \circ \are[rr] \are[rrrd] \are[rdd] & & \circ \are[rd] \are[ldd] \\
\circ \are[rrrr] \are[rrd] & & & & \circ \are[lld] \\
& & \circ
} .
\end{align*}
Of course, from such a drawing, we \textbf{cannot} unambiguously
reconstruct the graph, since we do not know which edge connects which
vertices. But often, all we want to know about the graph is visible on
such a drawing, and the names of the vertices would only be
distracting.
\end{remark}

\subsection{\label{sect.intro.R33}A first fact: The Ramsey number
$R\tup{3,3} = 6$}

After these definitions, it might be time for a first result. The
following classical fact (which is actually the beginning of a deep
theory -- the so-called \textit{Ramsey theory}) should neatly
illustrate the concepts introduced above:

\begin{proposition} \label{prop.simple.R33}
Let $G$ be a simple graph with $\abs{\verts{G}} \geq 6$ (that is,
$G$ has at least $6$ vertices). Then, at least one of the following
two statements holds:

\begin{itemize}
\item \textit{Statement 1:} There exist three distinct vertices $a$,
$b$ and $c$ of $G$ such that $ab$, $bc$ and $ca$ are edges of $G$.

\item \textit{Statement 2:} There exist three distinct vertices $a$,
$b$ and $c$ of $G$ such that none of $ab$, $bc$ and $ca$ is an edge of
$G$.
\end{itemize}
\end{proposition}

In other words, Proposition~\ref{prop.simple.R33} says that if a graph
$G$ has at least $6$ vertices, then we can either find three distinct
vertices that are mutually adjacent\footnote{by which we mean (of
course) that any two \textbf{distinct} ones among these three vertices
are adjacent} or find three distinct vertices that are mutually
non-adjacent (i.e., no two of them are adjacent), or both.

Proposition~\ref{prop.simple.R33} can be stated more tersely as
follows: ``In any graph containing at least six vertices, you can
always find three vertices that are mutually adjacent, or three
vertices that are mutually non-adjacent''. It is also often restated
as follows:
``In any group of at least six people, you can always find three that
are (pairwise) friends to each other, or three no two of whom are
friends'' (provided that friendship is a symmetric relation). This
follows the old paradigm of restating facts about graphs in terms of
people and friendship.

\begin{example} \label{exa.simple.R33}
Let us show four examples of graphs $G$ to which
Proposition~\ref{prop.simple.R33} applies, as well as an example to
which it does not (because it fails to satisfy the condition
$\abs{\verts{G}} \geq 6$):

\begin{enumerate}

\item[\textbf{(a)}] Let $G$ be the graph $\tup{V, E}$, where
\begin{align*}
V &= \set{1, 2, 3, 4, 5, 6} \qquad \text{and} \\
E &= \set{\set{1,2}, \set{2,3}, \set{3,4}, \set{4,5}, \set{5,6},
          \set{6,1}} .
\end{align*}
(This graph can be drawn in such a way as to look like a hexagon:
\begin{align*}
\xymatrix{
& 1 \are[r] & 2 \are[rd] \\
6 \are[ru] & & & 3 \are[ld] \\
& 5 \are[lu] & 4 \are[l]
} .
\end{align*}
)
This graph satisfies Statement 2 of Proposition~\ref{prop.simple.R33}
for $a=1$, $b=3$ and $c=5$,
because its three vertices $1$, $3$ and $5$ are mutually
non-adjacent (i.e., none of $13$, $35$ and $51$ is an edge of $G$).
It also satisfies Statement 2 of Proposition~\ref{prop.simple.R33}
for $a=2$, $b=4$ and $c=6$.
So in this situation, we witness something slightly stronger than what
Proposition~\ref{prop.simple.R33} says: There are at least
\textit{two} choices of $a$, $b$ and $c$ making one of the Statements
1 and 2 valid (and these two choices are not the same up to order).
See Exercise~\ref{exa.simple.R33.two} \textbf{(a)} below.

\item[\textbf{(b)}] Let $G$ be the simple graph $\tup{V, E}$, where
\begin{align*}
V &= \set{-2, -1, 0, 1, 2, 3} \qquad \text{and} \\
E &= \left\{\set{u,v} \in \powset[2]{V} \mid
           u \not\equiv v \mod 3, \right. \\
  & \qquad \qquad \qquad \qquad \qquad
        \left. \text{and exactly one of $u$ and $v$ is positive}
                \right\} .
\end{align*}
(This graph can be drawn as follows:
\begin{align*}
\xymatrix{
0 \are[d] \are[dr] & -1 \are[dl] \are[dr] & -2 \are[dl] \are[d] \\
1 & 2 & 3
} .
\end{align*}
)
This graph satisfies Statement 2 of Proposition~\ref{prop.simple.R33}
for $a=1$, $b=2$ and $c=3$. (Again, other choices of $a$, $b$ and $c$
are also possible.)

\item[\textbf{(c)}] Let $G$ be the graph $\tup{V, E}$, where
\begin{align*}
V &= \set{1, 2, 3, 4, 5, 6} \qquad \text{and} \\
E &= \set{\set{1,2}, \set{2,3}, \set{3,4}, \set{4,5}, \set{5,6},
          \set{6,1}, \set{1,3}} .
\end{align*}
(This graph can be drawn in such a way as to look like a hexagon with
one extra diagonal:
\begin{align*}
\xymatrix{
& 1 \are[r] \are[rrd] & 2 \are[rd] \\
6 \are[ru] & & & 3 \are[ld] \\
& 5 \are[lu] & 4 \are[l]
} .
\end{align*}
)
This graph satisfies Statement 1 of Proposition~\ref{prop.simple.R33}
for $a=1$, $b=2$ and $c=3$. It also satisfies Statement 2 of
Proposition~\ref{prop.simple.R33} for $a=2$, $b=4$ and $c=6$.

\item[\textbf{(d)}] Let $G$ be the graph $\tup{V, E}$, where
\begin{align*}
V &= \set{1, 2, 3, 4, 5, 6} \qquad \text{and} \\
E &= \set{\set{1,2}, \set{2,3}, \set{3,4}, \set{4,5}, \set{5,6},
          \set{6,1}, \set{1,3}, \set{4,6}} .
\end{align*}
(This graph can be drawn in such a way as to look like a hexagon with
two extra diagonals:
\begin{align*}
\xymatrix{
& 1 \are[r] \are[rrd] & 2 \are[rd] \\
6 \are[ru] \are[rrd] & & & 3 \are[ld] \\
& 5 \are[lu] & 4 \are[l]
} .
\end{align*}
)
This graph no longer satisfies Statement 2 of
Proposition~\ref{prop.simple.R33}, but it satisfies Statement 1
for $a=1$, $b=2$ and $c=3$.

\item[\textbf{(e)}] Let $G$ be the graph $\tup{V, E}$, where
\begin{align*}
V &= \set{1, 2, 3, 4, 5} \qquad \text{and} \\
E &= \set{\set{1,2}, \set{2,3}, \set{3,4}, \set{4,5}, \set{5,1}} .
\end{align*}
(This graph can be drawn to look like a pentagon:
\begin{align*}
\xymatrix{
& 2 \are[rr] & & 3 \are[rd] \\
1 \are[ru] \are[rrd] & & & & 4 \are[lld] \\
& & 5
}
\end{align*}
)
Proposition~\ref{prop.simple.R33} says nothing about this graph,
since this graph does not satisfy the assumption of
Proposition~\ref{prop.simple.R33} (in fact, its number of vertices
$\abs{\verts{G}}$ fails to be $\geq 6$).
By itself, this does not yield that the claim of
Proposition~\ref{prop.simple.R33} is false for this graph. However,
it is easy to check that the claim actually \textbf{is} false for this
graph: Neither Statement 1 nor Statement 2 hold.
\end{enumerate}

\end{example}

\begin{proof}[Proof of Proposition~\ref{prop.simple.R33}.]We need to
prove that either Statement 1 holds or Statement 2 holds (or both).

Choose any vertex $u \in \verts{G}$. (This is clearly possible,
since $\abs{\verts{G}} \geq 6 \geq 1$.) Then,
$\abs{\verts{G} \setminus \set{u}} = \abs{\verts{G}} - 1 \geq 5$
(since $\abs{\verts{G}} \geq 6$). We are in one of the following two
cases:

\textit{Case 1:} At least $3$ vertices in
$\verts{G} \setminus \set{u}$ are adjacent to $u$.

\textit{Case 2:} At most $2$ vertices in
$\verts{G} \setminus \set{u}$ are adjacent to $u$.

Let us consider Case 1 first. In this case, at least $3$ vertices in
$\verts{G} \setminus \set{u}$ are adjacent to $u$. Hence, we can find
three distinct vertices $p$, $q$ and $r$ in
$\verts{G} \setminus \set{u}$ that are adjacent to $u$. Consider
these $p$, $q$ and $r$. If none of $pq$, $qr$ and $rp$ is an edge of
$G$, then Statement 2 holds (in fact, we can just take $a = p$,
$b = q$ and $c = r$). Thus, if none of $pq$, $qr$ and $rp$ is an edge
of $G$, then our proof is complete\footnote{because our goal in this
proof is to show that either Statement 1 holds or Statement 2 holds
(or both)}. Thus, we WLOG\footnote{The word ``WLOG'' means
``without loss of generality''. See, e.g., the corresponding Wikipedia
page \url{https://en.wikipedia.org/wiki/Without_loss_of_generality}
for its meaning.} assume that at least one $pq$, $qr$ and $rp$ is an
edge of $G$. In other words, we can pick two distinct elements $g$
and $h$ of $\set{p, q, r}$ such that $gh$ is an edge of $G$. Consider
these $g$ and $h$.

The vertex $g$ is one of $p$, $q$ and $r$
(since $g \in \set{p, q, r}$).
The vertices $p$, $q$ and $r$ are adjacent to $u$. Hence, the vertex
$g$ is adjacent to $u$ (since the vertex $g$ is one of $p$, $q$ and
$r$). In other words, $ug$ is an edge of $G$. Similarly, $uh$ is an
edge of $G$. In other words, $hu$ is an edge of $G$ (since $hu = uh$).

We have $g \in \set{p, q, r} \subseteq \verts{G} \setminus \set{u}$
(since $p$, $q$ and $r$ belong to $\verts{G} \setminus \set{u}$).
Hence, $g \neq u$. In other words, $u \neq g$.
% Clearly, $ug$ is a $2$-element set (since $ug$ is an edge of $G$, but
% each edge of $G$ is a $2$-element set). Thus, $u \neq g$.
Similarly,
$u \neq h$. Hence, $h \neq u$. Finally, $g \neq h$ (since $g$ and $h$
are distinct). Now, we know that the three vertices $u$, $g$ and $h$
are distinct (since $u \neq g$, $g \neq h$ and $h \neq u$), and have
the property that $ug$, $gh$ and $hu$ are edges of $G$. Therefore,
Statement 1 holds (in fact, we can just take $a = u$, $b = g$ and
$c = h$). Hence, the proof is complete in Case 1.

Let us now consider Case 2. In this case, at most $2$ vertices in
$\verts{G} \setminus \set{u}$ are adjacent to $u$. Thus, at least $3$
vertices in $\verts{G} \setminus \set{u}$ are non-adjacent to $u$
\ \ \ \ \footnote{\textit{Proof.} Let $k$ be the number of vertices
in $\verts{G} \setminus \set{u}$ that are adjacent to $u$. Let $\ell$
be the number of vertices in $\verts{G} \setminus \set{u}$ that are
non-adjacent to $u$. Then,
$k + \ell = \abs{\verts{G} \setminus \set{u}}$ (since each vertex
in $\verts{G} \setminus \set{u}$ is either adjacent to $u$ or
non-adjacent to $u$, but not both at the same time). But $k \leq 2$
(since at most $2$ vertices in $\verts{G} \setminus \set{u}$ are
adjacent to $u$). Hence, $k + \ell \leq 2 + \ell = \ell + 2$, so that
$\ell + 2 \geq k + \ell = \abs{\verts{G} \setminus \set{u}} \geq 5$
and thus $\ell \geq 3$. In other words, at least $3$
vertices in $\verts{G} \setminus \set{u}$ are non-adjacent to $u$.
Qed.}. Hence, we can find
three distinct vertices $p$, $q$ and $r$ in
$\verts{G} \setminus \set{u}$ that are non-adjacent to $u$. Consider
these $p$, $q$ and $r$. If all of $pq$, $qr$ and $rp$ are edges of
$G$, then Statement 1 holds (in fact, we can just take $a = p$,
$b = q$ and $c = r$). Thus, if all of $pq$, $qr$ and $rp$ are edges
of $G$, then our proof is complete. Thus, we WLOG
assume that not all of $pq$, $qr$ and $rp$ are edges of $G$. In other
words, at least one of $pq$, $qr$ and $rp$ is \textbf{not} an edge of
$G$. In other words, we can pick two distinct elements $g$ and $h$ of
$\set{p, q, r}$ such that $gh$ is not an edge of $G$. Consider
these $g$ and $h$.

The vertex $g$ is one of $p$, $q$ and $r$
(since $g \in \set{p, q, r}$).
The vertices $p$, $q$ and $r$ are non-adjacent to $u$. Hence, the
vertex $g$ is non-adjacent to $u$ (since the vertex $g$ is one of $p$,
$q$ and $r$). In other words, $ug$ is not an edge of $G$. Similarly,
$uh$ is not an edge of $G$. In other words, $hu$ is not an edge of $G$
(since $hu = uh$).

We have $g \in \set{p, q, r} \subseteq \verts{G} \setminus \set{u}$
(since $p$, $q$ and $r$ belong to $\verts{G} \setminus \set{u}$).
Thus, $g \neq u$. In other words, $u \neq g$. Similarly, $u \neq h$.
Finally, $g \neq h$ (since $g$ and $h$
are distinct). Now, we know that the three vertices $u$, $g$ and $h$
are distinct (since $u \neq g$, $g \neq h$ and $h \neq u$), and have
the property that none of $ug$, $gh$ and $hu$ is an edge of $G$ (since
$ug$ is not an edge of $G$, since $gh$ is not an edge of $G$, and
since $hu$ is not an edge of $G$). Therefore,
Statement 2 holds (in fact, we can just take $a = u$, $b = g$ and
$c = h$). Hence, the proof is complete in Case 2.

We have now proven Proposition~\ref{prop.simple.R33} in each of the
two Cases 1 and 2. Thus, the proof of
Proposition~\ref{prop.simple.R33} is complete.
\end{proof}

\begin{remark}
I have written the above proof in much detail, since it is the first
proof in these notes. I could have easily made it much shorter if I
had relied on the reader to fill in some details (and in fact, I
\textbf{will} rely on the reader in similar situations further below).
The proof can further be shortened by noticing that part of the
argument for Case 2 was a ``mirror version'' of the argument for
Case 1, with the only difference that ``adjacent'' is replaced by
``non-adjacent'' (and vice versa), and ``is an edge'' is replaced by
``is not an edge'' (and vice versa).
\end{remark}

\begin{remark}
Let me observe that Proposition~\ref{prop.simple.R33} could be proven
by brute force as well (using a computer). Indeed, here is how such a
proof would proceed: Let $x_1, x_2, x_3, x_4, x_5, x_6$ be six
distinct vertices of $G$. (Such six vertices exist, since
$\abs{\verts{G}} \geq 6$.) Let
$X = \set{x_1, x_2, x_3, x_4, x_5, x_6}$ be the set of these six
vertices. Notice that the set $X$ has $6$ elements, and thus the set
$\powset[2]{X}$ has $\dbinom{6}{2} = 15$ elements.
Let $F$ be the set of all edges $uv$ of $G$ for which both
$u$ and $v$ belong to $X$. (In other words, $F$ is the set of all
edges of $G$ having the form $x_i x_j$.)
Clearly, it suffices to prove Proposition~\ref{prop.simple.R33} for
the graph $\tup{X, F}$ instead of $G$ (because if we have found, for
example, three distinct vertices $a$, $b$ and $c$ of $\tup{X, F}$ such
that $ab$, $bc$ and $ca$ are edges of $\tup{X, F}$, then these $a$,
$b$ and $c$ are obviously also three vertices of $G$ such that
$ab$, $bc$ and $ca$ are edges of $G$). However,
$F$ is a subset of $\powset[2]{X}$. Since there
are only finitely many subsets of $\powset[2]{X}$ (in fact, there are
$2^{15}$ such subsets, since $\powset[2]{X}$ has $15$ elements), we
thus see that there are only finitely many choices for $F$ (when $X$
is being regarded as fixed). We can
check, for each of these choices, whether the graph $\tup{X, F}$
satisfies Proposition~\ref{prop.simple.R33}. (Just try each
possible choice of three distinct vertices $a$, $b$ and $c$ of this
graph $\tup{X, F}$, and check that at least one of these choices
satisfies either Statement 1 or Statement 2.) After a huge but
finite amount of checking (which you can automate), you will see that
Proposition~\ref{prop.simple.R33} holds for $\tup{X, F}$. Thus, as we
have already mentioned, Proposition~\ref{prop.simple.R33} also holds
for the original graph $G$.
\end{remark}

Proposition~\ref{prop.simple.R33} is the first result in a field
of graph theory known as \textit{Ramsey theory}. I shall not dwell on
this field in these notes, but let me make a few more remarks.
The first step beyond Proposition~\ref{prop.simple.R33} is the
following generalization:

\begin{proposition} \label{prop.simple.Rrs}
Let $r$ and $s$ be two positive integers.
Let $G$ be a simple graph with
$\abs{\verts{G}} \geq \dbinom{r+s-2}{r-1}$.
Then, at least one of the following two statements holds:

\begin{itemize}
\item \textit{Statement 1:} There exist $r$ distinct vertices
of $G$ that are mutually adjacent (i.e., each two distinct ones among
these $r$ vertices are adjacent).

\item \textit{Statement 2:} There exist $s$ distinct vertices
of $G$ that are mutually non-adjacent (i.e., no two distinct ones
among these $s$ vertices are adjacent).
\end{itemize}
\end{proposition}

Applying Proposition~\ref{prop.simple.Rrs} to $r=3$ and $s=3$, we can
recover Proposition~\ref{prop.simple.R33}.

One might wonder whether the number $\dbinom{r+s-2}{r-1}$ in
Proposition~\ref{prop.simple.Rrs} can be improved -- i.e., whether we
can replace it by a smaller number without making
Proposition~\ref{prop.simple.Rrs} false.
In the case of $r=3$ and $s=3$, this is impossible, because the
number $6$ in Proposition~\ref{prop.simple.R33} cannot be made
smaller\footnote{Indeed, there is a graph with $5$ vertices (namely,
the graph $G$ constructed in
Example~\ref{exa.simple.R33} \textbf{(e)}) that
satisfies neither Statement 1 nor Statement 2.}.
However, for some other values of $r$ and $s$, the value
$\dbinom{r+s-2}{r-1}$ can be improved.
(For example, for $r=4$ and $s=4$, the best possible value
is $18$ rather than $\dbinom{4+4-2}{4-1}=20$.)
The smallest possible value that could stand in place of
$\dbinom{r+s-2}{r-1}$ in Proposition~\ref{prop.simple.Rrs}
is called the \textit{Ramsey number}
$R\tup{r,s}$; thus, we have just showed that $R\tup{3,3} = 6$.
Finding $R\tup{r,s}$ for higher values of $r$ and $s$ is a
hard computational challenge; here are some values that have been
found with the help of computers:
\begin{align*}
R\tup{3,4} = 9; \qquad
R\tup{3,5} = 14; \qquad
R\tup{3,6} = 18; \qquad
R\tup{3,7} = 23; \\
R\tup{3,8} = 28; \qquad
R\tup{3,9} = 36; \qquad
R\tup{4,4} = 18; \qquad
R\tup{4,5} = 25.
\end{align*}
(We are only considering the cases $r \leq s$, since
it is easy to see that $R\tup{r,s} = R\tup{s,r}$ for all
$r$ and $s$. Also, the trivial values
$R\tup{1,s} = 1$ and $R\tup{2,s} = s+1$ for $s \geq 2$
are omitted.)
The Ramsey number $R\tup{5,5}$ is still unknown
(although it is known that $43 \leq R\tup{5,5} \leq 48$).
See \cite{Radzis21} for more about the current state of
affairs in computing Ramsey numbers.

Proposition~\ref{prop.simple.Rrs} can be further generalized to a
result called \textit{Ramsey's theorem}. The idea behind the
generalization is to slightly change the point of view, and replace
the simple graph $G$ by a complete graph (i.e., a simple graph in
which every two distinct vertices are adjacent) whose edges are
colored in two colors (say, blue and red). This is a completely
equivalent concept, because the concepts of ``adjacent'' and
``non-adjacent'' in $G$ can be identified with the concepts of
``adjacent through a blue edge'' (i.e., the edge connecting them is
colored blue) and ``adjacent through a red edge'', respectively.
Statements 1 and 2 then turn into ``there exist $r$ distinct vertices
that are mutually adjacent through blue edges'' and ``there exist $s$
distinct vertices that are mutually adjacent through red edges'',
respectively. From this point of view, it is only logical to
generalize Proposition~\ref{prop.simple.Rrs} further to the case when
the edges of a complete graph are colored in $k$ (rather than two)
colors. The corresponding generalization is known as Ramsey's theorem.
We refer to the well-written Wikipedia page
\url{https://en.wikipedia.org/wiki/Ramsey's_theorem} for a treatment
of this generalization with proof, as well as a table of known Ramsey
numbers $R\tup{r,s}$ and a self-contained (if somewhat terse) proof of
Proposition~\ref{prop.simple.Rrs}. Ramsey's theorem can be generalized
further, and many variations of it can be defined, which are usually
subsumed under the label ``Ramsey theory''.\footnote{See
\cite{RaWi-Ramsey} for a popular view on the philosophy of Ramsey
theory (in the wide sense of this word). It should probably be said
that mathematicians usually define the word ``Ramsey theory'' somewhat
more restrictively, and not every result of the form ``you can find a
pattern in any sufficiently large structure'' belongs to Ramsey
theory; but the rough idea is correct.} There are many papers and at
least one textbook \cite{GrRoSp90} available on Ramsey theory. For
elementary introductions, see the Cut-the-knot page
\url{http://www.cut-the-knot.org/Curriculum/Combinatorics/ThreeOrThree.shtml}
in \cite{cut-the-knot}, the above-mentioned Wikipedia article, as well
as \cite[\S 4.2]{Harju14}, \cite[Chapter VI]{Bollob79} and
\cite[Section 8.3]{West01}.

\begin{exercise} \label{exa.simple.R33.two}
Let $G$ be a simple graph. A \textit{triangle} in $G$ means a set
$\set{a, b, c}$ of three distinct vertices $a$, $b$ and $c$ of $G$
such that $ab$, $bc$ and $ca$ are edges of $G$. An
\textit{anti-triangle} in $G$ means a set $\set{a, b, c}$ of three
distinct vertices $a$, $b$ and $c$ of $G$ such that none of $ab$, $bc$
and $ca$ is an edge of $G$. A \textit{triangle-or-anti-triangle} in
$G$ is a set that is either a triangle or an anti-triangle.
(Of the three words I have just introduced, only ``triangle'' is
standard.)

\begin{enumerate}

\item[\textbf{(a)}] Assume that $\abs{\verts{G}}\geq 6$.
Prove that $G$ has at least two
triangle-or-anti-triangles. (For comparison:
Proposition~\ref{prop.simple.R33} shows that $G$ has at least one
triangle-or-anti-triangle.)

\item[\textbf{(b)}] Assume that $\abs{\verts{G}} = m+6$ for some
$m \in \NN$. Prove that $G$ has at least $m+1$
triangle-or-anti-triangles.

\end{enumerate}

\end{exercise}

\subsection{\label{sect.intro.deg}Degrees}

Next, we introduce the notion of the \textit{degree} of a vertex of a
graph. This is simply the number of edges containing this vertex:

\begin{definition} \label{def.intro.deg}
Let $G = \tup{V, E}$ be a simple graph. Let $v \in V$ be a vertex of
$G$. Then, the \textit{degree} of $v$ (with respect to $G$) is defined
as the number of all edges of $G$ that contain $v$.
This degree is a nonnegative integer, and is denoted by $\deg v$. It
is also denoted by $\deg_G v$, when the graph $G$ is not clear from
the context.

Thus,
\begin{align}
\deg v &= \deg_G v =
\left(\text{the number of all edges of } G \text{ that contain } v
\right)
\label{eq.def.intro.deg.0a} \\
&=
\left(\text{the number of all } e \in E \text{ that contain } v
\right)
\label{eq.def.intro.deg.0b} \\
& \qquad
\left(\text{since the edges of } G \text{ are precisely the } e \in E
\right) \nonumber \\
&= \abs{\set{e \in E \ \mid \ v \in e }} .
\label{eq.def.intro.deg.0c}
\end{align}
\end{definition}

\begin{example} \label{exa.intro.deg}
Define the sets $U$ and $E$ as in Example~\ref{exa.intro.simple.VE}.
Let $G$ be the graph $\tup{U, E}$. Then, $\deg 1$ is the number of all
edges of $G$ that contain $1$. These edges are $14$ and $15$; hence,
their number is $2$. Thus, $\deg 1 = 2$. Similarly, $\deg 4 = 4$
(since $\deg 4$ is the number of all edges of $G$ that contain $4$,
but these edges are $14$, $24$, $34$ and $45$). Similarly,
$\deg 2 = 3$, $\deg 3 = 3$ and $\deg 5 = 4$.
\end{example}

Here are some different characterizations of degrees in a simple
graph:

\begin{proposition} \label{prop.intro.deg.def2}
Let $G = \tup{V, E}$ be a simple graph. Let $v \in V$ be a vertex of
$G$. Then,
\begin{align}
\deg v &= \deg_G v
\label{eq.def.intro.deg.G} \\
&= \left(\text{the number of all neighbors of } v\right)
\label{eq.def.intro.deg.1} \\
&= \abs{\set{u \in V \ \mid \ u \text{ is a neighbor of } v }}
\label{eq.def.intro.deg.2} \\
&= \abs{\set{u \in V \ \mid \ u \text{ is adjacent to } v }}
\label{eq.def.intro.deg.3} \\
&= \abs{\set{u \in V \ \mid \ uv \text{ is an edge of } G }}
\label{eq.def.intro.deg.4} \\
&= \abs{\set{u \in V \ \mid \ uv \in E }} .
\label{eq.def.intro.deg.5}
\end{align}
\end{proposition}

Proposition~\ref{prop.intro.deg.def2} is essentially obvious: It
follows from observing that each edge of $E$ that contains $v$
contains exactly one neighbor of $v$, and that different such edges
contain different neighbors of $v$. For the mere sake of completeness,
let me include a formal proof.

\begin{proof}[Proof of Proposition~\ref{prop.intro.deg.def2}.]Both
$\deg v$ and $\deg_G v$ denote the degree of $v$. Thus,
$\deg v = \deg_G v$. Hence, \eqref{eq.def.intro.deg.G} is proven.

Let $U$ be the set of all neighbors of $v$. Then,
\[
\abs{U}
= \left(\text{the number of all neighbors of } v\right) .
\]

Let $E_v$ be the subset $\set{e \in E \ \mid \ v \in e }$ of $E$. This
is the set of all edges $e \in E$ that contain $v$.

For each $u \in U$, we have
$vu \in E_v$\ \ \ \ \footnote{\textit{Proof.} Let $u \in U$. Thus, $u$
is a neighbor of $v$ (by the definition of $U$). In other words,
$vu \in E$ (by the definition of a ``neighbor''). Furthermore,
$v \in \set{v, u} = vu$. Hence, $vu$ is an element $e \in E$
satisfying $v \in e$ (since $vu \in E$ and $v \in vu$). In other
words, $vu \in \set{e \in E \ \mid \ v \in e } = E_v$. Qed.}.
Hence, we can define a map $\alpha : U \to E_v$ by setting
\[
\alpha\tup{u} = vu \qquad \text{for each } u \in U .
\]
Consider this map $\alpha$. This map $\alpha$ is
injective\footnote{\textit{Proof.} Let $u_1$ and $u_2$ be two elements
of $U$ satisfying $\alpha\tup{u_1} = \alpha\tup{u_2}$. We shall show
that $u_1 = u_2$.

The definition of $\alpha$ yields $\alpha\tup{u_1} = vu_1
= \set{v, u_1}$. Hence, $\set{v, u_1} = \alpha\tup{u_1} \in E_v
\subseteq E
\subseteq \powset[2]{V}$. Therefore, $\set{v, u_1}$ is a two-element
set. Hence, $v \neq u_1$. Therefore, $\set{v, u_1} \setminus \set{v}
= \set{u_1}$. Hence,
$\set{u_1}
= \underbrace{\set{v, u_1}}_{= \alpha\tup{u_1}} \setminus \set{v}
= \alpha\tup{u_1} \setminus \set{v}$.
The same argument (but applied to $u_2$ instead of $u_1$) shows that
$\set{u_2} = \alpha\tup{u_2} \setminus \set{v}$.
Now,
\[
\set{u_1}
= \underbrace{\alpha\tup{u_1}}_{= \alpha\tup{u_2}} \setminus \set{v}
= \alpha\tup{u_2} \setminus \set{v} = \set{u_2} ,
\]
so that
$u_1 \in \set{u_1} = \set{u_2}$ and therefore $u_1 = u_2$.

Now, forget that we fixed $u_1$ and $u_2$. We thus have proven that
if $u_1$ and $u_2$ are two elements of $U$ satisfying
$\alpha\tup{u_1} = \alpha\tup{u_2}$, then $u_1 = u_2$. In other words,
the map $\alpha$ is injective.} and
surjective\footnote{\textit{Proof.} Let $f \in E_v$. Thus,
$f \in E_v = \set{e \in E \ \mid \ v \in e }$. In other words, $f$
is an element of $E$ satisfying $v \in f$.

We have $f \in E \subseteq \powset[2]{V}$. Thus, $f$ is a $2$-element
subset of $V$. Thus, $\abs{f} = 2$. Since $v \in f$, we have
$\abs{f \setminus \set{v}} = \abs{f} - 1 = 1$ (since $\abs{f} = 2$).
Hence, $f \setminus \set{v}$ is a $1$-element set. Therefore,
$f \setminus \set{v} = \set{u}$ for some $u$. Consider this $u$. Now,
$u \in \set{u} = f \setminus \set{v} \subseteq f \subseteq V$; thus,
$u$ is a vertex of $G$. Moreover, $v \in f$ and thus
$f = \underbrace{\tup{f \setminus \set{v}}}_{= \set{u}} \cup \set{v}
= \set{u} \cup \set{v} = \set{u, v} = \set{v, u} = vu$. Hence,
$vu = f \in E$. In other words, $u$ is a neighbor of $v$ (by the
definition of ``neighbor''). In other words, $u \in U$ (since $U$ is
the set of all neighbors of $v$). The definition of $\alpha$ now
yields $\alpha\tup{u} = vu = f$. Hence, $f = \alpha\tup{u} \in
\alpha\tup{U}$.

Now, forget that we fixed $f$. We thus have shown that
$f \in \alpha\tup{U}$ for each $f \in E_v$. In other words, $E_v
\subseteq \alpha\tup{U}$. In other words, the map $\alpha$ is
surjective.}. Hence, the map $\alpha$ is bijective. Thus, we have
found a bijective map $\alpha$ from $U$ to $E_v$. Therefore,
$\abs{E_v} = \abs{U}$.

But $E_v = \set{e \in E \ \mid \ v \in e }$ and thus
$\abs{E_v} = \abs{\set{e \in E \ \mid \ v \in e }} = \deg v$
(by \eqref{eq.def.intro.deg.0c}). Hence,
$\deg v = \abs{E_v} = \abs{U}
= \left(\text{the number of all neighbors of } v\right)$. This proves
\eqref{eq.def.intro.deg.1}.

Now,
\begin{align*}
\deg v &= \left(\text{the number of all neighbors of } v\right) \\
&= \abs{\underbrace{\text{the set of all neighbors of } v }_{
         = \set{u \in V \ \mid \ u \text{ is a neighbor of } v }}} \\
&= \abs{\set{u \in V \ \mid \ u \text{ is a neighbor of } v }} \\
&= \abs{\set{u \in V \ \mid \ u \text{ is adjacent to } v }}
\end{align*}
(because for any vertex $u \in V$, the condition
``$u$ is a neighbor of $v$'' is equivalent to the condition
``$u$ is adjacent to $v$''). Thus, \eqref{eq.def.intro.deg.2} and
\eqref{eq.def.intro.deg.3} are proven.

Furthermore,
\begin{align*}
\deg v &= \abs{\set{u \in V \ \mid \ u \text{ is a neighbor of } v }} \\
&= \abs{\set{u \in V \ \mid \ vu \in E }}
\end{align*}
(because for any vertex $u \in V$, the condition ``$u$ is a neighbor
of $v$'' is equivalent to the condition ``$vu \in E$''). Hence,
\begin{align*}
\deg v &= \abs{\set{u \in V \ \mid \ \underbrace{vu}_{=uv} \in E }}
= \abs{\set{u \in V \ \mid \ uv \in E }} \\
&= \abs{\set{u \in V \ \mid \ uv \text{ is an edge of } G }}
\end{align*}
(because for any vertex $u \in V$, the condition ``$uv \in E$'' is
equivalent to the condition ``$uv$ is an edge of $G$''). Thus, we
have proven \eqref{eq.def.intro.deg.5} and
\eqref{eq.def.intro.deg.4}. The proof of
Proposition~\ref{prop.intro.deg.def2} is thus complete.
\end{proof}

\begin{remark}
Different sources use different notations for the degree of a
vertex $v$ of a simple graph $G$. We call it $\deg v$ (and so do
Ore's \cite{Ore90} and the introductory notes
\cite{LeLeMe16}); Ore's \cite{Ore74} calls it $\rho\left(v\right)$;
Bollob\'as's \cite{Bollob79} and Bondy's and Murty's
\cite{BonMur08} and \cite{BonMur76} call it $d\left(v\right)$.
% Not sure if anyone uses $\delta\left(v\right)$.
\end{remark}

At this point, we can state a few simple facts about degrees:

\begin{proposition} \label{prop.intro.deg.in-set}
Let $G$ be a simple graph. Let $n = \abs{\verts{G}}$. Let $v$ be a
vertex of $G$. Then, $\deg v \in \set{0, 1, \ldots, n-1}$.
\end{proposition}

\begin{proof}[Proof of Proposition~\ref{prop.intro.deg.in-set}.]
Let $U$ be the set of all neighbors of $v$. Then,
\[
\abs{U}
= \left(\text{the number of all neighbors of } v\right) .
\]
Comparing
this with \eqref{eq.def.intro.deg.1}, we obtain $\abs{U} = \deg v$.

But $U \subseteq \verts{G} \setminus \set{v}$
\ \ \ \ \footnote{\textit{Proof.} Let $u \in U$. Thus, $u$ is
a neighbor of $v$ (by the definition of $U$). In other words, $u$
is a vertex of $G$ such that $vu \in \edges{G}$ (by the definition
of ``neighbor''). Thus, $vu \in \edges{G} \subseteq
\powset[2]{\verts{G}}$ (since each edge of $G$ is a $2$-element
subset of $\verts{G}$). In particular, $vu$ is a $2$-element set.
Hence, $u \neq v$. But $u \in \verts{G}$ (since $u$ is a vertex of
$G$). Combining this with $u \neq v$, we obtain
$u \in \verts{G} \setminus \set{v}$.

Now, forget that we fixed $u$. We thus have shown that
$u \in \verts{G} \setminus \set{v}$ for each $u \in U$. In other
words, $U \subseteq \verts{G} \setminus \set{v}$.}. Hence,
$\abs{U} \leq \abs{\verts{G} \setminus \set{v}}
= \abs{\verts{G}} - 1$ (since $v \in \verts{G}$). Since
$\abs{\verts{G}} = n$, this inequality can be rewritten as
$\abs{U} \leq n - 1$. Since $\abs{U}$ is a nonnegative integer, we
thus have $\abs{U} \in \set{0, 1, \ldots, n-1}$. Since
$\abs{U} = \deg v$, we can rewrite this as
$\deg v \in \set{0, 1, \ldots, n-1}$. This proves
Proposition~\ref{prop.intro.deg.in-set}.
\end{proof}

\begin{proposition} \label{prop.intro.2n}
Let $G$ be a simple graph.
The sum of the degrees of all vertices of $G$ equals
twice the number of edges of $G$. In other words,
\begin{equation}
\sum_{v \in \verts{G}} \deg v = 2 \abs{\edges{G}} .
\label{eq.prop.intro.2n.main}
\end{equation}
\end{proposition}

\begin{proof}[Proof of Proposition~\ref{prop.intro.2n}.]
Write the simple graph $G$ in the form $G = \tup{V, E}$. Thus,
$\verts{G} = V$ and $\edges{G} = E$.

Now, let $N$ be the number of all pairs $\tup{v, e} \in V \times E$
such that $v \in e$. Then, we can express $N$ in the following two
ways:

\begin{itemize}
\item We can obtain $N$ by computing, for each $v \in V$, the number
of all $e \in E$ satisfying $v \in e$, and then adding up these
numbers over all $v \in V$. Thus, we obtain
\begin{equation}
N
= \sum_{v \in V}
  \underbrace{\left(\text{the number of all } e \in E
                \text{ satisfying } v \in e\right)}_{
              \substack{= \left(\text{the number of all } e \in E
                \text{ that contain } v \right) = \deg v \\
                \text{(by \eqref{eq.def.intro.deg.0b}})}}
= \sum_{v \in V} \deg v .
\label{pf.prop.intro.2n.1}
\end{equation}

\item We can also obtain $N$ by computing, for each $e \in E$, the
number of all $v \in V$ satisfying $v \in e$, and then adding up these
numbers over all $e \in E$. Thus, we obtain
\begin{equation}
N
= \sum_{e \in E}
   \left(\text{the number of all } v \in V
          \text{ satisfying } v \in e\right) .
\label{pf.prop.intro.2n.2a}
\end{equation}
But for each $e \in E$, we have
\[
\left(\text{the number of all } v \in V
          \text{ satisfying } v \in e\right) = 2
\]
\footnote{\textit{Proof.} Let $e \in E$. Then,
$e \in E \subseteq \powset[2]{V}$. In other words, $e$ is a
$2$-element subset of $V$. Hence, $\abs{e} = 2$ (since $e$ is a
$2$-element set).

But the $v \in V$ satisfying $v \in e$ are precisely the $v \in e$
(because $e$ is a subset of $V$). Hence,
\[
\left(\text{the number of all } v \in V
          \text{ satisfying } v \in e\right)
= \left(\text{the number of all } v \in e \right)
= \abs{e} = 2 ,
\]
qed.}. Hence, \eqref{pf.prop.intro.2n.2a} becomes
\begin{align}
N
&= \sum_{e \in E}
   \underbrace{\left(\text{the number of all } v \in V
                 \text{ satisfying } v \in e\right)}_{= 2}
\\
&= \sum_{e \in E} 2 = \abs{E} \cdot 2
\label{pf.prop.intro.2n.2}
\end{align}
(because the sum $\sum_{e \in E} 2$ consists of $\abs{E}$ addends,
each of which equals $2$).
\end{itemize}

Comparing \eqref{pf.prop.intro.2n.1} with \eqref{pf.prop.intro.2n.2},
we obtain $\sum_{v \in V} \deg v = \abs{E} \cdot 2 = 2 \abs{E}$.
Since $\verts{G} = V$ and $\edges{G} = E$, we can rewrite this as
$\sum_{v \in \verts{G}} \deg v = 2 \abs{\edges{G}}$.
Thus, Proposition~\ref{prop.intro.2n} is proven.
\end{proof}

\begin{remark}
The above proof of Proposition~\ref{prop.intro.2n} is a classical
example of the technique of \textit{double-counting}: i.e., finding
two different expressions for one and the same value (which, in
itself, is not of much interest), and then obtaining an equality by
comparing these two different expressions. In the above proof,
the value was $N$, and the two expressions were
\eqref{pf.prop.intro.2n.1} and \eqref{pf.prop.intro.2n.2}.
\end{remark}

\begin{proposition} \label{prop.intro.even-odd}
Let $G$ be a simple graph.
Then, the number of vertices $v$ of $G$ whose degree $\deg v$ is odd
is even.
\end{proposition}

\begin{proof}[Proof of Proposition~\ref{prop.intro.even-odd}.]
From \eqref{eq.prop.intro.2n.main}, we obtain
\[
\sum_{v \in \verts{G}} \deg v = 2 \abs{\edges{G}} \equiv 0 \mod 2.
\]
Hence,
\begin{align*}
0 &\equiv \sum_{v \in \verts{G}} \deg v
=
\sum_{\substack{v \in \verts{G}; \\ \deg v \text{ is even}}}
  \underbrack{\deg v}{\equiv 0 \mod 2 \\
                      \text{(since } \deg v \text{ is even)}}
+
\sum_{\substack{v \in \verts{G}; \\ \deg v \text{ is odd}}}
  \underbrack{\deg v}{\equiv 1 \mod 2 \\
                      \text{(since } \deg v \text{ is odd)}} \\
&\equiv
\underbrace{\sum_{\substack{v \in \verts{G}; \\ \deg v
            \text{ is even}}} 0}_{= 0}
+
\sum_{\substack{v \in \verts{G}; \\ \deg v \text{ is odd}}} 1 \\
&= \sum_{\substack{v \in \verts{G}; \\ \deg v \text{ is odd}}} 1
= \abs{\set{v \in \verts{G} \ \mid \ \deg v \text{ is odd}}} \cdot 1
\\
&= \abs{\set{v \in \verts{G} \ \mid \ \deg v \text{ is odd}}} \\
&= \left(\text{the number of vertices } v \text{ of } G
            \text{ whose degree } \deg v \text{ is odd}\right)
\mod 2 .
\end{align*}
In other words, the number of vertices $v$ of $G$ whose degree
$\deg v$ is odd is even. This proves
Proposition~\ref{prop.intro.even-odd}.
\end{proof}

As usual, Proposition~\ref{prop.intro.even-odd} can be restated in
terms of friendships among people; this restatement takes the
following form: ``In a group of (finitely many) people, the number of
people having an odd number of friends is even''. (Once again, this
assumes that friendship is a symmetric relation, and that nobody
counts as a friend of his own.)

\begin{proposition} \label{prop.intro.pigeonhole}
Let $G$ be a simple graph with at least two vertices.
Then, there exist two distinct vertices $v$ and $w$ of $G$ having the
same degree (that is, having $\deg v = \deg w$).
\end{proposition}

\begin{proof}[Proof of Proposition~\ref{prop.intro.pigeonhole}.]
Assume the contrary. Then, there exist \textbf{no} two distinct
vertices $v$ and $w$ of $G$ having the same degree. Hence, the degrees
of all vertices of $G$ are distinct. In other words, any two distinct
vertices $v$ and $w$ of $G$ satisfy
\begin{equation}
\deg v \neq \deg w .
\label{pf.prop.intro.pigeonhole.neq}
\end{equation}

Let $n = \abs{\verts{G}}$. Proposition~\ref{prop.intro.deg.in-set}
shows that $\deg v \in \set{0, 1, \ldots, n-1}$ for each
$v \in \verts{G}$. Hence, we can define a map
$d : \verts{G} \to \set{0, 1, \ldots, n-1}$ by
\[
d\tup{v} = \deg v \qquad \text{for each } v \in \verts{G} .
\]
Consider this map $d$. The map $d$ is
injective\footnote{\textit{Proof.} Let $v$ and $w$ be two distinct
elements of $\verts{G}$. Then, $\deg v \neq \deg w$
(by \eqref{pf.prop.intro.pigeonhole.neq}). But
the definition of $d$ yields
$d\tup{v} = \deg v$ and $d\tup{w} = \deg w$. Now,
$d\tup{v} = \deg v \neq \deg w = d\tup{w}$.

Now, forget that we fixed $v$ and $w$. We thus have shown that if $v$
and $w$ are two distinct elements of $\verts{G}$, then
$d\tup{v} \neq d\tup{w}$. In other words, the map $d$ is injective.}.

Recall the following elementary fact: If $X$ and $Y$ are two finite
sets satisfying $\abs{X} = \abs{Y}$, then each injective map from $X$
to $Y$ is bijective\footnote{Here is a quick \textit{proof} of this
fact: Let $X$ and $Y$ be two finite sets satisfying $\abs{X} =
\abs{Y}$. Let $f$ be an injective map from $X$ to $Y$. We must show
that $f$ is bijective.

The set $Y$ is finite. Hence, the only subset of $Y$ that has as
many elements as $Y$ is $Y$ itself.

The set $f\tup{X}$ consists of the images of the elements of $X$ under
the map $f$. These images are pairwise distinct (since $f$ is
injective), and there are precisely $\abs{X}$ many of them.
Hence, $\abs{f\tup{X}} = \abs{X}$. Now, $f\tup{X}$ is a subset of $Y$
that has as many elements as $Y$ (since $\abs{f\tup{X}} = \abs{X}
= \abs{Y}$). Therefore, $f\tup{X}$ must be $Y$ itself (since the only
subset of $Y$ that has as many elements as $Y$ is $Y$ itself). In
other words, $Y = f\tup{X}$. Hence, the map $f$ is surjective. Thus,
$f$ is bijective (since $f$ is injective and surjective). This proves
the fact that we wanted to prove.}. Applying this to $X = \verts{G}$,
$Y = \set{0, 1, \ldots, n-1}$ and $f = d$, we conclude that
the map $d$ is bijective (since $\verts{G}$ and
$\set{0, 1, \ldots, n-1}$ are finite sets satisfying
$\abs{\verts{G}} = n = \abs{\set{0, 1, \ldots, n-1}}$, and since
$d : \verts{G} \to \set{0, 1, \ldots, n-1}$ is an injective map).
In particular, $d$ is surjective.

Now, $n = \abs{\verts{G}} \geq 2$ (since $G$ has at least two
vertices). Hence, $0 \in \set{0, 1, \ldots, n-1}$. Since $d$ is
surjective, this shows that there exists some vertex $a \in \verts{G}$
such that $d\tup{a} = 0$. Consider this $a$. The definition of $d$
yields $d\tup{a} = \deg a$, so that $\deg a = d\tup{a} = 0$.

Also, $n-1 \in \set{0, 1, \ldots, n-1}$ (since $n \geq 2$). Since $d$
is surjective, this shows that there exists some vertex
$b \in \verts{G}$
such that $d\tup{b} = n-1$. Consider this $b$. The definition of $d$
yields $d\tup{b} = \deg b$, so that $\deg b = d\tup{b} = n-1$.

From $n \geq 2$, we obtain $n-1 \geq 1 > 0$, so that $n-1 \neq 0$,
and thus $\deg a = 0 \neq n-1 = \deg b$. Hence, $a \neq b$.

Now, let $B$ be the set of all neighbors of $b$. Then,
\[
\abs{B} = \left(\text{the number of all neighbors of } b\right) .
\]
Comparing this with
\[
\deg b = \left(\text{the number of all neighbors of } b\right)
\qquad \left(\text{by \eqref{eq.def.intro.deg.1}, applied to } v = b
\right) ,
\]
we obtain $\abs{B} = \deg b = n-1$.

But $B \subseteq
\verts{G} \setminus \set{a, b}$\ \ \ \ \footnote{\textit{Proof.}
Write the graph $G$ in the form $G = \tup{V, E}$.
Let $v \in B$. Then, $v$ is a neighbor of $b$ (since $B$ is the set of
all neighbors of $b$). In other words, $bv \in E$ (by the definition
of ``neighbor''). Hence, $bv \in E \subseteq \powset[2]{V}$.
Therefore, $bv$ is a $2$-element set. Hence, $v \neq b$.

Also, $vb = bv \in E$. Thus, $b$ is a neighbor of $v$. Hence, the
vertex $v$ has at least one neighbor (namely, $b$).
Now, \eqref{eq.def.intro.deg.1} yields
\[
\deg v = \left(\text{the number of all neighbors of } v\right)
\geq 1
\]
(since the vertex $v$ has at least one neighbor), so that
$\deg v \geq 1 > 0 = \deg a$ and thus $\deg v \neq \deg a$ and
therefore $v \neq a$. Combining this with $v \neq b$, we find
$v \notin \set{a, b}$. Hence, $v \in \verts{G} \setminus \set{a, b}$.

Now, forget that we fixed $v$. We thus have proven that each $v \in B$
satisfies $v \in \verts{G} \setminus \set{a, b}$. In other words,
$B \subseteq \verts{G} \setminus \set{a, b}$.}. Hence,
\begin{align*}
\abs{B} &\leq \abs{\verts{G} \setminus \set{a, b}}
= \underbrace{\abs{\verts{G}}}_{= n}
- \underbrack{\abs{\set{a, b}}}{
                = 2 \\ \text{(since } a \neq b \text{)}}
\qquad \left(\text{since } \set{a, b} \subseteq \verts{G} \right) \\
&= n - 2 < n - 1 .
\end{align*}
This contradicts $\abs{B} = n - 1$. This contradiction shows that our
assumption was wrong. Proposition~\ref{prop.intro.pigeonhole} is
proven.
\end{proof}

Degrees of vertices can be used to prove various facts about graphs.
For an example, let us show \textit{Mantel's theorem}:

\begin{theorem} \label{thm.intro.mantel}
Let $G$ be a simple graph. Let $n = \abs{\verts{G}}$ be the number of
vertices of $G$. Assume that $\abs{\edges{G}} > n^2 / 4$. (In other
words, assume that $G$ has more than $n^2 / 4$ edges.) Then, there
exist three distinct vertices $a$,
$b$ and $c$ of $G$ such that $ab$, $bc$ and $ca$ are edges of $G$.
\end{theorem}

\begin{example}
Let $G$ be the graph $\tup{V, E}$, where
\begin{align*}
V &= \set{1, 2, 3, 4, 5, 6} \qquad \text{and} \\
E &= \set{\set{1,2}, \set{2,3}, \set{3,4}, \set{4,5}, \set{5,6},
          \set{6,1}, \set{1,4}, \set{2,5}, \set{3,6}} .
\end{align*}
(This graph can be drawn in such a way as to look like a hexagon with
its three longest diagonals:
\begin{align*}
\xymatrix{
& 1 \are[r] \are[rdd] & 2 \are[rd] \are[ldd] \\
6 \are[ru] \are[rrr] & & & 3 \are[ld] \\
& 5 \are[lu] & 4 \are[l]
} .
\end{align*}
)

This $G$ does not satisfy the conditions of
Theorem~\ref{thm.intro.mantel}, because its number of edges is
$\abs{\edges{G}} = \abs{E} = 9 = 6^2 / 4$ (whereas
Theorem~\ref{thm.intro.mantel} requires it to be $> 6^2 / 4$).
Therefore, it should come as no surprise that there exist no three
distinct vertices $a$, $b$ and $c$ of $G$ such that $ab$, $bc$ and
$ca$ are edges of $G$. (There is a particularly quick way to convince
yourself of this: Observe that if $u$ and $v$ are two adjacent
vertices of $G$, then the integer $u+v$ is odd. Therefore, if there
were three distinct vertices $a$, $b$ and $c$ of $G$ such that $ab$,
$bc$ and $ca$ are edges of $G$, then the integers $a+b$, $b+c$ and
$c+a$ would all be odd, and therefore their sum
$\tup{a+b} + \tup{b+c} + \tup{c+a}$ would be odd as well. But this
is impossible, since this sum is even (in fact, it equals
$2\tup{a+b+c}$). Hence, no such three vertices $a$, $b$ and $c$
exist.)

However, if we add any new edge to $G$, then the resulting graph $G'$
satisfies $\abs{\edges{G'}} = 9 + 1 > 6^2 / 4$, and therefore
Theorem~\ref{thm.intro.mantel} can be applied to $G'$ instead of $G$.
Hence, there exist three distinct vertices $a$,
$b$ and $c$ of $G'$ such that $ab$, $bc$ and $ca$ are edges of $G'$.
Which three vertices they will be depends on which edge we have
added. For example, if we have added the edge $15$, then (for example)
the three vertices $a = 1$, $b = 5$ and $c = 6$ work. Here is a
drawing of this $G'$:
\begin{align*}
\xymatrix{
& 1 \are[r] \are[rdd] \are[dd] & 2 \are[rd] \are[ldd] \\
6 \are[ru] \are[rrr] & & & 3 \are[ld] \\
& 5 \are[lu] & 4 \are[l]
} .
\end{align*}

Of course, the converse of Theorem~\ref{thm.intro.mantel} does not
hold. There are many simple graphs $G$ which do not satisfy the
assumption of Theorem~\ref{thm.intro.mantel}, but nevertheless have
three distinct vertices $a$, $b$ and $c$
such that $ab$, $bc$ and $ca$ are edges of $G$. The reader can easily
construct such a graph.
\end{example}

For proofs of Theorem~\ref{thm.intro.mantel}, see (for example) the
rather well-explained \cite{Choo16} (note that the fourth proof is
incomplete, as the existence of the maximum $S^*$ needs to be proven).

\begin{todo}
Proof.
\end{todo}

\begin{remark}
Let us contrast Proposition~\ref{prop.simple.R33} with
Theorem~\ref{thm.intro.mantel}. The former guarantees that a graph
$G$ with sufficiently many vertices (namely, at least $6$ vertices)
must have a triangle or an anti-triangle (where we are using the
terminology from Exercise~\ref{exa.simple.R33.two}).
Theorem~\ref{thm.intro.mantel} says that any graph $G$ with
sufficiently many edges (namely, more than $n^2 / 4$ edges, where
$n = \abs{\verts{G}}$) must have a triangle. These two facts are
similar and yet different in nature (e.g., the number $6$ in
Proposition~\ref{prop.simple.R33} is a constant, whereas the $n^2 / 4$
in Theorem~\ref{thm.intro.mantel} is not). We can also wonder how many
edges a graph must have in order to be guaranteed an anti-triangle;
this is answered by Exercise~\ref{exe.intro.mantel-co} below.
\end{remark}

\begin{exercise} \label{exe.intro.mantel-co}
Let $G$ be a simple graph. Let $n = \abs{\verts{G}}$ be the number of
vertices of $G$. Assume that $\abs{\edges{G}} < n\tup{n-2} / 4$. (In
other words, assume that $G$ has less than $n\tup{n-2} / 4$ edges.)
Prove that there exist three distinct vertices $a$, $b$ and $c$ of $G$
such that none of $ab$, $bc$ and $ca$ are edges of $G$.
\end{exercise}

Just as Proposition~\ref{prop.simple.R33} is merely the tip of a deep
iceberg called Ramsey theory, Theorem~\ref{thm.intro.mantel} is the
beginning of a longer story. The most famous piece of this story is
the following fact, known as \textit{Tur\'an's theorem}:

\begin{theorem} \label{thm.intro.turan}
Let $r$ be a positive integer.
Let $G$ be a simple graph. Let $n = \abs{\verts{G}}$ be the number of
vertices of $G$. Assume that
$\abs{\edges{G}} > \dfrac{r-1}{r} \cdot \dfrac{n^2}{2}$. Then, there
exist $r + 1$ distinct vertices
of $G$ that are mutually adjacent (i.e., each two distinct ones among
these $r + 1$ vertices are adjacent).
\end{theorem}

Theorem~\ref{thm.intro.mantel} is the particular case of
Theorem~\ref{thm.intro.turan} for $r = 2$. See
\cite[Chapter 4, Theorem 4.8]{Jukna11} or
\cite{Aigner95} or \cite[Chapter 41]{AigZie} for proofs of
Theorem~\ref{thm.intro.turan}.
Various proofs are also sketched in
\url{https://en.wikipedia.org/wiki/Tur%C3%A1n's_theorem} .

Results like Theorem~\ref{thm.intro.mantel} (and sometimes also like
Proposition~\ref{prop.simple.Rrs}) are commonly regarded as part of
a subject called \textit{extremal graph theory}. (The word
``extremal'' refers to the appearance of bounds, such as the
$\dfrac{r-1}{r} \cdot \dfrac{n^2}{2}$.) There are textbooks on this
subject, such as \cite{Jukna11}.

\subsection{\label{sect.intro.iso}Graph isomorphisms}

Two graphs can be distinct and yet ``the same up to the names of their
vertices''. For instance, the two graphs
$\tup{\set{1,2,3},\set{\set{1,2}}}$ and
$\tup{\set{1,2,3},\set{\set{1,3}}}$ are distinct (since $12$ is an
edge of the former graph but not of the latter), but if we rename the
vertices $2$ and $3$ of the former graph as $3$ and $2$, respectively,
then it becomes the latter graph. This kind of relation between two
graphs is weaker than (literal) equality, but still strong enough to
ensure that (roughly speaking) the two graphs have the same properties
(as long as the properties don't refer to specific vertices). Thus,
it is worth giving this relation a rigorous definition and a name:

\begin{definition} \label{def.intro.iso}
Let $G$ and $H$ be two simple graphs.

\begin{enumerate}

\item[\textbf{(a)}] A \textit{graph isomorphism} from $G$ to $H$ means
a bijection $\phi : \verts{G} \to \verts{H}$ such that for every two
vertices $u$ and $v$ of $G$, the following logical equivalence holds:
\begin{equation}
\left( uv \in \edges{G} \right)
\Longleftrightarrow
\left( \phi\tup{u}\phi\tup{v} \in \edges{H} \right) .
\label{eq.def.intro.iso.a.eq}
\end{equation}
(At this point, let me remind you that $uv$ is shorthand for
$\set{u,v}$, and similarly $\phi\tup{u}\phi\tup{v}$ is shorthand for
$\set{\phi\tup{u},\phi\tup{v}}$.)

When it is clear what we mean, we shall abbreviate
``graph isomorphism'' as ``\textit{isomorphism}''. We also often write
``graph isomorphism $G \to H$'' for ``graph isomorphism from $G$ to
$H$''.

\item[\textbf{(b)}] If there exists a graph isomorphism from $G$ to
$H$, then we say the graphs $G$ and $H$ are \textit{isomorphic}, and
we write $G \cong H$. Sometimes, the relation $G \cong H$ itself will
be called an ``isomorphism'' (although it is not a map).

\end{enumerate}

\end{definition}

Given this definition, we can now rigorously state the relation
between the two graphs $\tup{\set{1,2,3},\set{\set{1,2}}}$ and
$\tup{\set{1,2,3},\set{\set{1,3}}}$ we observed above. Namely, if we
denote these two graphs by $G$ and $H$, respectively, then the map
$\set{1,2,3} \to \set{1,2,3}$ that sends $1,2,3$ to $1,3,2$
(respectively) is a graph isomorphism from $G$ to $H$. Thus, these two
graphs are isomorphic. Note that we did not have to pick two graphs
with the same vertex set; isomorphisms are often observed between
graphs with completely different vertex sets and different
definitions. Also, it sometimes happens that there are several
isomorphisms between two graphs.

\begin{example} \label{exa.intro.iso.pentagon}
Consider the graph $G = \tup{V, E}$, where
\begin{align*}
V &= \set{1, 2, 3, 4, 5} \qquad \text{and} \\
E &= \set{\set{1,2}, \set{2,3}, \set{3,4}, \set{4,5}, \set{5,1}} .
\end{align*}
(This graph has already been introduced in
Example~\ref{exa.simple.R33} \textbf{(e)}. It can be drawn to look
like a pentagon.)

Consider furthermore the graph $H = \tup{V, F}$, where $V$ is as
before, and where
\begin{align*}
F &= \set{\set{1,3}, \set{2,4}, \set{3,5}, \set{4,1}, \set{5,2}} .
\end{align*}
(If we draw the vertices $1,2,3,4,5 \in V$ as the five vertices of a
pentagon, then the graph $H$ is the pentagram formed by the diagonals
of this pentagon:
\begin{align*}
\xymatrix{
& 2 \are[rrrd] \are[rdd] & & 3 \are[llld] \are[ldd] \\
1 \are[rrrr] & & & & 4 \\
& & 5
}
\end{align*}
.)

The graphs $G$ and $H$ are distinct, yet isomorphic. One isomorphism
$G \to H$ is the bijection $V \to V$ sending $1,2,3,4,5$ to
$1,3,5,2,4$, respectively. Another is the bijection $V \to V$ sending
$1,2,3,4,5$ to $4,2,5,3,1$, respectively. There are $10$ isomorphisms
in total.
\end{example}

One way to quickly (visually) observe that two graphs are isomorphic
is to draw them in such a way that the drawings look exactly the same
but for the names of the vertices. Given such drawings, one can simply
``overlay them'' to obtain a graph isomorphism.
For example, we could have spotted the isomorphism $G \to H$ in
Example~\ref{exa.intro.iso.pentagon} by drawing the graph $H$ as
follows:
\begin{align*}
\xymatrix{
& 2 \are[rr] & & 5 \are[rd] \\
4 \are[ru] \are[rrd] & & & & 3 \are[lld] \\
& & 1
}
\end{align*}
and overlaying this with the drawing of $G$ made in
Example~\ref{exa.simple.R33} \textbf{(e)}.
(Of course, such drawings are not always easy to find: For example,
our first drawing of $H$ in Example~\ref{exa.intro.iso.pentagon}
looked completely different from our drawing of $G$.)

\begin{example}
The graph $G$ constructed in Example~\ref{exa.simple.R33} \textbf{(a)}
is isomorphic to the graph $G$
constructed in Example~\ref{exa.simple.R33} \textbf{(b)}, even though
this might not be obvious from their definitions. Probably the easiest
way to see this is to draw the latter graph $G$ as follows:
\begin{align*}
\xymatrix{
& 0 \are[r] & 2 \are[rd] \\
1 \are[ru] & & & -2 \are[ld] \\
& -1 \are[lu] & 3 \are[l]
} .
\end{align*}
This drawing makes it clear that it is isomorphic to the former graph
(after all, the picture is exactly the same; it's just that the
vertices are different numbers now). Formally speaking, the map
sending $1, 2, 3, 4, 5, 6$ to $0, 2, -2, 3, -1, 1$ (respectively) is
a graph isomorphism from the former graph to the latter graph.
\end{example}

Any isomorphism between two graphs is invertible (since, by its
definition, it is a bijection). Its inverse, too, is an isomorphism:

\begin{proposition} \label{prop.intro.iso.inverse}
Let $G$ and $H$ be two simple graphs. Let $\phi$ be a graph
isomorphism from $G$ to $H$. Then, its inverse $\phi^{-1}$ is a graph
isomorphism from $H$ to $G$.
\end{proposition}

\begin{proof}[Proof of Proposition~\ref{prop.intro.iso.inverse}.]
The map $\phi$ is a graph isomorphism from $G$ to $H$. By the
definition of ``graph isomorphism'', this means that $\phi$ is a
bijection $\verts{G} \to \verts{H}$ such that for every two
vertices $u$ and $v$ of $G$, the following logical equivalence holds:
\begin{equation}
\left( uv \in \edges{G} \right)
\Longleftrightarrow
\left( \phi\tup{u}\phi\tup{v} \in \edges{H} \right) .
\label{pf.prop.intro.iso.inverse.1}
\end{equation}
Now, for every two vertices $u$ and $v$ of $H$, the following
equivalence holds:
\begin{equation}
\left( uv \in \edges{H} \right)
\Longleftrightarrow
\left( \phi^{-1}\tup{u}\phi^{-1}\tup{v} \in \edges{G} \right)
\label{pf.prop.intro.iso.inverse.2}
\end{equation}
(because we have the following chain of equivalences:
\begin{align*}
& \left( \phi^{-1}\tup{u}\phi^{-1}\tup{v} \in \edges{G} \right)
\\
& \Longleftrightarrow
\left( \underbrace{\phi\tup{\phi^{-1}\tup{u}}}_{=u}
   \underbrace{\phi\tup{\phi^{-1}\tup{v}}}_{=v} \in \edges{H} \right) \\
&\qquad
\left( \text{by \eqref{pf.prop.intro.iso.inverse.1}, applied to }
\phi^{-1}\tup{u} \text{ and } \phi^{-1}\tup{v} \text{ instead of } u
\text{ and } v \right) \\
& \Longleftrightarrow
\left( uv \in \edges{H} \right)
\end{align*}
). Thus, $\phi^{-1}$ is a bijection $\verts{H} \to \verts{G}$ such
that for every two vertices $u$ and $v$ of $H$, the equivalence
\eqref{pf.prop.intro.iso.inverse.2} holds. By the definition of
``graph isomorphism'', this means precisely that $\phi^{-1}$ is a
graph isomorphism from $H$ to $G$. This proves
Proposition~\ref{prop.intro.iso.inverse}.
\end{proof}

The following fact is similarly easy to check:

\begin{proposition} \label{prop.intro.iso.comp}
Let $G$, $H$ and $I$ be three simple graphs. Let $\phi$ be a graph
isomorphism from $G$ to $H$. Let $\psi$ be a graph isomorphism from
$H$ to $I$. Then, the composition $\psi \circ \phi$ is a graph
isomorphism from $G$ to $I$.
\end{proposition}

As we have already alluded to, isomorphic graphs are ``equal in all
but names'', and thus share all properties that do not depend on the
names of vertices. For example, the following holds:

\begin{proposition} \label{prop.intro.iso.degrees}
Let $G$ and $H$ be two simple graphs. Let $\phi$ be a graph
isomorphism from $G$ to $H$.

\begin{enumerate}

\item[\textbf{(a)}] For every $v \in \verts{G}$, we have
$\deg_G v = \deg_H \tup{\phi\tup{v}}$.

\item[\textbf{(b)}] We have
$\abs{\edges{H}} = \abs{\edges{G}}$.

\end{enumerate}

\end{proposition}

\begin{proof}[Proof of Proposition \ref{prop.intro.iso.degrees}.]
The map $\phi$ is a graph isomorphism from $G$ to $H$. By the
definition of ``graph isomorphism'', this means that $\phi$ is a
bijection $\verts{G} \to \verts{H}$ such that for every two
vertices $u$ and $v$ of $G$, the following logical equivalence holds:
\begin{equation}
\left( uv \in \edges{G} \right)
\Longleftrightarrow
\left( \phi\tup{u}\phi\tup{v} \in \edges{H} \right) .
\label{pf.prop.intro.iso.degrees.1}
\end{equation}

\textbf{(a)} Fix $v \in \verts{G}$. Then, \eqref{eq.def.intro.deg.5}
(applied to $\verts{G}$ and $\edges{G}$ instead of $V$ and $E$)
yields
$\deg_G v = \abs{\set{u \in \verts{G} \ \mid \ uv \in \edges{G}}}$
(since $G = \tup{\verts{G}, \edges{G}}$).
Similarly,
\eqref{eq.def.intro.deg.5} (applied to $H$, $\verts{H}$, $\edges{H}$
and $\phi\tup{v}$ instead of $G$, $V$, $E$ and $v$) yields
\begin{align}
\deg_H \tup{\phi\tup{v}}
&=
\abs{\set{u \in \verts{H} \ \mid \ u \phi\tup{v} \in \edges{H}}}
\nonumber \\
&=
\abs{\set{w \in \verts{H} \ \mid \ w \phi\tup{v} \in \edges{H}}}
\label{pf.prop.intro.iso.degrees.a.1}
\end{align}
(here, we have renamed the index $u$ as $w$).
But recall that $\phi : \verts{G} \to \verts{H}$ is a bijection. Thus,
each $w \in \verts{H}$ can be written uniquely in the form
$\phi\tup{u}$ for some $u \in \verts{G}$. Thus, we can substitute
$\phi\tup{u}$ for $w$ in
$\set{w \in \verts{H} \ \mid \ w \phi\tup{v} \in \edges{H}}$.
We thus find
\[
\set{w \in \verts{H} \ \mid \ w \phi\tup{v} \in \edges{H}}
= \set{\phi\tup{u} \ \mid \ u \in \verts{G} \text{ and }
\phi\tup{u}\phi\tup{v} \in \edges{H}} .
\]
Hence,
\eqref{pf.prop.intro.iso.degrees.a.1} becomes
\begin{align*}
\deg_H \tup{\phi\tup{v}}
&=
\abs{
\underbrace{\set{w \in \verts{H} \ \mid \ w \phi\tup{v}
             \in \edges{H}}}
            _{= \set{\phi\tup{u} \ \mid \ u \in \verts{G}
              \text{ and } \phi\tup{u} \phi\tup{v} \in \edges{H}}}}
\\
&=
\abs{\set{\phi\tup{u} \ \mid \ u \in \verts{G}
          \text{ and } \phi\tup{u} \phi\tup{v} \in \edges{H}}} \\
&=
\abs{\set{u \in \verts{G} \ \mid
          \ \underbrace{\phi\tup{u} \phi\tup{v} \in \edges{H}}
                 _{\substack{\Longleftrightarrow
                     \left( uv \in \edges{G} \right) \\
                     \text{(by \eqref{pf.prop.intro.iso.degrees.1})}}}
          }}
\qquad \left(\text{since } \phi \text{ is a bijection}\right) \\
&= \abs{\set{u \in \verts{G} \ \mid \ uv \in \edges{G}}}
= \deg_G v .
\end{align*}
This proves Proposition~\ref{prop.intro.iso.degrees} \textbf{(a)}.
\medskip

\textbf{(b)}
Define a map $\phi' : \edges{G} \to \edges{H}$ as follows:
Let $e \in \edges{G}$. Write the edge $e$ in the form $uv$ for two
distinct vertices $u$ and $v$. Then, $uv = e \in \edges{G}$. Thus,
\eqref{pf.prop.intro.iso.degrees.1} shows that
$\phi\tup{u}\phi\tup{v} \in \edges{H}$. Moreover, this new edge
$\phi\tup{u}\phi\tup{v}$ is independent of the choice of $u$ and $v$
(as long as $e$ is fixed)\footnote{There are two ways of choosing $u$
and $v$, since the elements of a $2$-element set (like $e$) can be
listed in two different orders. We are claiming that the edge
$\phi\tup{u}\phi\tup{v}$ does not depend on the choice of order. The
slickest way of proving this is to rewrite this edge in a form that
makes no reference to the choice of $u$ and $v$ to begin with.
Namely: The edge $\phi\tup{u}\phi\tup{v}$ can be rewritten as
$\set{\phi\tup{x} \mid x \in e}$ (because
\begin{align*}
\set{\phi\tup{x} \mid x \in e}
&= \set{\phi\tup{x} \mid x \in \set{u,v}}
\qquad \left(\text{since } e = uv = \set{u,v}\right) \\
&= \set{\phi\tup{u},\phi\tup{v}} = \phi\tup{u}\phi\tup{v}
\end{align*}
), and this is clearly independent of the choice of $u$ and $v$.}.
Hence, we can set $\phi'\tup{e} = \phi\tup{u}\phi\tup{v}$. Doing so,
we obtain a map $\phi' : \edges{G} \to \edges{H}$. This map $\phi'$ is
a bijection\footnote{Indeed, the inverse map is constructed in the
same way as $\phi'$, except that $H$, $G$ and $\phi^{-1}$ take the
roles of $G$, $H$ and $\phi$.}. Thus, we have found a bijection from
$\edges{G}$ to $\edges{H}$. Consequently,
$\abs{\edges{H}} = \abs{\edges{G}}$.
This proves Proposition~\ref{prop.intro.iso.degrees} \textbf{(b)}.
\end{proof}

Results similar to Proposition~\ref{prop.intro.iso.degrees} (saying
that isomorphic graphs share the same properties) can be stated easily
for any property of graphs you can imagine (as long as the property
does not depend on the names of the vertices); the proofs are always
straightforward like the one given above (the only idea being to apply
bijectivity of $\phi$ and the equivalence
\eqref{pf.prop.intro.iso.degrees.1} over and over in order to
``transfer knowledge'' from one graph to the other). I shall not
state such results explicitly; instead, I shall merely refer to the
overarching idea that isomorphic graphs share the same properties
whenever it becomes useful.

One use of graph isomorphisms is to ``relabel vertices'' in a proof:
Instead of proving some property of a given graph $G$, we can just as
well prove the same property for a graph isomorphic to $G$ (since
isomorphic graphs share the same properties), i.e., a graph obtained
from $G$ by renaming its vertices. The vertices can be renamed as we
wish; a popular choice is to rename them as $1,2,\ldots,n$ (where
$n = \abs{\verts{G}}$). Formally speaking, this renaming is possible
because of the following fact:

\begin{proposition} \label{prop.intro.iso.rename}
Let $G$ be a simple graph. Let $S$ be a finite set such that
$\abs{S} = \abs{\verts{G}}$. Then, there exists a simple graph $H$
that is isomorphic to $G$ and has vertex set $\verts{H} = S$.
\end{proposition}

\begin{proof}[Proof of Proposition~\ref{prop.intro.iso.rename}.]
We have $\abs{S} = \abs{\verts{G}}$. Thus, there exists a bijection
$\phi : \verts{G} \to S$. Fix such a $\phi$. Define a set
$F$ by
\[
F = \set{ \phi\tup{e} \ \mid \ e \in \edges{G} }
= \set{ \phi\tup{u} \phi\tup{v} \ \mid \ uv \in \edges{G} }
\]
(where $\phi\tup{e}$, as usual, denotes the subset
$\set{\phi\tup{x} \ \mid \ x \in e}$ of $S$). Now, let $H$ be the
simple graph $\tup{S, F}$. Then, $\verts{H} = S$. Hence, $\phi$ is a
bijection $\verts{G} \to \verts{H}$. It is straightforward to check
that $\phi$ is a graph isomorphism from $G$ to $H$
(indeed, the definition of $F$ was tailored precisely to make the
equivalence \eqref{eq.def.intro.iso.a.eq} hold). Thus, $H$ is
isomorphic to $G$. As we have already seen, $H$ has vertex set
$\verts{H} = S$. The proof of Proposition~\ref{prop.intro.iso.rename}
is thus complete.
\end{proof}

There is more to say about graph isomorphisms. For example, we can ask
how to check whether two given graphs $G$ and $H$ are isomorphic.
This is a famous problem in computation, known as the
\textit{graph isomorphism problem}, and has seen recent
progress\footnote{See
\url{https://en.wikipedia.org/wiki/Graph_isomorphism_problem} for a
general overview. On the recent progress, see L\'aszl\`o Babai's
preprint \arxiv{1512.03547}
% Add this to list of references once it's finalized; citing a broken
% proof seems stupid.
for the work and \cite{Klarre17} for an amusing popularization.

In theory, the problem can be solved by brute force (just check all
possible bijections $\verts{G} \to \verts{H}$ for being graph
isomorphisms); but this is highly inefficient (the number of
such bijections becomes forbiddingly large very quickly). In many
cases, two non-isomorphic graphs $G$ and $H$ can be ``told apart'' by
some property that holds for one and not the other (e.g., if their
numbers of edges differ). But in general, a simple and fast algorithm
is not known. The problem is in complexity class NP, and Babai's work
claims to prove a quasipolynomial-time (but very complicated)
algorithm. In practice, software written for solving the graph
isomorphism problem makes tradeoffs between simplicity and
efficiency.}.

A related problem is to count, for a given $n \in \NN$,
the isomorphism classes of graphs with $n$ vertices. In other words:
If we pretend that isomorphic graphs are equal, then how many graphs
are there with $n$ vertices? The number is finite, since each graph
with $n$ vertices is isomorphic to a graph with vertex set
$\set{1,2,\ldots, n}$ (this follows from
Proposition~\ref{prop.intro.iso.rename}, applied to
$S = \set{1,2,\ldots, n}$), and the number of the latter graphs is
clearly finite. Yet, computing this number exactly is hard, and there
does not seem to be a closed-form formula. (There is a whole book
\cite{HarPal73} written about this. See also
\url{http://oeis.org/A000088} for references and small values.)

\subsection{\label{sect.intro.paths}Examples of graphs, and basic
operations}

\subsubsection{Complete and empty graphs}

Let us next define certain classes of graphs, as well as some specific
graphs. These will serve as examples in the first place, but some of
them will also reveal themselves to be useful later.

The simplest kinds of simple graphs are the \textit{complete graphs}
and the \textit{simple graphs}:

\begin{definition} \label{def.intro.complete-graph}
The \textit{complete graph} on a finite set $V$ means
the simple graph $\tup{V, \powset[2]{V}}$. It is the
simple graph with vertex set $V$ in which each two distinct vertices
are adjacent.
For each $n \in \NN$, we denote the complete graph on
$\set{1,2,\ldots,n}$ by $K_n$. Often, any graph that is isomorphic to
$K_n$ is called ``a $K_n$'' (with the indefinite article, to signify
that it isn't necessarily the exact graph $K_n$, but can be any graph
isomorphic to it).
\end{definition}

\begin{example}
\begin{enumerate}

\item[\textbf{(a)}]
Here is a drawing of the complete graph on the finite set
$\set{0, 3, 6}$:
\begin{align*}
\xymatrix{
& 0 \are[ld] \are[rd] \\
3 \are[rr] & & 6
} .
\end{align*}
Of course, many other drawings are possible.

\item[\textbf{(b)}]
The complete graph on the set $\set{1, 2, 3, 4, 5}$ is the graph
$\tup{\set{1, 2, 3, 4, 5}, \powset[2]{\set{1, 2, 3, 4, 5}}}$, which
we have drawn in Example~\ref{exa.intro.draw} \textbf{(c)}.
We are calling this graph $K_5$
now, according to Definition~\ref{def.intro.complete-graph}.

\end{enumerate}
\end{example}

Note that a simple graph $G = \tup{V, E}$ is a $K_n$ (i.e., is
isomorphic to $K_n$) if and only if it is the complete graph on $V$
and satisfies $\abs{V} = n$.

If $V$ is a finite set, then each vertex $v$ of the complete graph on
$V$ has degree $\deg v = \abs{V}-1$, because it is adjacent to the
$\abs{V}-1$ remaining vertices of the complete graph.

\begin{definition} \label{def.intro.empty-graph}
The \textit{empty graph} on a finite set $V$ means
the simple graph $\tup{V, \varnothing}$. It has no edges; thus, it is
the simple graph with vertex set $V$ in which no two vertices are
adjacent.
\end{definition}

The empty graphs are, in a sense, the opposite to the complete graphs:
In a complete graph, each two distinct vertices are adjacent, whereas
in an empty graph, no two vertices are adjacent.

\begin{example}
Here is a drawing of the empty graph on the finite set
$\set{0, 3, 6}$:
\begin{align*}
\xymatrix{
& 0 \\
3 & & 6
} .
\end{align*}
\end{example}


\begin{remark}
Let $V$ be a finite set. Let $G$ be the complete graph on this set
$V$. Then, each permutation of $V$ is a graph isomorphism $G \to G$.
(This is straightforward to check.) The same holds if $G$ is the
empty graph on $V$ instead of being the complete graph on $V$.
\end{remark}

\subsubsection{Path and cycle graphs}

Next, we define some slightly more interesting graphs:

\begin{definition} \label{def.intro.path-graph}
For each $n \in \NN$, we define the
\textit{$n$-th path graph} to be the simple graph
\begin{align*}
& \tup{\set{1,2,\ldots,n},
       \set{\set{i, i+1} \ \mid \  i \in \set{1,2,\ldots,n-1}}
      } \\
&=
  \tup{\set{1,2,\ldots,n},
       \set{\set{1,2}, \set{2,3}, \ldots, \set{n-1,n}}
      } .
\end{align*}
This graph is denoted by $P_n$. It has $n$ vertices and $n-1$ edges
(unless $n=0$, in which case it has $0$ edges). Again, we say that a
simple graph is ``a $P_n$'' if it is isomorphic to $P_n$.

Here is a drawing of $P_4$:
\begin{align}
\xymatrix{
1 \are[r] & 2 \are[r] & 3 \are[r] & 4
} .
\label{draw.def.intro.path-graph.P4}
\end{align}
Similarly, for each $n \in \NN$, we can draw $P_n$ by placing $n$
equally spaced points $1,2,\ldots,n$ on a straight line (in this order)
and connecting each point $i \in \set{1,2,\ldots,n-1}$ to the next
point $i+1$.
\end{definition}

Path graphs provide a first mildly interesting example of a graph
isomorphism:

\begin{remark}
Let $n \in \NN$.
The map
\[
\set{1,2,\ldots,n} \to \set{1,2,\ldots,n},
\qquad i \mapsto n+1-i
\]
\footnotemark
\ is an isomorphism $P_n \to P_n$. This isomorphism is called the
\textit{reflection} on the path $P_n$.
\end{remark}
\footnotetext{This map
is the permutation of $\set{1,2,\ldots,n}$ that sends the
numbers $1,2,\ldots,n$ to $n,n-1,\ldots,1$, respectively.}

A look at the drawing \eqref{draw.def.intro.path-graph.P4} of $P_4$
should explain why the name ``reflection'' was chosen for the
reflection on the path $P_n$.

\begin{definition} \label{def.intro.cycle-graph}
For each integer $n > 1$, we define the
\textit{$n$-th cycle graph} to be the simple graph
\begin{align*}
& \tup{\set{1,2,\ldots,n},
       \set{\set{i, i+1} \ \mid \  i \in \set{1,2,\ldots,n-1}}
              \cup \set{\set{n, 1}}
      } \\
&=
  \tup{\set{1,2,\ldots,n},
       \set{\set{1,2}, \set{2,3}, \ldots, \set{n-1,n}, \set{n,1}}
      } .
\end{align*}
This graph is denoted by $C_n$. It has $n$ vertices and
$\begin{cases} n, & \text{ if } n \geq 3; \\
               1, & \text{ if } n = 2
 \end{cases}$
edges.
Again, we say that a simple graph is ``a $C_n$'' if it is isomorphic
to $C_n$.

Here is a drawing of $C_6$:
\begin{align}
\xymatrix{
& 1 \are[r] & 2 \are[rd] \\
6 \are[ru] & & & 3 \are[ld] \\
& 5 \are[lu] & 4 \are[l]
} .
\label{draw.def.intro.cycle-graph.C6}
\end{align}
Similarly, we can draw $C_n$ for each $n > 2$ by drawing an
$n$-gon and labelling its vertices by $1,2,\ldots,n$.
(Some authors like to use a regular $n$-gon for this purpose,
but this is of course irrelevant for the purpose of illustrating
the graph.)
\end{definition}

Again, cycle graphs have nontrivial isomorphisms to themselves:

\begin{remark}
Let $n > 1$ be an integer. For each integer $x$, we define
$x \%_1 n$ to be the unique element $p \in \set{1,2,\ldots,n}$
that satisfies $p \equiv x \mod n$.
For each $k \in \ZZ$, the map
\[
\set{1,2,\ldots,n} \to \set{1,2,\ldots,n},
\qquad i \mapsto \tup{i+k} \%_1 n
\]
\footnotemark
\ is an isomorphism $C_n \to C_n$. This isomorphism is called the
\textit{$k$-rotation} on the cycle $C_n$. Note that this $k$-rotation
depends only on the remainder of $k$ modulo $n$, not on the integer
$k$ itself; in total, we thus have $n$ of these $k$-rotations.
Furthermore, there is a reflection on the cycle $C_n$, defined exactly
as the reflection on the path $P_n$. This reflection is also a graph
isomorphism $C_n \to C_n$. Hence, any composition of this
reflection with a $k$-rotation is also a graph isomorphism
$C_n \to C_n$ (because of Proposition~\ref{prop.intro.iso.comp}).
\end{remark}
\footnotetext{If $k \in \set{0, 1, \ldots, n}$, then this map
is the permutation of $\set{1,2,\ldots,n}$ that
sends the numbers $1,2,\ldots,n$ to $k+1,k+2,\ldots,n,1,2,\ldots,k$,
respectively.}

Again, the names ``rotation'' and ``reflection'' reflect the
fact that if the cycle graph $C_n$ is drawn as a regular $n$-gon,
then the $k$-rotation actually corresponds to rotating the $n$-gon
by an angle of $\dfrac{2\pi k}{n}$ around its center, whereas the
reflection corresponds to reflecting the $n$-gon through a certain
line.

\subsubsection{A few more examples}

Here are some further examples of simple graphs, mostly to illustrate
certain ideas.

\Needspace{8pc}
\begin{example} \label{exa.intro.discon-graph}
Let $n \in \NN$. Let $G$ be the graph $\tup{V, E}$, where
$V = \set{1, 2, \ldots, n}$ and
$E = \set{ \set{i, j} \in \powset[2]{V} \ \mid
           \  i \equiv j \mod 3 }$.
For example, if $n = 8$, then $G$ can be drawn as follows:
\begin{align*}
\xymatrix{
1 \are@/_2pc/[rr] \are[r] & 4 \are[r] & 7
& &
2 \are@/_2pc/[rr] \are[r] & 5 \are[r] & 8
& &
3 \are[r] & 6
} . \\
\phantom{a} % extra space to prevent the pictures from bleeding into the text
\end{align*}
If $n = 9$, then $G$ can be drawn as follows:
\begin{align*}
\xymatrix{
1 \are@/_2pc/[rr] \are[r] & 4 \are[r] & 7
& &
2 \are@/_2pc/[rr] \are[r] & 5 \are[r] & 8
& &
3 \are@/_2pc/[rr] \are[r] & 6 \are[r] & 9
} .
\end{align*}
\phantom{a} % extra space to prevent the pictures from bleeding into the text
\par
In both cases, we notice that the graph is ``disconnected'': It
really consists of three smaller graphs, with no edges connecting one
to another. We shall make this more rigorous later (in fact, we will
formalize the concept of ``disconnected'' in
Subsection~\ref{subsect.intro.connected}, and we will formalize the
concept of ``consisting of three smaller graphs'' in
Subsection~\ref{subsect.intro.djun}).
\end{example}

\begin{example} \label{exa.intro.kneser-graph}
If $S$ is a finite set, and if $k \in \NN$, then we let
$KG_{S, k}$ be the simple graph
\[
\tup{\powset[k]{S},
     \set{ \set{I, J} \in \powset[2]{\powset[k]{S}} \ \mid
           \  I \cap J = \varnothing }} .
\]
The vertices of this graph are the $k$-element subsets of $S$, and
two such vertices are adjacent if they (regarded as subsets of $S$)
are disjoint. The graph $KG_{S, k}$ is called the
\textit{$k$-th
\href{https://en.wikipedia.org/wiki/Kneser_graph}{Kneser graph}
of the set $S$}.

The graph $KG_{\set{1,2,3,4,5}, 2}$ is known as the
\textit{\href{https://en.wikipedia.org/wiki/Petersen_graph}{Petersen graph}}.
Here is one way to draw it:
\begin{align*}
\xymatrix@C=4pc{
& & & \set{2, 5} \are[lld] \are[d] \are[rrd] \\
& \set{1, 4} \are[rd] \are[ddd] & & \set{3, 4} \are[ldd] \are[rdd] & & \set{1, 3} \are[ld] \are[ddd] \\
& & \set{2, 3} \are[rr] \are[rrd] & & \set{4, 5} \are[lld] \\
& & \set{1, 2} \are[ld] & & \set{5, 1} \are[rd] \\
& \set{3, 5} \are[rrrr] & & & & \set{2, 4}
}
.
\end{align*}
% (It is usual to draw both the outside pentagon and the inside
% pentagram on this picture as regular, but we have not done so for
% technical reasons.)

For $n \in \NN$ and $k \in \NN$, it is common to denote the Kneser
graph $KG_{\set{1, 2, \ldots, n}, k}$ by $KG_{n, k}$. Thus, the
Petersen graph is $KG_{5, 2}$.
\end{example}

\begin{example} \label{exa.intro.interval-graph-easy}
Let $n \in \NN$ and $k \in \NN$. Define a graph $P_{n, k}$ by
\[
P_{n, k}
= \tup{\set{1, 2, \ldots, n},
       \set{ \set{i, j} \in \powset[2]{\set{1, 2, \ldots, n}} \ \mid
             \  \abs{i-j} \leq k }} .
\]
Thus, the vertices of this graph are $1, 2, \ldots, n$, and two
distinct vertices $i$ and $j$ are adjacent if and only if they
satisfy $\abs{i-j} \leq k$. Hence:
\begin{itemize}
\item The graph $P_{n, 0}$ is the empty graph on the finite set
      $\set{1, 2, \ldots, n}$. (It is empty because no two distinct
      vertices $i$ and $j$ satisfy $\abs{i-j} \leq 0$, and thus no
      two distinct vertices $i$ and $j$ are adjacent.)
\item The graph $P_{n, 1}$ is the path graph $P_n$ (because two
      distinct vertices $i$ and $j$ satisfy $\abs{i-j} \leq 1$ if and
      only if $i$ and $j$ are consecutive integers in some order).
\item Whenever $k \geq n-1$, the graph $P_{n, k}$ is the complete
      graph $K_n$. This is because whenever $k \geq n-1$,
      \textbf{any} pair of distinct vertices $i$ and $j$ of
      $P_{n, k}$ satisfies $\abs{i-j} \leq k$.
\item Here is how $P_{5, 2}$ can be drawn:
      \begin{align*}
      \xymatrix{ 1 \are[r] \are@/_2pc/[rr] & 2 \are[r] \are@/_2pc/[rr] & 3 \are[r] \are@/_2pc/[rr] & 4 \are[r] & 5 } . \\
      \phantom{a} %empty space to keep the graph from bleeding into the text
      \end{align*}
\end{itemize}

\end{example}

\subsubsection{Subgraphs}

If we are given a graph $G$, we can construct a smaller graph
$H$ by picking some of the vertices of $G$ and some of the
edges that connect these vertices, and forgetting the rest.
Such smaller graphs are known as \textit{subgraphs} of $G$.
Here is the formal definition:

\begin{definition} \label{def.intro.subgraph}
Let $G = \tup{V, E}$ be a simple graph.

\begin{enumerate}

\item[\textbf{(a)}] A \textit{subgraph} of $G$ means a simple graph
of the form $H = \tup{W, F}$ with $W \subseteq V$ and
$F \subseteq E$.
In other words, a \textit{subgraph} of $G$ means a simple graph whose
vertices are vertices of $G$ and whose edges are edges of $G$.

\item[\textbf{(b)}] Let $S$ be a subset of $V$. The
\textit{induced subgraph of $G$ on the set $S$} denotes the subgraph
$\tup{S, E \cap \powset[2]{S}}$ of $G$. In other words, it denotes
the subgraph of $G$ whose vertices are the elements of $S$, and whose
edges are precisely those edges of $G$ that connect two vertices in
$S$.

\item[\textbf{(c)}] An \textit{induced subgraph} of $G$ means a
subgraph of $G$ that is the induced subgraph of $G$ on $S$ for some
subset $S$ of $V$.

\end{enumerate}

\end{definition}

The definition of an induced subgraph readily leads to the following
criterion:

\begin{proposition} \label{prop.intro.subgraph.induced-crit}
Let $H$ be a subgraph of a simple graph $G$. Then, $H$ is an induced
subgraph of $G$ if and only if every edge $uv$ of $G$ whose endpoints
$u$ and $v$ both are vertices of $H$ is an edge of $H$.
\end{proposition}

\begin{proof}[Proof sketch for
Proposition~\ref{prop.intro.subgraph.induced-crit}.]
The ``only if'' part follows immediately from the definition of an
``induced subgraph''. The ``if'' part is almost as obvious (show that
$H$ is the induced subgraph of $G$ on $\verts{H}$). The details are
left to the reader.
\end{proof}

\begin{example} \label{exa.intro.subgraph}
Let $n > 1$ be an integer.

\begin{enumerate}

\item[\textbf{(a)}] The path graph $P_n$ (defined in
Definition~\ref{def.intro.path-graph}) is a subgraph of the cycle
graph $C_n$ (defined in Definition~\ref{def.intro.cycle-graph}),
since all vertices of $P_n$ are vertices of $C_n$ and since all edges
of $P_n$ are edges of $C_n$. However, if $n > 2$, then
$P_n$ is not an \textbf{induced} subgraph of $C_n$,
because not every edge $uv$ of $C_n$ whose
endpoints $u$ and $v$ both are vertices of $P_n$ is an edge of $P_n$.
(In fact, the edge $n1$ of $C_n$ is not an edge of $P_n$, although
both its endpoints $n$ and $1$ are vertices of $P_n$.)

\item[\textbf{(b)}] The path graph $P_{n-1}$ is an induced subgraph
of $P_n$ (namely, the induced subgraph of $P_n$ on the set
$\set{1, 2, \ldots, n-1}$).

\item[\textbf{(c)}] Assume that $n > 3$. Then, $C_{n-1}$ is not a
subgraph of $C_n$. Indeed, the edge $\tup{n-1}1$ of $C_{n-1}$ is
not an edge of $C_n$.

\end{enumerate}

\end{example}

\begin{todo}
Pictures for these examples.
\end{todo}

\subsubsection{\label{subsect.intro.djun}Disjoint unions}

Another basic operation that we might want to do with graphs is
taking \textit{disjoint unions}.
The idea behind disjoint unions is simple:
Take two simple graphs and place them ``alongside each other'',
without creating any new edges.
For example, the disjoint union of the two simple graphs
\begin{equation}
\xymatrix{
1 \are[r] \are[rd] & 2 \are[d] \\
& 3
}
\qquad \text{ and } \qquad
\xymatrix{
2 \are[r] \are[d] & 8 \are[d] \\
4 \are[r] & 6
}
\label{eq.djun.example.1.two-graphs}
\end{equation}
should be something like
\begin{equation}
\xymatrix{
1 \are[r] \are[rd] & 2 \are[d] & 2 \are[r] \are[d] & 8 \are[d] \\
& 3 & 4 \are[r] & 6
} .
\label{eq.djun.example.1.wrong}
\end{equation}
However, \eqref{eq.djun.example.1.wrong} does not represent a
valid graph, because a graph cannot have two vertices with the
same ``name'' (or, more precisely: there are two
``vertices $2$'' in \eqref{eq.djun.example.1.wrong},
which belong to different edges; but this makes no sense
in a graph).
So we have to rename the vertices of our original two graphs
in order to ensure that there is no overlap between them.
For example, we could rename each vertex $v$ of the first graph
as $\tup{1, v}$, and each vertex $w$ of the second graph as
$\tup{2, w}$;
the disjoint union would then be
\begin{equation}
\xymatrix{
\tup{1,1} \are[r] \are[rd] & \tup{1,2} \are[d] & \tup{2,2} \are[r] \are[d] & \tup{2,8} \are[d] \\
& \tup{1,3} & \tup{2,4} \are[r] & \tup{2,6}
} ,
\label{eq.djun.example.1.right}
\end{equation}
which is a perfectly well-defined simple graph with $7$
vertices.

This is indeed how we shall define the disjoint union of two
simple graphs.
The formal definition is as follows:

\begin{definition} \label{def.djun.simple}
Let $G = \tup{V, E}$ and $H = \tup{W, F}$ be two simple graphs.
The \textit{disjoint union} of $G$ and $H$ is defined to be
the simple graph $\tup{X, R}$, where
\begin{align*}
X &= \set{ \tup{1, v} \mid v \in V } \cup \set{ \tup{2, w} \mid w \in W }
\qquad \text{ and} \\
R &= \set{ \set{\tup{1, v_1}, \tup{1, v_2}} \mid \set{v_1, v_2} \in E }
     \cup
	 \set{ \set{\tup{2, w_1}, \tup{2, w_2}} \mid \set{w_1, w_2} \in F } .
\end{align*}
(Informally speaking, $X$ is the set consisting of all vertices
$v$ of $G$ renamed as $\tup{1, v}$ and all vertices $w$ of $H$
renamed as $\tup{2, w}$, whereas $R$ is the set of all edges
of $G$ ``transplanted'' onto $X$ (that is, an edge $\set{v_1, v_2}$
becomes $\set{\tup{1, v_1}, \tup{1, v_2}}$) and all edges of $H$
``transplanted'' onto $X$.)

We denote the disjoint union $\tup{X, R}$ of $G$ and $H$ as
$G \sqcup H$.
\end{definition}

For example, if $G$ and $H$ are the two simple graphs
in \eqref{eq.djun.example.1.two-graphs} (in this order),
then their disjoint union $G \sqcup H$ is the simple graph
in \eqref{eq.djun.example.1.right}.

\begin{proposition}
\label{prop.djun.simple.exists}
Let $G$ and $H$ be two simple graphs.

\begin{enumerate}

\item[\textbf{(a)}]
Then, the disjoint union $G \sqcup H$ of $G$ and $H$ is a
well-defined simple graph and satisfies
$\abs{\verts{G \sqcup H}} = \abs{\verts{G}} + \abs{\verts{H}}$
and
$\abs{\edges{G \sqcup H}} = \abs{\edges{G}} + \abs{\edges{H}}$.

\item[\textbf{(b)}]
The simple graphs $G \sqcup H$ and $H \sqcup G$ are
isomorphic.

\end{enumerate}

\end{proposition}

Notice that the simple graphs
$G \sqcup H$ and $H \sqcup G$ in
Proposition~\ref{prop.djun.simple.exists} \textbf{(b)}
are generally not identical; the vertices of the form
$\tup{1, v}$ in the former correspond to the vertices
$\tup{2, v}$ in the latter.

\begin{example}
Let $G$ be the graph
\[
\tup{ \set{1, 2, 3, 4, 5}, \set{13, 24, 45, 25} }.
\]
This can be drawn as follows:
\begin{align*}
\xymatrix{
& 3 \are[dl] & & 2 \are[rd] \are[ldd] \\
1 & & & & 4 \are[lld] \\
& & 5
} \ .
\end{align*}
This picture shows that the graph is ``built from two
disjoint pieces'': One consisting of the vertices $1$
and $3$, and another consisting of the vertices $2$, $4$
and $5$. No edge connects a vertex in one piece with
a vertex in the other.
Formally, this is saying that our graph $G$ is
isomorphic to the disjoint union $H \sqcup K$ of the
graph $H = \tup{ \set{1, 3}, \set{13} }$ and the graph
$K = \tup{ \set{2, 4, 5}, \set{24, 45, 25} }$.
To wit, the disjoint union $H \sqcup K$ can be drawn
as follows:
\begin{align*}
\xymatrix{
& \tup{1,3} \are[dl] & & \tup{2,2} \are[rd] \are[ldd] \\
\tup{1,1} & & & & \tup{2,4} \are[lld] \\
& & \tup{2,5}
} \ .
\end{align*}
The map that sends each vertex $\tup{i, v}$ of $H \sqcup K$
to the vertex $v$ of $G$ is a graph isomorphism.
\end{example}

We can extend Definition~\ref{def.djun.simple} to define
a disjoint union of several graphs:

\begin{definition} \label{def.djun-k.simple}
Let $k$ be a nonnegative integer.
For each $i \in \set{1, 2, \ldots, k}$, let
$G_i = \tup{V_i, E_i}$ be a simple graph.
The \textit{disjoint union} of these $k$ simple graphs
$G_1, G_2, \ldots, G_k$ is defined to be
the simple graph $\tup{X, R}$, where
\begin{align*}
X &= \set{ \tup{i, v} \mid i \in \set{1, 2, \ldots, k}
                           \text{ and } v \in V_i }
\qquad \text{ and} \\
R &= \set{ \set{\tup{i, v_1}, \tup{i, v_2}} \mid
               i \in \set{1, 2, \ldots, k}
                           \text{ and } \set{v_1, v_2} \in E_i } .
\end{align*}
(Informally speaking, $X$ is the set consisting of all vertices
$v_i$ of all graphs $G_i$ renamed as $\tup{i, v_i}$, whereas
$R$ is the set of all edges of all graphs $G_i$ ``transplanted''
onto $X$ (that is, an edge $\set{v_1, v_2}$ of a graph $G_i$
becomes $\set{\tup{i, v_1}, \tup{i, v_2}}$).)

We denote the disjoint union $\tup{X, R}$ of
$G_1, G_2, \ldots, G_k$ as
$G_1 \sqcup G_2 \sqcup \cdots \sqcup G_k$.
\end{definition}

It is easy to see that Definition~\ref{def.djun.simple} is
the particular case of Definition~\ref{def.djun-k.simple}
for $k = 2$.

\Needspace{15pc}
\begin{example}
Let $G_1$, $G_2$ and $G_3$ be the simple graphs
\begin{align*}
G_1 &= \set{ \set{1, 2}, \set{12} }, \\
G_2 &= \set{ \set{1, 2}, \set{12} }, \\
G_3 &= \set{ \set{1, 2, 3, 4}, \set{12, 23, 34, 13, 14} }.
\end{align*}
(Yes, $G_1$ is equal to $G_2$, but this does not matter.)
Here are their drawings:
\begin{align*}
G_1 &= \xymatrix{
1 \are[r] & 2
}
\qquad ; \qquad
G_2 &= \xymatrix{
1 \are[r] & 2
}
\qquad ; \qquad
G_3 &= \xymatrix{
1 \are[r] \are[d] \are[dr] & 2 \are[dl] \\
3 \are[r] & 4
}
\qquad .
\end{align*}
Then, the disjoint union $G_1 \sqcup G_2 \sqcup G_3$ is the
following graph:
\[
\xymatrix{
\tup{1,1} \are[r] & \tup{1,2} & &
\tup{2,1} \are[r] & \tup{2,2} & &
\tup{3,1} \are[r] \are[d] \are[dr] & \tup{3,2} \are[dl] \\
& & & & & & \tup{3,3} \are[r] & \tup{3,4}
}
.
\]
\end{example}

Here are some further examples of simple graphs, mostly to illustrate
certain ideas.

\begin{example}
Recall the graph $G$ from Exercise~\ref{exa.intro.discon-graph},
where $n = 8$. As we recall, $G$ looks as follows:
\begin{align*}
\xymatrix{
1 \are@/_2pc/[rr] \are[r] & 4 \are[r] & 7
& &
2 \are@/_2pc/[rr] \are[r] & 5 \are[r] & 8
& &
3 \are[r] & 6
} . \\
\phantom{a} % extra space to prevent the pictures from bleeding into the text
\end{align*}
As we already noticed, this graph consists of three disjoint
pieces. These three pieces are the graphs
\begin{align*}
& \xymatrix{
G_1 = & 1 \are@/_2pc/[rr] \are[r] & 4 \are[r] & 7\ ;
& &
G_2 = & 2 \are@/_2pc/[rr] \are[r] & 5 \are[r] & 8\ ;
} \\
& \phantom{\int} % extra space to prevent the pictures from bleeding into the text
\\
& \xymatrix{
G_3 = & 3 \are[r] & 6
} .
\end{align*}
Formally speaking, $G$ is isomorphic to the disjoint union
$G_1 \sqcup G_2 \sqcup G_3$. Namely, the map from
$\verts{G_1 \sqcup G_2 \sqcup G_3}$ to $\verts{G}$ that
sends each $\tup{i, v}$ to $v$ is a graph isomorphism.
\end{example}

The disjoint union operation on graphs is associative up
to isomorphism, in the following sense:

\begin{proposition}
\label{prop.djun.ass}
Let $G_1$, $G_2$ and $G_3$ be three simple graphs. Then,
the three graphs
$\tup{G_1 \sqcup G_2} \sqcup G_3$ and
$G_1 \sqcup \tup{G_2 \sqcup G_3}$ and
$G_1 \sqcup G_2 \sqcup G_3$ are isomorphic.
Specifically:

\begin{enumerate}

\item[\textbf{(a)}]
The map from $\verts{\tup{G_1 \sqcup G_2} \sqcup G_3}$
to $\verts{G_1 \sqcup G_2 \sqcup G_3}$ that sends each
$\tup{1, \tup{1, v}}$ to $\tup{1, v}$,
sends each $\tup{1, \tup{2, v}}$ to $\tup{2, v}$,
and sends each $\tup{2, v}$ to $\tup{3, v}$
is a graph isomorphism from
$\tup{G_1 \sqcup G_2} \sqcup G_3$ to
$G_1 \sqcup G_2 \sqcup G_3$.

\item[\textbf{(b)}]
The map from $\verts{G_1 \sqcup \tup{G_2 \sqcup G_3}}$
to $\verts{G_1 \sqcup G_2 \sqcup G_3}$ that sends each
$\tup{1, v}$ to $\tup{1, v}$,
sends each $\tup{2, \tup{1, v}}$ to $\tup{2, v}$,
and sends each $\tup{2, \tup{2, v}}$ to $\tup{3, v}$
is a graph isomorphism from
$G_1 \sqcup \tup{G_2 \sqcup G_3}$ to
$G_1 \sqcup G_2 \sqcup G_3$.

\end{enumerate}

\end{proposition}

The proof of this proposition is a straightforward
matter of checking the definition of a graph isomorphism;
intuitively, the proposition is already obvious enough:
All three graphs $\tup{G_1 \sqcup G_2} \sqcup G_3$ and
$G_1 \sqcup \tup{G_2 \sqcup G_3}$ and
$G_1 \sqcup G_2 \sqcup G_3$ are obtained by
putting the original three graphs $G_1$, $G_2$ and
$G_3$ alongside each other; they differ only in how
the vertices are relabeled.

There are generalizations of Proposition~\ref{prop.djun.ass}
for disjoint unions of more than three graphs. The reader
won't have much trouble stating and proving these.

\subsection{\label{sect.intro.walks}Walks and paths}

Walks and paths are among the most fundamental notions that can be
defined for a graph; a large part of graph theory is concerned with
these notions. Let us now introduce them.

\subsubsection{Definitions and examples}

\begin{definition} \label{def.intro.walks}
Let $G$ be a simple graph.

\begin{enumerate}

\item[\textbf{(a)}] A \textit{walk} (in $G$) means a finite
sequence $\tup{v_0, v_1, \ldots, v_k}$ of vertices of $G$
(with $k \geq 0$) such that all of
$v_0 v_1, v_1 v_2, \ldots, v_{k-1} v_k$ are edges of $G$. (We allow
$k$ to be $0$, in which case the condition that
``$v_0 v_1, v_1 v_2, \ldots, v_{k-1} v_k$ are edges of $G$'' is
vacuously true.)

\item[\textbf{(b)}] If $\mathbf{w} = \tup{v_0, v_1, \ldots, v_k}$
is a walk in $G$, then:

\begin{itemize}
\item The \textit{vertices of $\mathbf{w}$} are defined to be
the vertices $v_0, v_1, \ldots, v_k$.
\item The
\textit{edges of $\mathbf{w}$} are defined to be the edges
$v_0 v_1, v_1 v_2, \ldots, v_{k-1} v_k$ of $G$;
\item The nonnegative integer $k$ is called the
\textit{length} of $\mathbf{w}$. (This integer is the number of all
edges of $\mathbf{w}$, counted with multiplicity. It is $1$ smaller
than the number of all vertices of $\mathbf{w}$, counted with
multiplicity.)
\item The vertex $v_0$ is said to be the \textit{starting point} of
$\mathbf{w}$. We say that the walk $\mathbf{w}$ \textit{starts} at
$v_0$ (or, equivalently, \textit{begins} at $v_0$).
\item The vertex $v_k$ is said to be the \textit{ending point} of
$\mathbf{w}$. We say that the walk $\mathbf{w}$ \textit{ends} at
$v_k$.
\end{itemize}

\item[\textbf{(c)}] A \textit{path} (in $G$) means a walk (in $G$)
whose vertices are distinct. (In other words, a \textit{path} in
$G$ means a walk $\tup{v_0, v_1, \ldots, v_k}$ in $G$ such that
$v_0, v_1, \ldots, v_k$ are distinct.)

\item[\textbf{(e)}] Let $p$ and $q$ be two vertices of $G$. A
\textit{walk from $p$ to $q$} (in $G$) means a walk (in $G$) whose
starting point is $p$ and whose ending point is $q$. A
\textit{path from $p$ to $q$} (in $G$) means a path (in $G$) whose
starting point is $p$ and whose ending point is $q$.

\end{enumerate}

\end{definition}

\begin{example} \label{exa.intro.walks}
Let $V$ and $E$ be as in Example~\ref{exa.simple.R33} \textbf{(c)}.
Let $G$ be the graph $\tup{V, E}$. Then:

\begin{itemize}
\item The sequence $\tup{1, 3, 4, 5, 6, 1, 3, 2}$ of vertices of $G$
is a walk (in $G$), since all of $13$, $34$, $45$, $56$, $61$, $13$,
and $32$ are edges of $G$. This walk is a walk from $1$ to $2$ (since
its starting point is $1$, and its ending point is $2$). It is not a
path (since $1$, $3$, $4$, $5$, $6$, $1$, $3$ and $2$ are not
distinct). The length of this walk is $7$.
% It is not a circuit, and thus not a cycle.
Let us visualize this walk by marking all the edges that it uses with
double arrows:
\begin{align*}
\xymatrix{
& 1 \are[r] \ar@{=>}[rrd] \ar@/_2pc/@{=>}[rrd] & 2 \\
6 \ar@{=>}[ru] & & & 3 \ar@{=>}[ld] \ar@/_1pc/@{=>}[lu] \\
& 5 \ar@{=>}[lu] & 4 \ar@{=>}[l]
} .
\end{align*}

\item The sequence $\tup{1, 2, 4, 3}$ of vertices of $G$ is not a walk
(in fact, $12$ and $43$ are edges of $G$, but $24$ is not). Hence, it
is not a path either.

\item The sequence $\tup{1, 3, 2}$ of vertices of $G$ is a walk, but
also a path (from $1$ to $2$). The length of this path is $2$.

\item The sequence $\tup{1, 3, 2, 1}$ of vertices of $G$ is a walk
(from $1$ to $1$), but not a path (since $1$, $3$, $2$ and $1$ are
not distinct). The length of this walk is $3$.
% a circuit (since $1 = 1$), and a cycle

\item The sequence $\tup{1, 2, 1}$ of vertices of $G$ is a walk
(from $1$ to $1$). It is not a path. The length of this walk is $2$.

\item The sequence $\tup{5}$ of vertices of $G$ is a walk (from $5$
to $5$) and a path as well. The length of this path is $0$.

\item The sequence $\tup{5, 4}$ of vertices of $G$ is a walk (from $5$
to $4$) and a path as well. The length of this path is $1$.
\end{itemize}
\end{example}

\begin{exercise} \label{exe.intro.path.edges-dist}
Let $G$ be a simple graph. Let $\mathbf{w}$ be a path in $G$.
Prove that the edges of $\mathbf{w}$ are distinct. (This may look
obvious when you can point to a picture; but we ask you to give a
rigorous proof!)
\end{exercise}

\subsubsection{Composing and reverting walks}

Let us discuss a few basic properties of walks and paths.
First of all, if $v$ is a vertex of a simple graph $G$, then the
one-element list $\tup{v}$ is a path (in $G$) from $v$ to $v$.
This path has length $0$.
Hence, there exists a path (of length $0$) from each vertex to
itself.
Furthermore, if $\tup{v_0, v_1, \ldots, v_k}$ is a walk in a
simple graph $G$, and if $p$ and $q$ are two elements of
$\set{0, 1, \ldots, k}$ satisfying $p \leq q$, then
$\tup{v_p, v_{p+1}, \ldots, v_q}$ also is a walk in $G$.
The same statement holds if the word ``walk'' is replaced by
``path'' both times.
Simple facts like this will be used without further mention.

Let us now pass to a few facts which are less obvious (although
intuitively clear).
The first says that we can ``splice'' two walks together if
the ending point of the first is the starting point of the
second:

\begin{proposition} \label{prop.walks.concat}
Let $G$ be a simple graph. Let $u$, $v$ and $w$ be three vertices of
$G$.
Let $\mathbf{a} = \tup{a_0, a_1, \ldots, a_k}$ be a walk from $u$ to
$v$. 
Let $\mathbf{b} = \tup{b_0, b_1, \ldots, b_\ell}$ be a walk from $v$
to $w$.
Then,
\begin{align*}
\tup{a_0, a_1, \ldots, a_k, b_1, b_2, \ldots, b_\ell}
&= \tup{a_0, a_1, \ldots, a_{k-1}, b_0, b_1, \ldots, b_\ell} \\
&= \tup{a_0, a_1, \ldots, a_{k-1}, v, b_1, b_2, \ldots, b_\ell}
\end{align*}
is a walk from $u$ to $w$.

This walk shall be denoted by $\mathbf{a} * \mathbf{b}$.
\end{proposition}

\begin{example}
Let $V$ and $E$ be as in Example~\ref{exa.simple.R33} \textbf{(c)}.
Let $G$ be the graph $\tup{V, E}$.
Let $\mathbf{a} = \tup{1, 2, 3}$ and $\mathbf{b} = \tup{3, 1, 6}$.
These are two walks in $G$.
Then, $\mathbf{a} * \mathbf{b} = \tup{1, 2, 3, 1, 6}$.
As Proposition~\ref{prop.walks.concat} claims, this is a walk
from $1$ to $6$ (since $\mathbf{a}$ is a walk from $1$ to $3$,
while $\mathbf{b}$ is a walk from $3$ to $6$).
Note, however, that $\mathbf{a} * \mathbf{b}$ is not a path,
even though $\mathbf{a}$ and $\mathbf{b}$ are paths.
\end{example}

\begin{proof}[Proof of Proposition~\ref{prop.walks.concat}.]
The list $\tup{a_0, a_1, \ldots, a_k}$ is a walk from $u$ to $v$.
Hence, all of $a_0 a_1, a_1 a_2, \ldots, a_{k-1} a_k$ are edges of
$G$, and we have $a_0 = u$ and $a_k = v$.

The list $\tup{b_0, b_1, \ldots, b_\ell}$ is a walk from $v$ to $w$.
Hence, all of $b_0 b_1, b_1 b_2, \ldots, b_{\ell-1} b_\ell$ are edges
of $G$, and we have $b_0 = v$ and $b_\ell = w$.

We need to prove three claims:
\begin{itemize}
\item \textit{Claim 1:} We have
$\tup{a_0, a_1, \ldots, a_k, b_1, b_2, \ldots, b_\ell}
= \tup{a_0, a_1, \ldots, a_{k-1}, b_0, b_1, \ldots, b_\ell}$.
\item \textit{Claim 2:} We have
$\tup{a_0, a_1, \ldots, a_{k-1}, b_0, b_1, \ldots, b_\ell}
= \tup{a_0, a_1, \ldots, a_{k-1}, v, b_1, b_2, \ldots, b_\ell}$.
\item \textit{Claim 3:} The list
$\tup{a_0, a_1, \ldots, a_k, b_1, b_2, \ldots, b_\ell}$ is a walk
from $u$ to $w$.
\end{itemize}

\textit{Proof of Claim 1.} The two lists
$\tup{a_0, a_1, \ldots, a_k, b_1, b_2, \ldots, b_\ell}$
and \newline
$\tup{a_0, a_1, \ldots, a_{k-1}, b_0, b_1, \ldots, b_\ell}$
differ only in their $\tup{k+1}$-st entry, which is $a_k$ for the
first list and is $b_0$ for the second list. But since their
$\tup{k+1}$-st entries are also equal (because $a_k = v = b_0$),
this shows that these two lists are identical. In other words,
$\tup{a_0, a_1, \ldots, a_k, b_1, b_2, \ldots, b_\ell}
= \tup{a_0, a_1, \ldots, a_{k-1}, b_0, b_1, \ldots, b_\ell}$.
Claim 1 is proven.

\textit{Proof of Claim 2.} The two lists
$\tup{a_0, a_1, \ldots, a_{k-1}, b_0, b_1, \ldots, b_\ell}$
and \newline
$\tup{a_0, a_1, \ldots, a_{k-1}, v, b_1, b_2, \ldots, b_\ell}$
differ only in their $\tup{k+1}$-st entry, which is $b_0$ for the
first list and is $v$ for the second list. But since their
$\tup{k+1}$-st entries are also equal (because $b_0 = v$),
this shows that these two lists are identical. In other words,
$\tup{a_0, a_1, \ldots, a_{k-1}, b_0, b_1, \ldots, b_\ell}
= \tup{a_0, a_1, \ldots, a_{k-1}, v, b_1, b_2, \ldots, b_\ell}$.
Claim 2 is proven.

\textit{Proof of Claim 3.} Recall that all of
$b_0 b_1, b_1 b_2, \ldots, b_{\ell-1} b_\ell$ are edges of $G$.
Since $b_0 = v = a_k$, this rewrites as follows: All of
$a_k b_1, b_1 b_2, \ldots, b_{\ell-1} b_\ell$ are edges of $G$.
Combining this with the fact that all of
$a_0 a_1, a_1 a_2, \ldots, a_{k-1} a_k$ are edges of $G$,
we obtain the following: All of
$a_0 a_1, a_1, a_2, \ldots, a_{k-1} a_k,
a_k b_1, b_1 b_2, \ldots, b_{\ell-1} b_\ell$ are edges of $G$. Hence,
the list
$\tup{a_0, a_1, \ldots, a_k, b_1, b_2, \ldots, b_\ell}$ is a walk
(by the definition of ``walk''). Furthermore, this list is actually
a walk from $u$ to $w$ (since $a_0 = u$ and $b_\ell = w$). This
proves Claim 3.

Thus, all three Claims are proven. Together, they yield 
Proposition~\ref{prop.walks.concat}.
\end{proof}

The next easy property of walks is that a walk can be reversed
(i.e., walked backwards from its ending point to its starting
point), and moreover, if the walk is a path, then its reversal
is a path too:

\begin{proposition} \label{prop.walks.rev}
Let $G$ be a simple graph. Let $u$ and $v$ be two vertices of $G$.
Let $\mathbf{a} = \tup{a_0, a_1, \ldots, a_k}$ be a walk from $u$ to
$v$. Then:

\begin{enumerate}

\item[\textbf{(a)}] The list $\tup{a_k, a_{k-1}, \ldots, a_0}$ is
a walk from $v$ to $u$.

This list shall be denoted by $\rev \mathbf{a}$, and called the
\textit{reversal} of $\mathbf{a}$.

\item[\textbf{(b)}] If the walk $\mathbf{a}$ is a path, then
$\rev \mathbf{a}$ is a path as well.

\end{enumerate}
\end{proposition}

\begin{example}
Let $V$ and $E$ be as in Example~\ref{exa.simple.R33} \textbf{(c)}.
Let $G$ be the graph $\tup{V, E}$.
Let $\mathbf{a} = \tup{1, 2, 3, 1, 6}$.
Then, $\rev \mathbf{a} = \tup{6, 1, 3, 2, 1}$.
\end{example}

\begin{proof}[Proof of Proposition~\ref{prop.walks.rev}.]
The list $\tup{a_0, a_1, \ldots, a_k}$ is a walk from $u$ to $v$.
Hence, all of $a_0 a_1, a_1 a_2, \ldots, a_{k-1} a_k$ are edges of
$G$, and we have $a_0 = u$ and $a_k = v$.

\textbf{(a)} All of $a_0 a_1, a_1 a_2, \ldots, a_{k-1} a_k$ are edges
of $G$. In other words,
\begin{equation}
a_i a_{i+1} \text{ is an edge of } G
\qquad \text{ for each } i \in \set{0, 1, \ldots, k-1} .
\label{pf.prop.walks.rev.1}
\end{equation}

Now, $a_{k-j} a_{k-\tup{j+1}}$ is an edge of $G$ for each
$j \in \set{0, 1, \ldots, k-1}$\ \ \ \ \footnote{\textit{Proof.} Let
$j \in \set{0, 1, \ldots, k-1}$. Then,
$k-1-j \in \set{0, 1, \ldots, k-1}$. Hence,
\eqref{pf.prop.walks.rev.1} (applied to $i=k-1-j$) shows that
$a_{k-1-j} a_{\tup{k-1-j}+1}$ is an edge of $G$. Since
$\underbrace{a_{k-1-j}}_{= a_{k-\tup{j+1}}}
\underbrace{a_{\tup{k-1-j}+1}}_{= a_{k-j}}
= a_{k-\tup{j+1}} a_{k-j} = a_{k-j} a_{k-\tup{j+1}}$,
this rewrites as follows: $a_{k-j} a_{k-\tup{j+1}}$ is an edge of $G$.
This completes the proof.}. In other words, all of
$a_k a_{k-1}, a_{k-1} a_{k-2}, \ldots, a_1 a_0$ are edges of $G$.
In other words, the list $\tup{a_k, a_{k-1}, \ldots, a_0}$ is a walk.
This list is furthermore a walk from $v$ to $u$ (since $a_k = v$ and
$a_0 = v$). This proves Proposition~\ref{prop.walks.rev} \textbf{(a)}.

\textbf{(b)} The vertices of the walk $\mathbf{a}$ are
$a_0, a_1, \ldots, a_k$
(since $\mathbf{a} = \tup{a_0, a_1, \ldots, a_k}$),
whereas the vertices of the walk $\rev \mathbf{a}$ are
$a_k, a_{k-1}, \ldots, a_0$
(since $\rev \mathbf{a} = \tup{a_k, a_{k-1}, \ldots, a_0}$). Hence,
the vertices of the walk $\rev \mathbf{a}$ are the same as the
vertices of the walk $\mathbf{a}$, listed in a different order.

Now, assume that $\mathbf{a}$ is a path. Thus, the vertices of the
walk $\mathbf{a}$ are distinct (by the definition of ``path''). In
other words, the vertices of the walk $\rev \mathbf{a}$ are distinct
(since the vertices of the walk $\rev \mathbf{a}$ are the same as the
vertices of the walk $\mathbf{a}$, listed in a different order).
Hence, this walk $\rev \mathbf{a}$ is a path (by the definition of
``path''). This proves Proposition~\ref{prop.walks.rev} \textbf{(b)}.
\end{proof}

\subsubsection{Reducing a walk to a path}

The next result is simple but crucial:

\begin{proposition} \label{prop.walk.decrease}
Let $G$ be a simple graph. Let $u$ and $v$ be two vertices of $G$.
Let $\mathbf{a} = \tup{a_0, a_1, \ldots, a_k}$ be a walk from $u$ to
$v$.
Assume that $\mathbf{a}$ is not a path. Then, there exists a walk
from $u$ to $v$ whose length is smaller than $k$.
\end{proposition}

\begin{example} \label{exa.walk.decrease}
Let $G$ be the graph $\tup{V, E}$, where
\begin{align*}
V &= \set{1, 2, 3, 4, 5, 6} \qquad \text{and} \\
E &= \set{\set{1,2}, \set{2,3}, \set{3,4}, \set{4,5}, \set{5,6},
          \set{6,1}, \set{1,3}, \set{3,5}, \set{5,1}} .
\end{align*}
(This graph can be drawn in such a way as to look like a hexagon with
three extra diagonals forming a triangle.)
The sequence $\tup{4, 3, 2, 1, 3, 6}$ of vertices of $G$ is a walk
from $4$ to $6$. But it is not a path, since it passes through
the vertex $3$ twice (i.e., the vertex $3$ appears twice among its
vertices). Thus, Proposition~\ref{prop.walk.decrease} predicts that
there exists a walk from $4$ to $6$ whose length is smaller than $5$.
And indeed, such a walk can easily be obtained from our walk
$\tup{4, 3, 2, 1, 3, 6}$, just by removing the part between the two
appearances of the vertex $3$ (including one of these two
appearances). The result is the walk $\tup{4, 3, 6}$, whose length
$2$ is indeed smaller than $5$.

The same idea (removing a ``digression'' between two appearances of
the same vertex) can be used to prove
Proposition~\ref{prop.walk.decrease} in full generality. This is what
we shall do in the proof below.
\end{example}

\begin{proof}[Proof of Proposition~\ref{prop.walk.decrease}.]
Recall that $\tup{a_0, a_1, \ldots, a_k} = \mathbf{a}$ is a walk
from $u$ to $v$. Hence, $a_0 = u$ and $a_k = v$.

If the vertices of $\mathbf{a}$ were distinct, then the walk
$\mathbf{a}$ would be a path (by the definition of ``path''). Thus,
the vertices of $\mathbf{a}$ are not distinct (since $\mathbf{a}$ is
not a path). In other words, $a_0, a_1, \ldots, a_k$ are not
pairwise distinct (since the vertices of $\mathbf{a}$ are
$a_0, a_1, \ldots, a_k$). In other words, there exist two elements
$p$ and $q$ of $\set{0, 1, \ldots, k}$ such that $p < q$ and
$a_p = a_q$. Fix such $p$ and $q$. Define a vertex $x$ of $G$ by
$x = a_p = a_q$. (This is possible since $a_p = a_q$.)

Recall that $\tup{a_0, a_1, \ldots, a_k}$ is a walk. Hence, the list
$\tup{a_0, a_1, \ldots, a_p}$ is a walk as well. Moreover, this list
$\tup{a_0, a_1, \ldots, a_p}$ is a walk from $u$ to $x$ (since
$a_0 = u$ and $a_p = x$). Denote this walk by $\mathbf{p}$.

Again, recall that $\tup{a_0, a_1, \ldots, a_k}$ is a walk. Hence, the
list $\tup{a_q, a_{q+1}, \ldots, a_k}$ is a walk as well. Moreover,
this list $\tup{a_q, a_{q+1}, \ldots, a_k}$ is a walk from $x$ to $v$
(since $a_q = x$ and $a_k = v$). Denote this walk by $\mathbf{q}$.

Altogether, we know that $u$, $x$ and $v$ are three vertices of $G$;
we know that $\mathbf{p} = \tup{a_0, a_1, \ldots, a_p}$ is a walk from
$u$ to $x$; and we know that
$\mathbf{q} = \tup{a_q, a_{q+1}, \ldots, a_k}$ is a walk from $x$ to
$v$.
Hence, we can apply Proposition~\ref{prop.walks.concat} to
$x$, $v$, $\mathbf{p}$, $p$, $a_i$, $\mathbf{q}$, $k-q$ and $a_{q+j}$
instead of $v$, $w$, $\mathbf{a}$, $k$, $a_i$, $\mathbf{b}$, $\ell$
and $b_j$. We thus conclude that
\begin{align*}
\tup{a_0, a_1, \ldots, a_p, a_{q+1}, a_{q+2}, \ldots, a_k}
&= \tup{a_0, a_1, \ldots, a_{p-1}, a_q, a_{q+1}, \ldots, a_k} \\
&= \tup{a_0, a_1, \ldots, a_{p-1}, x, a_{q+1}, a_{q+2}, \ldots, a_k}
\end{align*}
is a walk from $u$ to $v$. This walk has length smaller than $k$
(in fact, its length is
$\tup{p-1} + 2 + \tup{k-q-1} = \underbrace{p}_{< q} + k - q
< q + k - q = k$). Hence, there exists a walk from $u$ to $v$ whose
length is smaller than $k$ (namely, the walk we have just
constructed).
This proves Proposition~\ref{prop.walk.decrease}.
\end{proof}

From Proposition~\ref{prop.walk.decrease}, we obtain the following
corollary:

\begin{corollary} \label{cor.walk.path}
Let $G$ be a simple graph. Let $u$ and $v$ be two vertices of $G$.
Let $k \in \NN$.
Assume that there exists a walk from $u$ to $v$ of length $k$.
Then, there exists a path from $u$ to $v$ of length $\leq k$.
\end{corollary}

\begin{example} \label{exa.walk.path}
Let $V$, $E$ and $G$ be as in Example~\ref{exa.intro.walks}. Consider
the walk $\mathbf{a} = \tup{4, 3, 2, 1, 3, 4, 3, 1, 5}$ from $4$ to
$5$, having length $8$. Corollary~\ref{cor.walk.path} predicts that
there exists a path from $4$ to $5$ of length $\leq 8$. This is, of
course, obvious (the path $\tup{4, 5}$ works), but let us try to
construct such a path from $\mathbf{a}$ by removing ``digressions''
as we did in Exercise~\ref{exa.walk.decrease}:

\begin{itemize}
\item One way to start is by noticing that the vertex $3$ appears
  three times in our walk, and removing the ``digression'' that the
  walk makes between the first and the second appearance of $3$. The
  result is the walk $\tup{4, 3, 4, 3, 1, 5}$. This latter walk is
  still not a path, since the vertex $4$ appears in it twice. Again,
  remove the ``digression'' to obtain the shorter walk
  $\tup{4, 3, 1, 5}$. This new walk is a path and has length
  $3 \leq 8$. So we have found a path from $4$ to $5$ of length
  $\leq 8$.
\item An alternative way to start is by noticing that the vertex $1$
  appears twice in our walk, and removing the ``digression'' that the
  walk makes between its two appearances. The result of this removal
  is the walk $\tup{4, 3, 2, 1, 5}$, which is already a path and has
  length $4 \leq 8$. So we have found a path from $4$ to $5$ of length
  $\leq 8$.
\item Yet another way to start is by noticing (again) that the vertex
  $3$ appears three times in our walk, and removing the ``digression''
  between its first and third appearances. The result is the path
  $\tup{4, 3, 1, 5}$.
\item There are two other possible ways to start; do you see them?
\end{itemize}

Note that the different ways to remove ``digressions'' don't always
lead to the same path in the end.

Corollary~\ref{cor.walk.path} can be proved in full generality using
this method. Our proof below will do exactly that.
\end{example}

\begin{proof}[Proof of Corollary~\ref{cor.walk.path}.]
The idea behind this proof is the following:
Proposition~\ref{prop.walk.decrease} shows that, whenever we have a
walk from $u$ to $v$ that is not a path, we can ``shorten it'' (i.e.,
find a shorter walk from $u$ to $v$).
Thus, we can start with any walk from $u$ to $v$ of length $k$ (its
existence is guaranteed by the hypothesis), and keep ``shortening it''
until it becomes a path (which has to happen eventually, because we
cannot keep shortening it indefinitely). The result will be a path
from $u$ to $v$ of length $\leq k$.

This idea can be formalized to give a rigorous proof. The way to
formalize it is fairly standard and straightforward, so I am only
going to show it now, but leave it to the reader in similar situations
that may appear further below.

Our rigorous proof of Corollary~\ref{cor.walk.path} shall proceed by
strong induction over $k$. So we fix an integer $\ell \in \NN$, and
we assume that Corollary~\ref{cor.walk.path} holds whenever
$k < \ell$. We then must prove that Corollary~\ref{cor.walk.path}
holds when $k = \ell$. (This is the induction step. Note that a strong
induction needs no induction base, because the induction step includes
the case $\ell = 0$ already.)

We have assumed that Corollary~\ref{cor.walk.path} holds whenever
$k < \ell$. In other words, the following claim holds:

\begin{statement}
\textit{Claim 1:} Let $G$ be a simple graph. Let $u$ and $v$ be two
vertices of $G$.
Let $k \in \NN$ be such that $k < \ell$.
Assume that there exists a walk from $u$ to $v$ of length $k$.
Then, there exists a path from $u$ to $v$ of length $\leq k$.
\end{statement}

Our goal is to prove that Corollary~\ref{cor.walk.path}
holds when $k = \ell$. In other words, our goal is to prove the
following claim:

\begin{statement}
\textit{Claim 2:} Let $G$ be a simple graph. Let $u$ and $v$ be two
vertices of $G$.
Let $k \in \NN$ be such that $k = \ell$.
Assume that there exists a walk from $u$ to $v$ of length $k$.
Then, there exists a path from $u$ to $v$ of length $\leq k$.
\end{statement}

\textit{Proof of Claim 2:} We have assumed that there exists a walk
from $u$ to $v$ of length $k$. Fix such a walk, and denote it by
$\mathbf{a}$. Write this walk $\mathbf{a}$ in the form
$\mathbf{a} = \tup{a_0, a_1, \ldots, a_k}$. (This is possible, since
$\mathbf{a}$ has length $k$.) We are in one of the following two
cases:

\begin{itemize}
\item \textit{Case 1:} The walk $\mathbf{a}$ is a path.
\item \textit{Case 2:} The walk $\mathbf{a}$ is not a path.
\end{itemize}

Let us consider Case 1 first. In this case, the walk $\mathbf{a}$ is a
path. Thus, $\mathbf{a}$ is a path from $u$ to $v$ (since $\mathbf{a}$
is a walk from $u$ to $v$), and has length $\leq k$ (since
$\mathbf{a}$ has length $k$). Hence, there exists a path from $u$ to
$v$ of length $\leq k$ (namely, $\mathbf{a}$). Thus, Claim 2 is proven
in Case 1.

Let us now consider Case 2. In this case, the walk $\mathbf{a}$ is not
a path. Hence, Proposition~\ref{prop.walk.decrease} shows that there
exists a walk from $u$ to $v$ whose length is smaller than $k$.
Fix such a walk, and denote it by $\mathbf{b}$. Thus, $\mathbf{b}$ is
a walk from $u$ to $v$ whose length is smaller than $k$.

Let $r$ be the length of $\mathbf{b}$. Then, $r < k$ (since the length
of $\mathbf{b}$ is smaller than $k$). Hence, $r < k = \ell$. Now,
there exists a walk from $u$ to $v$ of length $r$ (namely,
$\mathbf{b}$). Hence, Claim 1 (applied to $r$ instead of $k$) shows
that there exists a path from $u$ to $v$ of length $\leq r$.
This path has length $\leq r < k$; hence, it has length $\leq k$.
Therefore, there exists a path from $u$ to $v$ of length $\leq k$
(namely, the path that we have just constructed). Hence, Claim 2 is
proven in Case 2.

Now, we have proven Claim 2 in both possible cases. Therefore, the
proof of Claim 2 is complete. In other words,
Corollary~\ref{cor.walk.path} holds when $k = \ell$. This completes
the (inductive) proof of Corollary~\ref{cor.walk.path}.
\end{proof}

\subsubsection{The equivalence relation $\simeq_G$}

We can use the concept of paths (and the results proven above) to
define a certain equivalence relation on the vertex set of a graph
$G$. First, let us recall what relations and equivalence relations
are:

\begin{definition} \label{def.intro.relation}
Let $X$ be a set. A \textit{binary relation} on $X$ shall mean a
subset of the Cartesian product $X \times X$. (In other words, it
shall mean a set of pairs of elements of $X$.)

If $R$ is a binary relation on $X$, and if $a$ and $b$ are two
elements of $X$, then we shall often use the notation $a R b$ as a
synonym for $\tup{a, b} \in R$. This notation is called
\textit{infix notation}. For some relations $R$, it is usual to always
use this notation (i.e., one never writes $\tup{a, b} \in R$, but
always writes $a R b$ instead); in this case, we say that the relation
$R$ is \textit{written infix}.
\end{definition}

Examples of binary relations on the set $\ZZ$ are $=$, $\leq$, $<$,
$\geq$, $>$ and $\neq$. (These are all written infix: For example, we
write $2 < 4$, but we don't write $\tup{2, 4} \in <$.)

Now, what is an equivalence relation?

\begin{definition} \label{def.intro.equiv-rel}
Let $R$ be a binary relation on a set $X$. This relation $R$ shall be
written infix throughout this definition.

\textbf{(a)} We say that $R$ is \textit{reflexive} if it has the
following property: For each $x \in X$, we have $x R x$.

\textbf{(b)} We say that $R$ is \textit{symmetric} if it has the
following property: For each $x \in X$ and $y \in X$ satisfying
$x R y$, we have $y R x$.

\textbf{(c)} We say that $R$ is \textit{transitive} if it has the
following property: For each $x \in X$, $y \in X$ and $z \in X$
satisfying $x R y$ and $y R z$, we have $x R z$.

\textbf{(d)} We say that $R$ is an \textit{equivalence relation} if it
is reflexive, symmetric and transitive.
\end{definition}

We refer to \cite[\S 10.10]{LeLeMe16}, or \cite[Chapter 11]{Hammac15}
(for a particularly detailed treatment),
or \cite[Chapter 9]{Oggier14}, or \cite[\S 3.E]{Day-proofs}
% and other sources?
for more about equivalence relations. Let us here just briefly
mention a few examples:
\begin{itemize}
\item Among the six relations $=$, $\leq$, $<$, $\geq$, $>$ and $\neq$
on the set $\ZZ$, only the first one ($=$) is an equivalence relation.
However, $\leq$ and $\geq$ are reflexive and transitive (but not
symmetric), and $<$ and $>$ are transitive. Furthermore, $\neq$ is
symmetric.
\item For each positive integer $m$, we can define a binary relation
$\overset{m}{\equiv}$ on $\ZZ$ (written infix) by setting
\[
a \overset{m}{\equiv} b
\qquad \text{ if and only if } \qquad
a \equiv b \mod m .
\]
This is an equivalence relation (for each $m$).
\item Here is a fairly general example: If $X$ and $Y$ are two sets,
and if $f : X \to Y$ is any map, then we can define a binary relation
$\overset{f}{\equiv}$ on $X$ (written infix) by setting
\[
a \overset{f}{\equiv} b
\qquad \text{ if and only if } \qquad
f \tup{a} = f \tup{b} .
\]
This is an equivalence relation (for each $f$). The equivalence
relation $\overset{m}{\equiv}$ on $\ZZ$ defined in the previous
example is actually a particular case of this $\overset{f}{\equiv}$:
Namely, if we define $f : \ZZ \to \set{0, 1, \ldots, m-1}$ to be the
map that sends each $n \in \ZZ$ to the remainder obtained when $n$ is
divided by $m$, then the relation $\overset{f}{\equiv}$ becomes
identical with $\overset{m}{\equiv}$.
\end{itemize}

Each simple graph $G$ gives rise to an equivalence relation; here is
how it is defined:

\begin{definition} \label{def.intro.equivalence-from-graph}
Let $G$ be a simple graph. We define a binary relation $\simeq_G$ on
the set $\verts{G}$ (written infix) as follows:

If $u$ and $v$ are two elements of $\verts{G}$, then we set
$u \simeq_G v$ if and only if there exists a walk from $u$ to $v$ in
$G$.
\end{definition}

\begin{example} \label{exa.intro.equivalence-from-graph.1}
Let $G$ be the simple graph defined in
Example \ref{exa.simple.R33} \textbf{(a)}.
Then, there exists a walk from $1$ to $4$ in $G$ (for example,
the walk $\tup{1, 2, 3, 4}$, or the walk
$\tup{1, 2, 3, 2, 3, 4}$, or the walk $\tup{1, 6, 5, 4}$,
or many other walks).
In other words, $1 \simeq_G 4$.

It is actually easy to see that $u \simeq_G v$ for any two
vertices $u$ and $v$ of $G$.
If we want to prove this just using the definition of $\simeq_G$,
then we have to construct a walk from $u$ to $v$ for each pair
$\tup{u, v}$ of vertices of $G$.
However, there is a simpler way to prove this -- see
Example~\ref{exa.intro.equivalence-from-graph.2} below.
\end{example}

\begin{example} \label{exa.intro.equivalence-from-graph.3}
Let $G$ be the simple graph defined in
Example \ref{exa.intro.discon-graph} for $n = 8$.
Then, we have $1 \simeq_G 4$ and $1 \simeq_G 7$, but we don't
have $1 \simeq_G 3$.
It is easy to see that the relation $\simeq_G$ for the graph
$G$ is exactly the relation $\overset{3}{\equiv}$ (restricted
to the set $\set{1, 2, \ldots, n}$).
\end{example}

The binary relation $\simeq_G$ we just defined is of fundamental
importance, although many authors do not define a specific symbol for
it. As promised, it is an equivalence relation:

\begin{proposition} \label{prop.intro.equivalence-from-graph.equiv}
Let $G$ be a simple graph. Then, the binary relation $\simeq_G$
(defined in Definition~\ref{def.intro.equivalence-from-graph}) is an
equivalence relation.
\end{proposition}

\begin{example} \label{exa.intro.equivalence-from-graph.2}
Let $G$ be the simple graph defined in
Example \ref{exa.simple.R33} \textbf{(a)}.
Then, there exists a walk from $1$ to $2$ in $G$ (for example,
the walk $\tup{1, 2}$).
Hence, $1 \simeq_G 2$.
Similarly, $2 \simeq_G 3$, $3 \simeq_G 4$, $4 \simeq_G 5$ and
$5 \simeq_G 6$.
But Proposition~\ref{prop.intro.equivalence-from-graph.equiv}
shows that $\simeq_G$ is an equivalence relation.
Hence, from
$1 \simeq_G 2 \simeq_G 3 \simeq_G 4 \simeq_G 5 \simeq_G 6$,
we immediately conclude that $u \simeq_G v$ for any two
vertices $u$ and $v$ of $G$.
\end{example}

\begin{proof}[Proof of
Proposition~\ref{prop.intro.equivalence-from-graph.equiv}.]

We are going to show that the relation $\simeq_G$ is reflexive,
symmetric and transitive.

\begin{statement}
\textit{Claim 1:} The relation $\simeq_G$ is reflexive.
\end{statement}

\textit{Proof of Claim 1:} Let
$x \in \verts{G}$. Then, $\tup{x}$ is a walk from $x$ to $x$ in $G$.
Hence, there exists a walk from $x$ to $x$ in $G$ (namely,
$\tup{x}$). In other words, $x \simeq_G x$ (because the definition of
$\simeq_G$ reveals that $x \simeq_G x$ holds if and only if there
exists a walk from $x$ to $x$ in $G$).

Now, forget that we fixed $x$. We thus have shown that for each
$x \in \verts{G}$, we have $x \simeq_G x$. In other words, the
relation $\simeq_G$ is reflexive (by the definition of ``reflexive'').
This proves Claim 1.

\begin{statement}
\textit{Claim 2:} The relation $\simeq_G$ is symmetric.
\end{statement}

\textit{Proof of Claim 2:} Let $x \in \verts{G}$ and $y \in \verts{G}$
be such that $x \simeq_G y$.

We have $x \simeq_G y$. In other words, there exists a walk from $x$
to $y$ in $G$ (by the definition of $\simeq_G$). Fix such a walk, and
denote it by $\mathbf{a} = \tup{a_0, a_1, \ldots, a_k}$.

Thus, $\mathbf{a} = \tup{a_0, a_1, \ldots, a_k}$ is a walk from $x$
to $y$ in $G$.
Hence, Proposition~\ref{prop.walks.rev} \textbf{(a)} (applied to
$u = x$ and $v = y$) shows that the list
$\tup{a_k, a_{k-1}, \ldots, a_0}$ is a walk from $y$ to $x$. Thus,
there exists a walk from $y$ to $x$ in $G$ (namely, this list). In
other words, $y \simeq_G x$ (by the definition of $\simeq_G$).

Now, forget that we fixed $x$ and $y$. We thus have shown that for
each $x \in \verts{G}$ and $y \in \verts{G}$ satisfying
$x \simeq_G y$, we have $y \simeq_G x$. In other words, the
relation $\simeq_G$ is symmetric (by the definition of ``symmetric'').
This proves Claim 2.

\begin{statement}
\textit{Claim 3:} The relation $\simeq_G$ is transitive.
\end{statement}

\textit{Proof of Claim 3:} Let $x \in \verts{G}$, $y \in \verts{G}$
and $z \in \verts{G}$ be such that $x \simeq_G y$ and
$y \simeq_G z$.

We have $x \simeq_G y$. In other words, there exists a walk from $x$
to $y$ in $G$ (by the definition of $\simeq_G$). Fix such a walk, and
denote it by $\mathbf{a} = \tup{a_0, a_1, \ldots, a_k}$.

We have $y \simeq_G z$. In other words, there exists a walk from $y$
to $z$ in $G$ (by the definition of $\simeq_G$). Fix such a walk, and
denote it by $\mathbf{b} = \tup{b_0, b_1, \ldots, b_\ell}$.

We know that $\mathbf{a} = \tup{a_0, a_1, \ldots, a_k}$ is a walk from
$x$ to $y$ in $G$, and we also know that
$\mathbf{b} = \tup{b_0, b_1, \ldots, b_\ell}$ is a walk from $y$ to
$z$ in $G$. Hence, Proposition~\ref{prop.walks.concat} (applied to
$u = x$, $v = y$ and $w = z$) shows that
\begin{align*}
\tup{a_0, a_1, \ldots, a_k, b_1, b_2, \ldots, b_\ell}
&= \tup{a_0, a_1, \ldots, a_{k-1}, b_0, b_1, \ldots, b_\ell} \\
&= \tup{a_0, a_1, \ldots, a_{k-1}, y, b_1, b_2, \ldots, b_\ell}
\end{align*}
is a walk from $x$ to $z$. Thus, there exists a walk from $x$ to $z$
in $G$ (namely, the walk we have just constructed). In other words,
$x \simeq_G z$ (by the definition of $\simeq_G$).

Now, forget that we fixed $x$, $y$ and $z$. We thus have shown that
for each $x \in \verts{G}$, $y \in \verts{G}$ and $z \in \verts{G}$
satisfying $x \simeq_G y$ and $y \simeq_G z$, we have $x \simeq_G z$.
In other words, the relation $\simeq_G$ is transitive (by the
definition of ``transitive''). This proves Claim 3.

Altogether, we now have shown that the relation $\simeq_G$ is
reflexive, symmetric and transitive. In other words, this relation
$\simeq_G$ is an equivalence relation (by the definition of an
``equivalence relation'').
This proves Proposition~\ref{prop.intro.equivalence-from-graph.equiv}.
\end{proof}

The following proposition gives a few equivalent characterizations for
the relation $\simeq_G$ introduced in
Definition~\ref{def.intro.equivalence-from-graph}:

\begin{proposition} \label{prop.intro.paths-and-walks}
Let $G$ be a simple graph. Let $u$ and $v$ be two vertices of $G$.
The following seven statements are equivalent:

\begin{itemize}
\item \textit{Statement $\mathcal{S}_0$:} We have $u \simeq_G v$.

\item \textit{Statement $\mathcal{S}_1$:}
There exists a walk from $u$ to $v$.

\item \textit{Statement $\mathcal{S}_2$:}
There exists a walk from $v$ to $u$.

\item \textit{Statement $\mathcal{S}_3$:}
There exists a walk from one of the two vertices $u$ and $v$ to the
other.

\item \textit{Statement $\mathcal{S}_4$:}
There exists a path from $u$ to $v$.

\item \textit{Statement $\mathcal{S}_5$:}
There exists a path from $v$ to $u$.

\item \textit{Statement $\mathcal{S}_6$:}
There exists a path from one of the two
vertices $u$ and $v$ to the other.
\end{itemize}
\end{proposition}

\begin{proof}[Proof of Proposition~\ref{prop.intro.paths-and-walks}.]
The definition of the relation $\simeq_G$ shows that $u \simeq_G v$
holds if and only if there exists a walk from $u$ to $v$ in $G$. In
other words, Statement $\mathcal{S}_0$ holds if and only if
Statement $\mathcal{S}_1$ holds. This proves the equivalence
$\mathcal{S}_0 \Longleftrightarrow \mathcal{S}_1$.

The implication $\mathcal{S}_1 \Longrightarrow \mathcal{S}_2$ is
easy to prove\footnote{\textit{Proof.} Assume that $\mathcal{S}_1$
holds. We must show that $\mathcal{S}_2$ holds.

There exists a walk from $u$ to $v$ (since $\mathcal{S}_1$ holds).
Fix such a walk, and denote it by
$\mathbf{a} = \tup{a_0, a_1, \ldots, a_k}$.

Thus, $\mathbf{a} = \tup{a_0, a_1, \ldots, a_k}$ is a walk from $u$
to $v$ in $G$.
Hence, Proposition~\ref{prop.walks.rev} \textbf{(a)} shows that the
list $\tup{a_k, a_{k-1}, \ldots, a_0}$ is a walk from $v$ to $u$.
Thus, there exists a walk from $v$ to $u$ (namely, this list). In
other words, $\mathcal{S}_2$ holds. This proves the implication
$\mathcal{S}_1 \Longrightarrow \mathcal{S}_2$.}. The same argument,
but with the roles of $u$ and $v$ interchanged, proves the implication
$\mathcal{S}_2 \Longrightarrow \mathcal{S}_1$. Combining the two
implications $\mathcal{S}_1 \Longrightarrow \mathcal{S}_2$ and
$\mathcal{S}_2 \Longrightarrow \mathcal{S}_1$, we obtain the
equivalence
$\mathcal{S}_1 \Longleftrightarrow \mathcal{S}_2$.

The statement $\mathcal{S}_3$ is the disjunction of the two statements
$\mathcal{S}_1$ and $\mathcal{S}_2$. In other words,
$\mathcal{S}_3 \Longleftrightarrow
\tup{\mathcal{S}_1 \vee \mathcal{S}_2}$. But from
$\mathcal{S}_1 \Longleftrightarrow \mathcal{S}_2$, we obtain
$\tup{\mathcal{S}_1 \vee \mathcal{S}_2}
\Longleftrightarrow \tup{\mathcal{S}_2 \vee \mathcal{S}_2}
\Longleftrightarrow \mathcal{S}_2$. Hence,
$\mathcal{S}_3 \Longleftrightarrow
\tup{\mathcal{S}_1 \vee \mathcal{S}_2}
\Longleftrightarrow \mathcal{S}_2$.

Combining the three equivalences
$\mathcal{S}_0 \Longleftrightarrow \mathcal{S}_1$,
$\mathcal{S}_1 \Longleftrightarrow \mathcal{S}_2$ and
$\mathcal{S}_3 \Longleftrightarrow \mathcal{S}_2$, we obtain the
chain of equivalences
\begin{equation}
\mathcal{S}_0 \Longleftrightarrow \mathcal{S}_1 \Longleftrightarrow
\mathcal{S}_2 \Longleftrightarrow \mathcal{S}_3 .
\label{pf.prop.intro.paths-and-walks.chain1}
\end{equation}

The implication $\mathcal{S}_4 \Longrightarrow \mathcal{S}_1$ holds
for obvious reasons (namely, because every path is a walk). But
the implication $\mathcal{S}_1 \Longrightarrow \mathcal{S}_4$ also
holds\footnote{\textit{Proof.} Assume that $\mathcal{S}_1$
holds. We must show that $\mathcal{S}_4$ holds.

There exists a walk from $u$ to $v$ (since $\mathcal{S}_1$ holds).
Fix such a walk, and denote it by $\mathbf{a}$. Let $k$ be the length
of this walk $\mathbf{a}$. Then, exists a walk from $u$ to $v$ of
length $k$ (namely, $\mathbf{a}$). Corollary~\ref{cor.walk.path}
therefore shows that there exists a path from $u$ to $v$ of length
$\leq k$. In particular, there exists a path from $u$ to $v$. In other
words, $\mathcal{S}_4$ holds. This proves the implication
$\mathcal{S}_1 \Longrightarrow \mathcal{S}_4$.}. Combining the two
implications $\mathcal{S}_1 \Longrightarrow \mathcal{S}_4$ and
$\mathcal{S}_4 \Longrightarrow \mathcal{S}_1$, we obtain the
equivalence
$\mathcal{S}_1 \Longleftrightarrow \mathcal{S}_4$. The same argument
(but with the roles of $u$ and $v$ interchanged) proves the
equivalence
$\mathcal{S}_2 \Longleftrightarrow \mathcal{S}_5$. Thus,
$\mathcal{S}_1 \Longleftrightarrow \mathcal{S}_2
\Longleftrightarrow \mathcal{S}_5$.

The statement $\mathcal{S}_6$ is the disjunction of the two statements
$\mathcal{S}_4$ and $\mathcal{S}_5$. In other words,
$\mathcal{S}_6 \Longleftrightarrow
\tup{\mathcal{S}_4 \vee \mathcal{S}_5}$. But from
$\mathcal{S}_4 \Longleftrightarrow \mathcal{S}_1$ and
$\mathcal{S}_5 \Longleftrightarrow \mathcal{S}_1$, we obtain
$\tup{\mathcal{S}_4 \vee \mathcal{S}_5}
\Longleftrightarrow \tup{\mathcal{S}_1 \vee \mathcal{S}_1}
\Longleftrightarrow \mathcal{S}_1$. Hence,
$\mathcal{S}_6 \Longleftrightarrow
\tup{\mathcal{S}_4 \vee \mathcal{S}_5}
\Longleftrightarrow \mathcal{S}_1$.

Because of the three equivalences
$\mathcal{S}_1 \Longleftrightarrow \mathcal{S}_4$,
$\mathcal{S}_1 \Longleftrightarrow \mathcal{S}_5$ and
$\mathcal{S}_1 \Longleftrightarrow \mathcal{S}_6$, we can extend the
chain of equivalences \eqref{pf.prop.intro.paths-and-walks.chain1}
to
\[
\mathcal{S}_0 \Longleftrightarrow \mathcal{S}_1 \Longleftrightarrow
\mathcal{S}_2 \Longleftrightarrow \mathcal{S}_3 \Longleftrightarrow
\mathcal{S}_4 \Longleftrightarrow \mathcal{S}_5 \Longleftrightarrow
\mathcal{S}_6 .
\]
Thus, Proposition~\ref{prop.intro.paths-and-walks} is proven.
\end{proof}

\subsubsection{Dijkstra's algorithm}

\begin{todo}
Dijkstra's algorithm. Use tables of paths to formalize it.
\end{todo}

\begin{todo}
Proof.
\end{todo}

\subsubsection{Circuits and cycles}

\begin{definition} \label{def.intro.circuits}
Let $G$ be a simple graph.

\begin{enumerate}

\item[\textbf{(a)}] A \textit{circuit} (in $G$) means a walk
$\tup{v_0, v_1, \ldots, v_k}$ in $G$ satisfying $v_k = v_0$.

\item[\textbf{(b)}] A \textit{cycle} (in $G$) means a circuit
$\tup{v_0, v_1, \ldots, v_k}$ in $G$ such that $k \geq 3$, and such
that the vertices $v_0, v_1, \ldots, v_{k-1}$ are distinct.

\end{enumerate}

\end{definition}

\begin{example} \label{exa.intro.circuits}
Let $V$ and $E$ be as in Example~\ref{exa.simple.R33} \textbf{(c)}.
Let $G$ be the graph $\tup{V, E}$. Then:

\begin{itemize}
\item The sequence $\tup{1, 3, 2, 1, 6, 5, 6, 1}$ of vertices of $G$
is a circuit (in $G$), since it is a walk and satisfies $1 = 1$.
But it is not a cycle, since the vertices $1, 3, 2, 1, 6, 5, 6$ are
not distinct.
Let us visualize this circuit by marking all the edges that it uses with
double arrows:
\begin{align*}
\xymatrix{
& 1 \ar@{=>}[rrd] \ar@/^0.5pc/@{=>}[ld] & 2 \ar@{=>}[l] \\
6 \ar@/^0.5pc/@{=>}[ru] \ar@/^1pc/@{=>}[rd] & & & 3 \are[ld] \ar@/_1pc/@{=>}[lu] \\
& 5 \ar@/^1pc/@{=>}[lu] & 4 \are[l]
} .
\end{align*}
Note that this picture does not determine the circuit uniquely;
it could just as well stand for the circuit
$\tup{1, 6, 5, 6, 1, 3, 2, 1}$ or for the circuit
$\tup{2, 1, 6, 5, 6, 1, 3, 2}$ (or for several other
circuits).

\item The sequence $\tup{1, 2, 3, 1}$ of vertices of $G$ is a circuit
and a cycle (since the vertices $1, 2, 3$ are distinct).

\item The sequence $\tup{1, 2, 1}$ of vertices of $G$ is a circuit,
but not a cycle (since it fails the $k \geq 3$ condition in
Definition~\ref{def.intro.circuits}).

\item The sequence $\tup{1, 3, 4, 5}$ of vertices of $G$ is not a
circuit (since $5 \neq 1$).
\end{itemize}
\end{example}

\begin{example}

\begin{enumerate}

\item[\textbf{(a)}]
The path graph $P_n$ introduced in
Definition~\ref{def.intro.path-graph}
has no cycles (although it has several circuits, such as
$\tup{1, 2, 3, 2, 1}$ for $n \geq 3$).

\item[\textbf{(b)}]
The cycle graph $C_n$ introduced in
Definition~{def.intro.cycle-graph} has $2n$ cycles
(for $n \geq 3$):
Namely, for each vertex $i \in \set{1, 2, \ldots, n}$ of $C_n$,
we can either move forward along the circle (obtaining the
cycle $\tup{i, i+1, i+2, \ldots, n, 1, 2, \ldots, i}$)
or move backward along the circle (obtaining the cycle
$\tup{i, i-1, i-2, \ldots, 1, n, n-1, \ldots, i}$).
These $2n$ cycles are all distinct according to our definition
of a cycle, although they all use the same edges.

\end{enumerate}

\end{example}

\begin{todo}
Explain that various books define cycles differently, e.g. Diestel:
\begin{itemize}
\item Several sources define a cycle not as a list of vertices,
      but rather as a subgraph isomorphic to $C_k$ for some
      $k \geq 3$. For example, \cite{Dieste16}, \cite{BonMur08} and
      \cite{West01} do this. This definition is closely related to
      ours, because when $\tup{v_0, v_1, \ldots, v_k}$ is a cycle of
      $G$ (in our meaning of this word), then the subgraph
      \[
      \tup{ \set{v_0, v_1, \ldots, v_k} ,
            \set{v_i v_{i+1} \ \mid \  i \in \set{0, 1, \ldots, k-1}}
          }
      \]
      of $G$ is isomorphic to $C_k$. But the two definitions are not
      literally equivalent, because the cycle
      $\tup{v_0, v_1, \ldots, v_k}$ (in our sense) cannot be uniquely
      reconstructed from the subgraph defined by it; indeed, any
      rotated version $\tup{v_i, v_{i+1}, \ldots, v_{k-1}, v_0, v_1,
      \ldots, v_{i-1}}$ of this cycle (as well as its reflected
      version $\tup{v_k, v_{k-1}, \ldots, v_0}$) yields the same
      subgraph.
\item \cite{Bollob98} defines a cycle as an equivalence class of what
      we call ``cycle'' with respect to reflections and rotations.
      This ends up equivalent to the convention used in
      \cite{Dieste16}, \cite{BonMur08} and \cite{West01}.
\item I am not sure how Ore defines cycles in \cite{Ore90} and
      \cite{Ore74}; but he doesn't seem to do much with this notion.
\item \cite{Harju14} defines cycles in the same way as we do, but
      erroneously omits the condition $k \geq 3$.
\item The definition of cycles in \cite{BehCha71} is equivalent to
      ours (though circuits are defined slightly differently).
\item ...
\end{itemize}
I haven't even started discussing the various definitions of ``walk'',
``path'' and ``circuit''. Be careful.

Some authors define ``circuit'' to mean ``closed walk with distinct
edges'', which too differs from what we do.
\end{todo}

% Here are some facts about circuits and cycles:

\begin{todo}
Facts about cycles and circuits:

- Rotating a circuit yields a circuit.

- Rotating a cycle yields a cycle.

- Reversal of a circuit is a circuit.

- Reversal of a cycle is a cycle.

- Edges of a cycle are distinct.

- Each walk with no two identical adjacent edges contains a cycle,
  unless it is a path.

- If there are two different paths from $u$ to $v$, then there is a
  cycle.
\end{todo}

\subsubsection{\label{subsect.intro.connected}Connectedness}

\begin{definition}
Let $G = \tup{V, E}$ be a simple graph. We say that $G$ is
\textit{connected} if $G$ satisfies the following two properties:
\begin{itemize}
\item Its vertex set $V$ is nonempty.
\item Each two vertices $u \in V$ and $v \in V$ satisfy
$u \simeq_G v$.
\end{itemize}
\end{definition}

\begin{todo}
Examples. Reuse Example~\ref{exa.intro.discon-graph}.
\end{todo}

\begin{definition}
Let $G = \tup{V, E}$ be a simple graph. Let $v \in V$. The
\textit{connected component of $v$} (in $G$) denotes the subset
$\set{ u \in V \ \mid \  u \simeq_G v }$ of $V$.
We shall denote this connected component of $v$ by
$\conncomp v$ (or by $\conncomp_G v$, when $G$ is not clear from the
context).

A \textit{connected component of $G$} means a subset of $V$ that is
the connected component of some $v \in V$. (Note that the $v$ is
usually not uniquely determined -- in fact, several different vertices
$v \in V$ often have the same connected component.)
\end{definition}

Roughly speaking, if $v$ is a vertex of a simple graph $G$, then
the connected component of $v$ is the set of all vertices that are
reachable from $v$ by walks (i.e., that are ending points of walks
that start at $v$).

\begin{example}

\begin{enumerate}

\item[\textbf{(a)}] Let $G = \tup{V, E}$ be the simple graph defined
in Example \ref{exa.simple.R33} \textbf{(a)}.
Then, we know (from Example~\ref{exa.intro.equivalence-from-graph.1})
that $u \simeq_G v$ for any two vertices $u$ and $v$ of $G$.
Hence, for each $v \in V$, we have
$\conncomp v
= \set{ u \in V \ \mid \  \underbrack{u \simeq_G v}
                                     {\text{this always holds}} }
= \set{ u \in V } = V$.
In other words, the connected component of any vertex $v$
of $G$ is the whole set $V$.
Thus, $G$ has only one connected component -- namely, $V$.

\item[\textbf{(b)}] Let $n \in \NN$ be such that $n \geq 3$.
Let $G = \tup{V, E}$ be the simple graph defined in
Example \ref{exa.intro.discon-graph}.
Then, two vertices $u$ and $v$ of $G$ satisfy
$u \simeq_G v$ if and only if they satisfy
$u \equiv v \mod 3$.
Hence, $G$ has exactly three connected components:
\begin{itemize}
\item The connected component
      $\conncomp 1
       = \set{ u \in V \ \mid \  u \simeq_G 1 }
       = \set{ u \in V \ \mid \  u \equiv 1 \mod 3 }
       = \set{1, 4, 7, \ldots} \cap V$.
\item The connected component
      $\conncomp 2
       = \set{ u \in V \ \mid \  u \simeq_G 2 }
       = \set{ u \in V \ \mid \  u \equiv 2 \mod 3 }
       = \set{2, 5, 8, \ldots} \cap V$.
\item The connected component
      $\conncomp 3
       = \set{ u \in V \ \mid \  u \simeq_G 3 }
       = \set{ u \in V \ \mid \  u \equiv 3 \mod 3 }
       = \set{3, 6, 9, \ldots} \cap V$.
\end{itemize}
If we have $n < 3$ instead, then some of these
connected components are missing (e.g., for $n = 2$,
there is no $\conncomp 3$, and $G$ has only $2$
connected components).

\end{enumerate}

\end{example}

\begin{todo}
Connected iff exactly 1 connected component.

Remark about the graph with $0$ vertices.

Remark about $K_n$ and $E_n$.
\end{todo}

\begin{todo}
Restate using paths and walks. Equivalence of:

- $u \simeq_G v$.

- There is a path from $u$ to $v$.

- There is a walk from $u$ to $v$.

- $\conncomp u = \conncomp v$.

- $u \in \conncomp v$.

- $v \in \conncomp u$.

- $\conncomp u \cap \conncomp v \neq \varnothing$.
\end{todo}

\begin{todo}
Each $v \in V$ lies in exactly one connected component.
\end{todo}

\begin{todo}
Connected components are connected and maximal at that.
\end{todo}

\begin{todo}
A graph is isomorphic to the disjoint union of its connected
components.
\end{todo}

\subsection{\label{sect.intro.teasers}Questions to ask about graphs}

\begin{todo}
Connectedness.
\end{todo}

\begin{todo}
Hamiltonian paths and cycles:
Do they exist? How to find them? (We will have partial results, mainly sufficient conditions.)
\end{todo}

\begin{todo}
Eulerian walks and circuits.
(Theory is really nice here, with easy necessary-and-sufficient conditions, but we'll wait for a better notion of graph.)
\end{todo}

\begin{todo}
Matchings:
how large, how many, structure? (Nice and large theory, still under research; we will see a lot of it but still barely scratch the surface.)
\end{todo}

\section{\label{sect.dominating}Dominating sets}

I will next digress to discuss the notion of \textit{dominating sets}.
The reason why I am doing this at this point is not that dominating
sets are of any particular fundamental importance (there are arguably
more crucial notions in graph theory left to consider), but rather
that they neatly illustrate the concepts we have seen so far and
provide some experience with graph-theoretical proofs, all that while
not requiring any complex theory or advanced techniques.

\subsection{\label{subsect.dominating.defs}Definition}

\begin{definition} \label{def.dominating}
Let $G$ be a simple graph. A subset $U$ of $\verts{G}$ is said to be
\textit{dominating} (for $G$) if it has the following property: For
every vertex $v \in \verts{G} \setminus U$, at least one neighbor of
$v$ belongs to $U$.

A \textit{dominating set} of $G$ means a dominating subset of
$\verts{G}$.
\end{definition}

\begin{example} \label{exa.dominating.pentagon}
For this example, let us consider the graph $G = \tup{V, E}$, where
\begin{align*}
V &= \set{1, 2, 3, 4, 5} \qquad \text{and} \\
E &= \set{\set{1,2}, \set{2,3}, \set{3,4}, \set{4,5}, \set{5,1}} .
\end{align*}
(This graph has already been introduced in
Example~\ref{exa.simple.R33} \textbf{(e)}. It can be drawn to look
like a pentagon.)

The subset $\set{1, 3}$ of $V$ is dominating (for $G$). (This can be
checked directly: We must show that
for every vertex $v \in \verts{G} \setminus \set{1, 3}$, at least
one neighbor of $v$ belongs to $\set{1, 3}$. There are three vertices
$v \in \verts{G} \setminus \set{1, 3}$, namely $2$, $4$ and $5$. For
$v = 2$, the neighbor $1$ of $v$ belongs to $\set{1, 3}$ (and so does
the neighbor $3$). For $v = 4$, the neighbor $3$ of $v$ belongs to
$\set{1, 3}$. For $v = 5$, the neighbor $1$ of $v$ belongs to
$\set{1, 3}$.)

The subset $\set{1, 2}$ of $V$ is \textbf{not} dominating (for $G$).
(Indeed, the vertex $4 \in \verts{G} \setminus \set{1, 2}$ does
\textbf{not} have the property that at least one neighbor of $4$
belongs to $\set{1, 2}$.)

Every subset of $V$ having at least $3$ elements is dominating,
whereas no subset of $V$ having at most $1$ element is dominating.
A $2$-element subset can be either dominating or not.

(Of course, more complicated graphs exhibit more complex behavior.)
\end{example}

\begin{exercise}
Let $n \in \NN$. What is the smallest possible size of a dominating
set of the cycle graph $C_{3n}$ ?
\end{exercise}

Clearly, if $G$ is a simple graph, then its vertex set $\verts{G}$ is
a dominating set.
One natural question to ask is how small a dominating set of a graph
can be. When the graph $G$ is empty, only the vertex set $\verts{G}$
itself is dominating. On the other hand, when $G$ is a complete graph
on $n \geq 1$ vertices, every nonempty subset of $\verts{G}$ is
dominating. Clearly, the more edges a simple graph has, the more
dominating sets it has (in the sense that if we add a new edge, then
all sets that are dominating remain dominating, and possibly new
dominating sets appear). It is furthermore clear that if a vertex of
a simple graph $G$ has no neighbors, then it must belong to each
dominating set of $G$ (because otherwise, at least one neighbor of
this vertex would need to lie in the dominating set; but this is
impossible, since it has no neighbors). Such vertices are said to be
\textit{isolated}.

\begin{definition} \label{def.intro.isolated}
Let $G$ be a simple graph. A vertex $v$ of $G$ is said to be
\textit{isolated} if it has no neighbors. (In other words, a vertex
$v$ of $G$ is said to be \textit{isolated} if it belongs to no edge
of $G$. In other words, a vertex
$v$ of $G$ is said to be \textit{isolated} if its degree $\deg v$
equals $0$.)
\end{definition}

\begin{proposition} \label{prop.dominating.|V|/2}
Let $G = \tup{V, E}$ be a simple graph that has no isolated vertices.

\textbf{(a)} There exist two disjoint dominating subsets $A$ and $B$
of $V$ such that $A \cup B = V$.

\textbf{(b)} There exists a dominating subset of $V$ having size
$\leq \abs{V}/2$.
\end{proposition}

We will see two proofs of this proposition later, in Subsections
\ref{subsect.dominating.part1} and \ref{subsect.dominating.part2}.
% when we have defined
% the notion of the \textit{distance} between two vertices in a graph.

Again, Proposition~\ref{prop.dominating.|V|/2} can be neatly restated
in terms of people and friendships\footnote{Namely:

\textit{Restatement of
Proposition~\ref{prop.dominating.|V|/2} \textbf{(a)}:} Given a group
of people, each of whom has at least one friend (among the others),
it is always possible to subdivide the group into two teams such that
each person has a friend in the opposite team.

\textit{Restatement of
Proposition~\ref{prop.dominating.|V|/2} \textbf{(b)}:} Given a group
of (finitely many) people each of whom has at least one friend (among
the others),
it is always possible to choose at most $\abs{V}/{2}$ people from this
group such that everyone who is not chosen has at least one of the
chosen ones among his friends.

(As usual, we assume that the group of people is finite, and that the
relation of friendship is symmetric.)}.

\begin{example}
\textbf{(a)} Let $V$, $E$ and $G$ be as in
Example~\ref{exa.dominating.pentagon}.
Then, Proposition~\ref{prop.dominating.|V|/2} \textbf{(b)} predicts
that there exists a dominating subset of $V$ having size
$\leq \abs{V}/2 = 5/2$. Since the size of a finite set is an integer,
this shows that there exists a dominating subset of $V$ having size
$\leq 2$ (because any integer that is $\leq 5/2$ must automatically be
$\leq 2$). And indeed, such a dominating subset exists
(for example, $\set{5, 3}$).

\textbf{(b)} Now, let us instead consider the graph $G = \tup{V, E}$,
where
\begin{align*}
V &= \set{1, 2, 3, 4, 5} \qquad \text{and} \\
E &= \powset[2]{V} .
\end{align*}
(This is the complete graph $K_5$.)
Again, Proposition~\ref{prop.dominating.|V|/2} \textbf{(b)} predicts
that there exists a dominating subset of $V$ having size
$\leq \abs{V}/2 = 5/2$. Again, this entails that
there exists a dominating subset of $V$ having size $\leq 2$ (since
the size of a finite set is an integer). This is indeed true, but for
this particular graph we can even find a dominating subset of $V$
having size $1$: for example, the subset $\set{1}$.
\end{example}

\subsection{\label{subsect.dominating.odd}Brouwer's theorem and the
Heinrich-Tittmann formula}

Next, we state a surprising recent result by Brouwer (\cite{Brouwe09},
from 2009) about the number of dominating sets of a graph:

\begin{theorem} \label{thm.dominating.brouwer}
Let $G$ be a simple graph. Then, the number of dominating sets of $G$
is odd.
\end{theorem}

Brouwer (in \cite{Brouwe09}) gives three proofs of this theorem. We
are going to give another. Better yet, we shall prove a more
precise result which is even more recent (a preprint \cite{HeiTit17}
from 2017), due to Heinrich and Tittmann:

\begin{theorem} \label{thm.dominating.heinrich}
Let $G = \tup{V, E}$ be a simple graph. Let $n = \abs{V}$. Assume that
$n > 0$.

A \textit{detached pair} shall mean a pair $\tup{A, B}$ of two
disjoint subsets $A$ and $B$ of $V$ having the property that
there exists no edge $ab \in E$ satisfying
$a \in A$ and $b \in B$.

Let $\alpha$ be the number of all detached pairs $\tup{A, B}$ for
which both numbers $\abs{A}$ and $\abs{B}$ are even and positive.

Let $\beta$ be the number of all detached pairs $\tup{A, B}$ for
which both numbers $\abs{A}$ and $\abs{B}$ are odd.

Then:

\textbf{(a)} The numbers $\alpha$ and $\beta$ are even.

\textbf{(b)} The number of dominating sets of $G$ is
$2^n - 1 + \alpha - \beta$.
\end{theorem}

At this point, let me stress that the word ``pair'' always means an
ordered pair throughout these notes. In particular, in
Theorem~\ref{thm.dominating.heinrich}, a detached pair $\tup{A, B}$
should be distinguished from $\tup{B, A}$, unless they actually are
equal (which only happens when both $A$ and $B$ are the empty set).

Theorem~\ref{thm.dominating.heinrich} is a restatement of
\cite[Theorem 8]{HeiTit17}. The proof we shall give below is
shorter than the proof in \cite{HeiTit17}, but does not lead us
through as many interesting intermediate results.

Let us first see how Theorem~\ref{thm.dominating.brouwer} can be
derived from Theorem~\ref{thm.dominating.heinrich}:

\begin{proof}[Proof of Theorem~\ref{thm.dominating.brouwer} using
Theorem~\ref{thm.dominating.heinrich}.]

Write the graph $G$ in the form $G = \tup{V, E}$. If $\abs{V} = 0$,
then Theorem~\ref{thm.dominating.brouwer}
holds\footnote{\textit{Proof.} Assume that $\abs{V} = 0$. Hence,
the set $V$ is empty. Thus, the only subset of $V$ is $\varnothing$.
This subset $\varnothing$ is dominating (because it is the whole
set $V$). Thus, there exists exactly $1$ dominating set of $G$
(namely, $\varnothing$). In other words, the number of dominating
sets of $G$ is $1$. Therefore, this number is odd. Hence,
Theorem~\ref{thm.dominating.brouwer} is proven (under the assumption
that $\abs{V} = 0$).}. Hence, for the rest of this proof, we
WLOG assume that $\abs{V} = 0$ does not hold.

Let us use the notations of Theorem~\ref{thm.dominating.heinrich}.
From Theorem~\ref{thm.dominating.heinrich} \textbf{(a)}, we know
that $\alpha$ and $\beta$ are even. In other words,
$\alpha \equiv 0 \mod 2$ and $\beta \equiv 0 \mod 2$.
Furthermore, $n = \abs{V} \neq 0$ (since $\abs{V} = 0$ does not
hold), so that $n$ is a positive integer. Thus, $2^n$ is even.
In other words, $2^n \equiv 0 \mod 2$. Now,
Theorem~\ref{thm.dominating.heinrich} \textbf{(b)} shows that
the number of dominating sets of $G$ is
\[
\underbrace{2^n}_{\equiv 0 \mod 2} - 1
  + \underbrace{\alpha}_{\equiv 0 \mod 2}
  - \underbrace{\beta}_{\equiv 0 \mod 2}
\equiv 0 - 1 + 0 - 0 = -1 \equiv 1 \mod 2.
\]
In other words, the number of dominating sets of $G$ is odd.
This proves Theorem~\ref{thm.dominating.brouwer}.
\end{proof}

Thus, it remains to prove Theorem~\ref{thm.dominating.heinrich}.

\subsection{\label{subsect.dominating.iverson}The Iverson bracket}

Our proof of Theorem~\ref{thm.dominating.heinrich} will rely on
a few lemmas. But first, let us introduce a very basic notation, which
has nothing to do with graphs specifically but is useful
throughout mathematics (particularly combinatorics):

\begin{definition} \label{def.intro.iverson}
Let $\mathcal{A}$ be a logical statement. (It is not required to be
true.) Then, a number
$\ive{\mathcal{A}} \in \set{0, 1}$ is defined as follows:
We set $\ive{\mathcal{A}} =
\begin{cases}
1, & \text{if }\mathcal{A}\text{ is true};\\
0, & \text{if }\mathcal{A}\text{ is false}
\end{cases}$.
This number $\ive{\mathcal{A}}$ is called the \textit{truth value}
of $\mathcal{A}$.
(For example, $\ive{1+1 = 2} = 1$ and $\ive{1+1 = 3} = 0$. For
another example,
$\ive{\text{Proposition~\ref{prop.simple.R33} holds}}
= 1$, because we have proven Proposition~\ref{prop.simple.R33}.)
The notation $\ive{\mathcal{A}}$ for the truth value of $\mathcal{A}$
is known as the \textit{Iverson bracket notation}.
\end{definition}

Truth values satisfy certain simple rules:

\begin{proposition} \label{prop.intro.iverson.rules}
\textbf{(a)} If $\mathcal{A}$ and $\mathcal{B}$ are two equivalent
logical statements, then
$\ive{\mathcal{A}} = \ive{\mathcal{B}}$.

\textbf{(b)} If $\mathcal{A}$ is any logical statement, then
$\ive{\text{not } \mathcal{A}} = 1 - \ive{\mathcal{A}}$.

\textbf{(c)} If $\mathcal{A}$ and $\mathcal{B}$ are two logical
statements, then
$\ive{\mathcal{A} \wedge \mathcal{B}}
= \ive{\mathcal{A}} \ive{\mathcal{B}}$.

\textbf{(d)} If $\mathcal{A}$ and $\mathcal{B}$ are two logical
statements, then
$\ive{\mathcal{A} \vee \mathcal{B}}
= \ive{\mathcal{A}} + \ive{\mathcal{B}}
  - \ive{\mathcal{A}} \ive{\mathcal{B}}$.
\end{proposition}

\begin{proposition} \label{prop.intro.iverson.sums}
Let $P$ be a finite set. Let $Q$ be a subset of $P$.

\textbf{(a)} Then, $\abs{Q} = \sum_{p \in P} \ive{p \in Q}$.

\textbf{(b)} For each $p \in P$, let $a_p$ be a number (for example,
a real number). Then, $\sum_{p \in P} \ive{p \in Q} a_p
= \sum_{p \in Q} a_p$.

\textbf{(c)} For each $p \in P$, let $a_p$ be a number (for example,
a real number). Let $q \in P$. Then,
$\sum_{p \in P} \ive{p = q} a_p = a_q$.
\end{proposition}

\begin{exercise} \label{exe.intro.iverson}
\textbf{(a)} Prove Proposition~\ref{prop.intro.iverson.rules}.

\textbf{(b)} Prove Proposition~\ref{prop.intro.iverson.sums}.

Now, let $G$ be a simple graph.

\textbf{(c)} Prove that
$\deg v = \sum_{u \in \verts{G}} \ive{uv \in \edges{G}}$
for each vertex $v$ of $G$.

\textbf{(d)} Prove that
$2 \abs{\edges{G}}
= \sum_{u \in \verts{G}} \sum_{v \in \verts{G}}
  \ive{uv \in \edges{G}}$.
\end{exercise}

The following lemma is fundamental to much of combinatorics (if not
to say much of mathematics):

\begin{lemma} \label{lem.dominating.heinrich-lemma1}
Let $P$ be a finite set. Then,
\[
\sum_{\substack{A \subseteq P}} \tup{-1}^{\abs{A}}
= \ive{P = \varnothing} .
\]
(The symbol ``$\sum_{\substack{A \subseteq P}}$'' means ``sum over
all subsets $A$ of $P$''. In other words, it means
``$\sum_{\substack{A \in \powset{P}}}$''.)
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lem.dominating.heinrich-lemma1}.]
If $P = \varnothing$, then Lemma~\ref{lem.dominating.heinrich-lemma1}
holds\footnote{\textit{Proof.} Assume that $P = \varnothing$. Then,
there exists only one subset of $P$, namely $\varnothing$. Hence,
the sum $\sum_{\substack{A \subseteq P}} \tup{-1}^{\abs{A}}$
has only one addend, namely the addend for $A = \varnothing$.
Therefore, this sum simplifies to
\[
\sum_{\substack{A \subseteq P}} \tup{-1}^{\abs{A}}
= \tup{-1}^{\abs{\varnothing}} = 1
\]
(since $\abs{\varnothing} = 0$). Comparing this with
$\ive{P = \varnothing} = 1$ (which holds, since $P = \varnothing$ is
true), we obtain
$\sum_{\substack{A \subseteq P}} \tup{-1}^{\abs{A}}
= \ive{P = \varnothing}$. Hence, we have shown that
Lemma~\ref{lem.dominating.heinrich-lemma1} holds if
$P = \varnothing$.}. Hence, for the rest of this proof, we WLOG
assume that $P \neq \varnothing$. Thus, there exists at least one
element $p$ of $P$. Pick such a $p$.

There are two kinds of subsets of $P$: the ones that contain $p$,
and the ones that do not. Hence, the sum
$\sum_{\substack{A \subseteq P}} \tup{-1}^{\abs{A}}$ can be
decomposed as follows:
\begin{equation}
\sum_{\substack{A \subseteq P}} \tup{-1}^{\abs{A}}
= \sum_{\substack{A \subseteq P; \\ p \in A}} \tup{-1}^{\abs{A}}
  + \sum_{\substack{A \subseteq P; \\ p \notin A}} \tup{-1}^{\abs{A}} .
\label{pf.lem.dominating.heinrich-lemma1.1}
\end{equation}

But every subset $A$ of $P$ that contains $p$ has the form
$B \cup \set{p}$ for a \textbf{unique} subset $B$ of $P$ that does
not contain $p$ (namely, for $B = A \setminus \set{p}$). Conversely,
of course, if $B$ is a subset of $P$ that does not contain $p$,
then $B \cup \set{p}$ will always be a subset of $P$ that contains
$p$. Hence, there exists a bijection (i.e., a bijective map)
from the set of all subsets of $P$ that do not contain $p$
  to the set of all subsets of $P$ that do contain $p$;
namely, this bijection sends each subset $B$ of $P$ that does not
contain $p$ to $B \cup \set{p}$. Using this bijection, we can rewrite
the sum
$\sum_{\substack{A \subseteq P; \\ p \in A}} \tup{-1}^{\abs{A}}$ as
follows:
\begin{align*}
\sum_{\substack{A \subseteq P; \\ p \in A}} \tup{-1}^{\abs{A}}
&= \sum_{\substack{B \subseteq P; \\ p \notin B}}
   \underbrace{\tup{-1}^{\abs{B \cup \set{p}}}}_{\substack{
     = \tup{-1}^{\abs{B}+1} \\
     \text{(since } \abs{B \cup \set{p}} = \abs{B} + 1 \\
     \text{(because } p \notin B \text{))}
   }}
= \sum_{\substack{B \subseteq P; \\ p \notin B}}
  \underbrace{\tup{-1}^{\abs{B}+1}}_{= - \tup{-1}^{\abs{B}}}
= \sum_{\substack{B \subseteq P; \\ p \notin B}}
  \tup{- \tup{-1}^{\abs{B}}} \\
&= - \sum_{\substack{B \subseteq P; \\ p \notin B}} \tup{-1}^{\abs{B}}
= - \sum_{\substack{A \subseteq P; \\ p \notin A}} \tup{-1}^{\abs{A}}
\end{align*}
(here, we have renamed the summation index $B$ as $A$).
Thus, \eqref{pf.lem.dominating.heinrich-lemma1.1} becomes
\[
\sum_{\substack{A \subseteq P}} \tup{-1}^{\abs{A}}
= \underbrace{\sum_{\substack{A \subseteq P; \\ p \in A}}
       \tup{-1}^{\abs{A}}}_{
     = - \sum_{\substack{A \subseteq P; \\ p \notin A}}
         \tup{-1}^{\abs{A}}}
  + \sum_{\substack{A \subseteq P; \\ p \notin A}} \tup{-1}^{\abs{A}}
= - \sum_{\substack{A \subseteq P; \\ p \notin A}} \tup{-1}^{\abs{A}}
  + \sum_{\substack{A \subseteq P; \\ p \notin A}} \tup{-1}^{\abs{A}}
= 0.
\]
Comparing this with $\ive{P = \varnothing} = 0$ (which holds, since
$P = \varnothing$ is false (because we assumed $P \neq \varnothing$)),
we obtain
$\sum_{\substack{A \subseteq P}} \tup{-1}^{\abs{A}}
= \ive{P = \varnothing}$. Hence,
Lemma~\ref{lem.dominating.heinrich-lemma1} is proven.
\end{proof}

A different proof of Lemma~\ref{lem.dominating.heinrich-lemma1} can
be obtained using the binomial formula and the combinatorial
interpretation of binomial coefficients. The main idea of this latter
proof is to observe that for each $k \in \set{0,1,\ldots,\abs{P}}$,
exactly $\dbinom{\abs{P}}{k}$ among the subsets of $P$ have size $k$,
and therefore
$\sum_{\substack{A \subseteq P}} \tup{-1}^{\abs{A}}$ can be rewritten
as $\sum_{k=0}^{\abs{P}} \dbinom{\abs{P}}{k} \tup{-1}^k$, which in
turn can be simplified (using the binomial formula) to
$\tup{1-1}^{\abs{P}} = 0^{\abs{P}} = \ive{\abs{P} = 0}
= \ive{P = \varnothing}$. We leave the details of this alternative
proof to the interested reader.

The following exercise demonstrates an application of
Lemma~\ref{lem.dominating.heinrich-lemma1} to (elementary) number
theory:

\begin{exercise} \label{exe.dominating.heinrich-lemma-moeb-NT}
Let $\NN_+$ denote the set $\set{1, 2, 3, \ldots}$. An integer $n$ is
said to be \textit{squarefree} if it is not divisible by any perfect
square apart from $1$. (Note that if $n$ is a positive integer, then
$n$ is squarefree if and only if $n$ is a product of
\textbf{distinct} prime numbers.) We define a map
$\mu : \NN_+ \to \set{-1, 0, 1}$ by setting
\[
\mu\tup{n}
= \begin{cases}
\tup{-1}^{\omega\tup{n}}, & \text{ if } n \text{ is squarefree;} \\
0,                        & \text{ if not }
\end{cases}
\qquad \text{ for each } n \in \NN_+ ,
\]
where $\omega\tup{n}$ denotes the number of distinct prime divisors
of $n$. (For example, $\mu\tup{6} = \tup{-1}^2 = 1$,
$\mu\tup{30} = \tup{-1}^3 = -1$,
$\mu\tup{1} = \tup{-1}^0 = 1$, and $\mu\tup{12} = 0$. The map $\mu$
is called the \textit{(number-theoretical) M\"obius function}.)

Prove that each $n \in \NN_+$ satisfies
\[
\sum_{d\mid n} \mu\tup{d} = \ive{n = 1},
\]
where the sum on the left hand side should be understood as a sum over
all positive divisors of $n$.

[\textbf{Hint:} Apply Lemma~\ref{lem.dominating.heinrich-lemma1} with
$P$ being the set of all prime factors of $n$. Relate the subsets of
$P$ to the squarefree divisors of $n$.]
\end{exercise}

\begin{exercise} \label{exe.dominating.heinrich-lemma-sum}
Let $A$ be a finite set. Let $A_1, A_2, \ldots, A_m$ be some subsets
of $A$. Prove that
\begin{align*}
& \abs{\set{U \subseteq A \ \mid \ U \cap A_i \neq \varnothing
          \text{ for all } i \in \set{1, 2, \ldots, m}}} \\
&\equiv
\abs{\set{J \subseteq \set{1, 2, \ldots, m} \ \mid
          \ \bigcup_{j \in J} A_j = A }}
\mod 2.
\end{align*}

[\textbf{Hint:} Set $I = \set{1, 2, \ldots, m}$. Compute the double
sum
\[
\sum_{U \subseteq A} \sum_{J \subseteq I}
\tup{-1}^{\abs{J}} \tup{-1}^{\abs{U}}
\ive{U \subseteq A \setminus \bigcup_{j \in J} A_j}
\]
in two different ways (once by interchanging the summations, and once
again by rewriting
$\ive{U \subseteq A \setminus \bigcup_{j \in J} A_j}$ as
$\ive{J \subseteq \set{i \in I \mid U \cap A_i = \varnothing}}$).
Notice that powers of $-1$ can be discarded when working
modulo $2$.]
\end{exercise}

\subsection{\label{subsect.dominating.lemmas}Proving
the Heinrich-Tittmann formula}

In this section, we shall finally prove
Theorem~\ref{thm.dominating.heinrich}. Instead of presenting the
proof as a monolithic piece of work, I shall distribute most of it
into a series of easy lemmas (some of which are of independent
interest).

Throughout this
section, we consider a simple graph $G = \tup{V, E}$. The notion of a
``detached pair'' is to be understood as in
Theorem~\ref{thm.dominating.heinrich}.

\begin{lemma} \label{lem.dominating.heinrich-lemma2}
Let $B$ be a subset of $V$. Then,
\[
\sum_{\substack{A \subseteq V; \\ \tup{A, B}
\text{ is a detached pair}}} \tup{-1}^{\abs{A}}
= \ive{B \text{ is dominating}}.
\]
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lem.dominating.heinrich-lemma2}.]
Let $B'$ be the set of all vertices $v \in V \setminus B$ such
that at least one neighbor of $v$ belongs to $B$. Then, for each
subset $A$ of $V$, the following equivalence holds:
\begin{equation}
\left( \tup{A, B} \text{ is a detached pair} \right)
\Longleftrightarrow
\left( A \subseteq V \setminus \tup{B \cup B'} \right)
\label{pf.lem.dominating.heinrich-lemma2.1}
\end{equation}
\footnote{\textit{Proof of
\eqref{pf.lem.dominating.heinrich-lemma2.1}.} We shall prove the
$\Longrightarrow$ and $\Longleftarrow$ directions of the equivalence
\eqref{pf.lem.dominating.heinrich-lemma2.1} separately:

$\Longrightarrow$: Assume that $\tup{A, B}$ is a detached pair. We
must show that $A \subseteq V \setminus \tup{B \cup B'}$.

We know that $\tup{A, B}$ is a detached pair. By the definition of a
``detached pair'', this means that $A$ and $B$ are two disjoint
subsets of $V$ having the property that there exists no edge
$ab \in E$ satisfying $a \in A$ and $b \in B$.

We have $A \subseteq V \setminus B$ (since $A$ and $B$ are
disjoint).

We claim that $A \cap B' = \varnothing$. Indeed, assume the contrary.
Thus, the set $A \cap B'$ is nonempty. Hence, there exists some
$v \in A \cap B'$. Fix such a $v$.
From $v \in A \cap B' \subseteq B'$, we conclude that $v$ is a vertex
in $V \setminus B$ such that at least one neighbor of $v$
belongs to $B$ (by the definition of $B'$). In particular, at least
one neighbor of $v$ belongs to $B$. Let $w$ be such a neighbor. Then,
$vw \in E$ (since $w$ is a neighbor of $v$). Notice also that
$v \in A \cap B' \subseteq A$ and $w \in B$.
Now, recall that there exists no edge $ab \in E$ satisfying $a \in A$
and $b \in B$. This contradicts the fact that $vw$ is such an edge
(since $vw \in E$, $v \in A$ and $w \in B$). This contradiction shows
that our assumption was wrong. Hence, $A \cap B' = \varnothing$.
Thus, $A \subseteq V \setminus B'$.

Combining $A \subseteq V \setminus B$ with
$A \subseteq V \setminus B'$, we find
\[
A \subseteq \tup{V \setminus B} \cap \tup{V \setminus B'}
  = V \setminus \tup{B \cup B'} .
\]
Hence, the $\Longrightarrow$ direction of the equivalence
\eqref{pf.lem.dominating.heinrich-lemma2.1} is proven.

$\Longleftarrow$: Assume that
$A \subseteq V \setminus \tup{B \cup B'}$. We must then show
that $\tup{A, B}$ is a detached pair.

First of all, we have
$A \subseteq V \setminus \tup{B \cup B'}
\subseteq V \setminus B$ (since $B \cup B' \supseteq B$).
Thus, the sets $A$ and $B$ are disjoint.

Next, I claim that there exists no edge
$ab \in E$ satisfying $a \in A$ and $b \in B$. Indeed, assume the
contrary. Thus, there exists an edge $ab \in E$ satisfying $a \in A$
and $b \in B$. Consider such an edge. Then, $b$ is a neighbor of $a$
(since $ab \in E$) and belongs to $B$. Hence, at least one neighbor
of $a$ belongs to $B$ (namely, the neighbor $b$).
The element $a$ is a vertex in
$V \setminus B$ (since $a \in A \subseteq V \setminus B$) such that
at least one neighbor of $a$ belongs to $B$. In other words, $a$
belongs to $B'$ (by the definition of $B'$). Thus, $a \in B'
\subseteq B \cup B'$. But from
$a \in A \subseteq V \setminus \tup{B \cup B'}$, we obtain
$a \notin B \cup B'$. This contradicts $a \in B \cup B'$. This
contradiction shows that our assumption was wrong. Hence, we have
shown that there exists no edge
$ab \in E$ satisfying $a \in A$ and $b \in B$. Since the subsets $A$
and $B$ of $V$ are disjoint, this shows that $\tup{A, B}$ is a
detached pair (by the definition of a ``detached pair''). This proves
the $\Longleftarrow$ direction of the equivalence
\eqref{pf.lem.dominating.heinrich-lemma2.1}.

Hence, both directions of \eqref{pf.lem.dominating.heinrich-lemma2.1}
are proven.}.

On the other hand, the following equivalence holds:
\begin{equation}
\left( V \setminus \tup{B \cup B'} = \varnothing \right)
\Longleftrightarrow
\left( B \text{ is dominating} \right)
\label{pf.lem.dominating.heinrich-lemma2.2}
\end{equation}
\footnote{\textit{Proof of
\eqref{pf.lem.dominating.heinrich-lemma2.2}.}
% We have the following
% chain of equivalences:
% \begin{align*}
% & \left( V \setminus \tup{B \cup B'} = \varnothing \right) \\
% \Longleftrightarrow
% & \left( \tup{V \setminus B} \setminus B' = \varnothing \right)
% \qquad \left( \text{since } V \setminus \tup{B \cup B'} =
                % \tup{V \setminus B} \setminus B' \right) \\
% & \left( V \setminus B \subseteq B' \right) \\
% & \left( \text{for every vertex } v \in V \setminus B
            % \text{, we have } \underbrace{v \in B'}_{\substack{
                % \Longleftrightarrow
                % \left( v \text{ is a vertex
                % \right) \\
% & \left( \text{for every vertex } v \in V \setminus B
            % \text{, we have } \v \in B' \right) 
% \end{align*}
Again, we are going to
separately prove the $\Longrightarrow$ and $\Longleftarrow$ directions
of the equivalence:

$\Longrightarrow$: Assume that
$V \setminus \tup{B \cup B'} = \varnothing$. We must show that $B$
is dominating.

Let $v \in \verts{G} \setminus B$. Thus, $v \in \verts{G}$ and
$v \notin B$. We have $v \in \verts{G} = V \subseteq B \cup B'$
(since $V \setminus \tup{B \cup B'} = \varnothing$). Combined with
$v \notin B$, we obtain $v \in \tup{B \cup B'} \setminus B \subseteq
B'$. According to the definition of $B'$, this means that $v$ is a
vertex in $V \setminus B$ such that at least one neighbor of
$v$ belongs to $B$. In particular, we thus have shown that at least
one neighbor of $v$ belongs to $B$.

Now, forget that we fixed $v$. We thus have proven that for
every vertex $v \in \verts{G} \setminus B$, at least one neighbor of
$v$ belongs to $B$. In other words, the set $B$ is dominating
(because this is how we defined ``dominating sets''). This proves
the $\Longrightarrow$ direction of the equivalence
\eqref{pf.lem.dominating.heinrich-lemma2.2}.

$\Longleftarrow$: Assume that $B$ is dominating. We must prove that
$V \setminus \tup{B \cup B'} = \varnothing$.

We know that $B$ is dominating. In other words, for every vertex
$v \in \verts{G} \setminus B$, at least one neighbor of $v$ belongs to
$B$ (because this is what ``dominating'' means). In other words, for
every $v \in V \setminus B$, at least one neighbor of $v$ belongs to
$B$ (since $\verts{G} = V$).

Now, let $v \in V \setminus B$ be arbitrary. As we have just seen,
we then know that at least one neighbor of $v$ belongs to $B$.

Let $v \in V \setminus B$. Thus, $v$ is a
vertex in $V \setminus B$ such that at least one neighbor of
$v$ belongs to $B$. This means that $v \in B'$ (by the definition of
$B'$).

Now, forget that we fixed $v$. We thus have shown that $v \in B'$ for
each $v \in V \setminus B$. In other words,
$V \setminus B \subseteq B'$. But now,
$ V \setminus \tup{B \cup B'} = \tup{V \setminus B} \setminus B'
= \varnothing$
(since $V \setminus B \subseteq B'$).
Thus, the $\Longleftarrow$ direction of the equivalence
\eqref{pf.lem.dominating.heinrich-lemma2.2} is proven.}.

Now, we can use the equivalence
\eqref{pf.lem.dominating.heinrich-lemma2.1} to rewrite the summation
sign $\sum_{\substack{A \subseteq V; \\ \tup{A, B}
\text{ is a detached pair}}}$ as
$\sum_{\substack{A \subseteq V; \\
A \subseteq V \setminus \tup{B \cup B'}}}$. This latter summation sign
can be further simplified to
$\sum_{A \subseteq V \setminus \tup{B \cup B'}}$ (since every subset
$A$ of $V \setminus \tup{B \cup B'}$ is clearly a subset of $V$ as
well). Hence, we can replace the summation sign
$\sum_{\substack{A \subseteq V; \\ \tup{A, B}
\text{ is a detached pair}}}$ by
$\sum_{A \subseteq V \setminus \tup{B \cup B'}}$. In particular,
\begin{align*}
& \sum_{\substack{A \subseteq V; \\ \tup{A, B}
\text{ is a detached pair}}} \tup{-1}^{\abs{A}} \\
&= \sum_{A \subseteq V \setminus \tup{B \cup B'}} \tup{-1}^{\abs{A}}
\\
&= \ive{ V \setminus \tup{B \cup B'} = \varnothing }
\qquad
\left( \text{by Lemma~\ref{lem.dominating.heinrich-lemma1}, applied
         to } P = V \setminus \tup{B \cup B'} \right) \\
&= \ive{B \text{ is dominating}}
\end{align*}
(by the equivalence \eqref{pf.lem.dominating.heinrich-lemma2.2}).
This proves Lemma~\ref{lem.dominating.heinrich-lemma2}.
\end{proof}

Lemma~\ref{lem.dominating.heinrich-lemma2} has the following
consequence:

\begin{corollary} \label{cor.dominating.heinrich-lemma2c}
Let $B$ be a subset of $V$. Then,
\[
\sum_{\substack{A \subseteq V; \\ A \neq \varnothing; \\
\tup{A, B} \text{ is a detached pair}}}
\tup{-1}^{\abs{A}}
= \ive{B \text{ is dominating}} - 1.
\]
\end{corollary}

\begin{proof}[Proof of Corollary~\ref{cor.dominating.heinrich-lemma2c}.]
The pair $\tup{\varnothing, B}$ is a detached
pair\footnote{\textit{Proof.} The sets
$\varnothing$ and $B$ are two disjoint subsets of $V$
(disjoint because $\varnothing \cap B = \varnothing$)
having the property that there exists no edge
$ab \in E$ satisfying $a \in \varnothing$ and $b \in B$
(this is vacuously true, since there exists no
$a \in \varnothing$). In other words, $\tup{\varnothing, B}$ is
a detached pair (by the definition of a ``detached pair'').}.
Hence, the sum \newline
$\sum_{\substack{A \subseteq V; \\ \tup{A, B}
\text{ is a detached pair}}} \tup{-1}^{\abs{A}}$
has an addend for $A = \varnothing$. If we split off this addend
from this sum, we obtain
\[
\sum_{\substack{A \subseteq V; \\ \tup{A, B}
\text{ is a detached pair}}} \tup{-1}^{\abs{A}}
=
\tup{-1}^{\abs{\varnothing}}
+
\sum_{\substack{A \subseteq V; \\ \tup{A, B}
\text{ is a detached pair}; \\ A \neq \varnothing}}
\tup{-1}^{\abs{A}} .
\]
Hence,
\begin{align*}
\sum_{\substack{A \subseteq V; \\ \tup{A, B}
\text{ is a detached pair}; \\ A \neq \varnothing}}
\tup{-1}^{\abs{A}}
&=
\underbrace{\sum_{\substack{A \subseteq V; \\ \tup{A, B}
             \text{ is a detached pair}}} \tup{-1}^{\abs{A}}}
           _{\substack{= \ive{B \text{ is dominating}} \\
             \text{(by Lemma~\ref{lem.dominating.heinrich-lemma2})}}}
- \underbrace{\tup{-1}^{\abs{\varnothing}}}_{
                =\tup{-1}^0=1} \\
&= \ive{B \text{ is dominating}} - 1.
\end{align*}
Thus,
\begin{align*}
\sum_{\substack{A \subseteq V; \\ A \neq \varnothing; \\
\tup{A, B} \text{ is a detached pair}}}
\tup{-1}^{\abs{A}}
= \sum_{\substack{A \subseteq V; \\ \tup{A, B}
\text{ is a detached pair}; \\ A \neq \varnothing}}
\tup{-1}^{\abs{A}}
= \ive{B \text{ is dominating}} - 1.
\end{align*}
This proves Corollary~\ref{cor.dominating.heinrich-lemma2c}.
\end{proof}

\begin{lemma} \label{lem.dominating.heinrich-lemma3}
Let $A$ and $B$ be two subsets of $V$. Then, $\tup{A, B}$ is a
detached pair if and only if $\tup{B, A}$ is a detached pair.
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lem.dominating.heinrich-lemma3}.]
We have the following chain of logical equivalences:
\begin{align*}
& \left( \tup{B, A} \text{ is a detached pair} \right) \\
\Longleftrightarrow \ 
& \left( B \text{ and } A \text{ are two disjoint subsets of } V
     \text{ having the property that } \right. \\
& \qquad \qquad \left. \text{ there exists no edge } ab \in E
     \text{ satisfying } a \in B \text{ and } b \in A \right) \\
& \qquad \left( \text{by the definition of ``detached pair''} \right)
\\
\Longleftrightarrow \ 
& \left( B \text{ and } A \text{ are two disjoint subsets of } V
     \text{ having the property that } \right. \\
& \qquad \qquad \left. \text{ there exists no edge } ba \in E
     \text{ satisfying } b \in B \text{ and } a \in A \right) \\
& \qquad \left( \text{here, we renamed } a \text{ and } b \text{ as }
            b \text{ and } a\text{, respectively} \right)
\\
\Longleftrightarrow \ 
& \left( A \text{ and } B \text{ are two disjoint subsets of } V
     \text{ having the property that } \right. \\
& \qquad \qquad \left. \text{ there exists no edge } ba \in E
     \text{ satisfying } a \in A \text{ and } b \in B \right) \\
& \qquad \left( \text{since } B \text{ and } A
            \text{ are disjoint if and only if }
            A \text{ and } B \text{ are disjoint} \right)
\\
\Longleftrightarrow \ 
& \left( A \text{ and } B \text{ are two disjoint subsets of } V
     \text{ having the property that } \right. \\
& \qquad \qquad \left. \text{ there exists no edge } ab \in E
     \text{ satisfying } a \in A \text{ and } b \in B \right) \\
& \qquad \left( \text{since } ba = ab \right)
\\
\Longleftrightarrow \ 
& \left( \tup{A, B} \text{ is a detached pair} \right) \\
& \qquad \left( \text{by the definition of ``detached pair''} \right)
.
\end{align*}
This proves Lemma~\ref{lem.dominating.heinrich-lemma3}.
\end{proof}

Next comes another general lemma about cardinalities of sets:

\begin{lemma} \label{lem.dominating.heinrich-lemma-inv}
Let $S$ be a finite set. Let $\sigma : S \to S$ be a map such that
$\sigma \circ \sigma = \id_S$. (Such a map is called an
\textit{involution} on $S$.)

Let $F = \set{ i \in S \mid \sigma\tup{i} = i }$. (The elements of $F$
are known as the \textit{fixed points} of $\sigma$.) Then,
$\abs{F} \equiv \abs{S} \mod 2$.
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lem.dominating.heinrich-lemma-inv}.]
In a nutshell, the proof of
Lemma~\ref{lem.dominating.heinrich-lemma-inv} is very simple: We have
$\sigma \circ \sigma = \id_S$; in other words, we have
$\sigma\tup{\sigma\tup{i}} = i$ for each $i \in S$. Each
$i \in S \setminus F$ satisfies $\sigma\tup{i} \neq i$. Thus, we can
assign to each element $i \in S \setminus F$ the two-element subset
$\set{i, \sigma\tup{i}} \subseteq S$. This assignment has the property
that the two-element subset assigned to $\sigma\tup{i}$ is the same
as the one assigned to $i$ (since
$\set{\sigma\tup{i}, \underbrace{\sigma\tup{\sigma\tup{i}}}_{= i}}
= \set{\sigma\tup{i}, i} = \set{i, \sigma\tup{i}}$).
Thus, each two-element subset
that gets assigned at all is assigned to exactly two elements of
$S \setminus F$ (namely, the subset assigned to $i$ is assigned to
$i$ and $\sigma\tup{i}$, and to no other elements of
$S \setminus F$).
As a consequence, the elements of $S \setminus F$ are paired up
(each pair consisting of two elements to which the same subset is
assigned). Correspondingly, $\abs{S \setminus F}$ is even. Thus,
$\abs{S} - \abs{F} = \abs{S \setminus F}$ is even, so that
$\abs{F} \equiv \abs{S} \mod 2$.

If you found this insufficiently rigorous (or unclear), here is a
\textit{rigorous version of this proof:}
For each $i \in S \setminus F$, we have
$\set{i, \sigma\tup{i}} \in \powset[2]{S}$
\ \ \ \ \footnote{\textit{Proof.} Let $i \in S \setminus F$. Thus,
$i \in S$ and $i \notin F$. If we had $\sigma\tup{i} = i$, then we
would have $i \in F$ (by the definition of $F$), which would
contradict $i \notin F$. Hence, we do not have
$\sigma\tup{i} = i$. Thus, we have $\sigma\tup{i} \neq i$. Hence,
$\set{i, \sigma\tup{i}}$ is a $2$-element set. Since
$\set{i, \sigma\tup{i}} \subseteq S$ (because both $i$ and
$\sigma\tup{i}$ are elements of $S$), this shows that
$\set{i, \sigma\tup{i}}$ is a $2$-element subset of $S$. In other
words, $\set{i, \sigma\tup{i}} \in \powset[2]{S}$, qed.}. In other
words, for each $i \in S \setminus F$, there exists a unique
$K \in \powset[2]{S}$ satisfying $\set{i, \sigma\tup{i}} = K$
(namely, $K = \set{i, \sigma\tup{i}}$).

But
\begin{align}
\abs{S \setminus F}
&= \tup{ \text{the number of } i \in S \setminus F } \nonumber \\
&= \sum_{K \in \powset[2]{S}}
      \tup{ \text{the number of } i \in S \setminus F
              \text{ satisfying } \set{i, \sigma\tup{i}} = K }
\label{pf.lem.dominating.heinrich-lemma-inv.1}
\end{align}
(because for each $i \in S \setminus F$, there exists a unique
$K \in \powset[2]{S}$ satisfying $\set{i, \sigma\tup{i}} = K$).

On the other hand, for each $K \in \powset[2]{S}$, we have
\begin{equation}
\tup{ \text{the number of } i \in S \setminus F
      \text{ satisfying } \set{i, \sigma\tup{i}} = K }
\equiv 0 \mod 2
\label{pf.lem.dominating.heinrich-lemma-inv.2}
\end{equation}
\footnote{\textit{Proof of
\eqref{pf.lem.dominating.heinrich-lemma-inv.2}.}
Let $K \in \powset[2]{S}$. If there exists no $i \in S \setminus F$
satisfying $\set{i, \sigma\tup{i}} = K$, then
\eqref{pf.lem.dominating.heinrich-lemma-inv.2} is true (because
in this case, we have
$\tup{ \text{the number of } i \in S \setminus F
       \text{ satisfying } \set{i, \sigma\tup{i}} = K }
= 0 \equiv 0 \mod 2$). Hence, we WLOG assume that there does exist
some $i \in S \setminus F$ satisfying $\set{i, \sigma\tup{i}} = K$.
Fix such an $i$, and denote it by $j$. Thus, $j$ is an element of
$S \setminus F$ and satisfies $\set{j, \sigma\tup{j}} = K$.

Set $k = \sigma\tup{j}$. Thus,
$\sigma\tup{k} = \sigma\tup{\sigma\tup{j}}
= \underbrace{\tup{\sigma \circ \sigma}}_{= \id_S} \tup{j}
= \id_S \tup{j} = j$
and therefore
$\set{\underbrace{k}_{=\sigma\tup{j}},
     \underbrace{\sigma\tup{k}}_{=j}}
= \set{\sigma\tup{j}, j} = \set{j, \sigma\tup{j}} = K$.
Furthermore, from $j \in S \setminus F$, we obtain $j \in S$ and
$j \notin F$. If we had $\sigma\tup{j} = j$, then we
would have $j \in F$ (by the definition of $F$), which would
contradict $j \notin F$. Hence, we do not have
$\sigma\tup{j} = j$. Thus, we have $\sigma\tup{j} \neq j$. Hence,
$j \neq \sigma\tup{j} = k$. In other words, $j$ and $k$ are distinct.

If $i \in S \setminus F$ satisfies $\set{i, \sigma\tup{i}} = K$, then
$i$ must be either $j$ or $k$ (because
$i \in \set{i, \sigma\tup{i}} = K
= \set{j, \underbrace{\sigma\tup{j}}_{=k}} = \set{j, k}$).

If we had $k \in F$, then we would have $\sigma\tup{k} = k$ (by the
definition of $F$), which would contradict $\sigma\tup{k} = j \neq k$.
Hence, we do not have $k \in F$. Thus, $k \notin F$. Combining this
with $k \in S$, we find $k \in S \setminus F$. Hence, $j$ and $k$ are
two elements of $S \setminus F$. Furthermore, $j$ and $k$ are two
$i \in S \setminus F$ satisfying $\set{i, \sigma\tup{i}} = K$ (since
$j$ is an element of $S \setminus F$ satisfying
$\set{j, \sigma\tup{j}} = K$, and since $k$ is an element of
$S \setminus F$ satisfying $\set{k, \sigma\tup{k}} = K$). Furthermore,
$j$ and $k$ are the \textbf{only} such $i$ (because we have shown that
if $i \in S \setminus F$ satisfies $\set{i, \sigma\tup{i}} = K$, then
$i$ must be either $j$ or $k$). Therefore, we conclude that there are
exactly two $i \in S \setminus F$ that satisfy
$\set{i, \sigma\tup{i}} = K$ (namely, $j$ and $k$) (because $j$ and
$k$ are distinct). Hence,
$\tup{ \text{the number of } i \in S \setminus F
        \text{ satisfying } \set{i, \sigma\tup{i}} = K }
= 2 \equiv 0 \mod 2$. This completes the proof of
\eqref{pf.lem.dominating.heinrich-lemma-inv.2}.}.

Now, \eqref{pf.lem.dominating.heinrich-lemma-inv.1} becomes
\begin{align*}
\abs{S \setminus F}
&= \sum_{K \in \powset[2]{S}}
      \underbrace{\tup{ \text{the number of } i \in S \setminus F
                   \text{ satisfying } \set{i, \sigma\tup{i}} = K }}
                 _{\substack{\equiv 0 \mod 2 \\
                              \text{(by \eqref{pf.lem.dominating.heinrich-lemma-inv.2})}}}
\\
& \equiv \sum_{K \in \powset[2]{S}} 0 = 0 \mod 2 .
\end{align*}
Since $\abs{S \setminus F} = \abs{S} - \abs{F}$ (because
$F \subseteq S$), this rewrites as
$\abs{S} - \abs{F} \equiv 0 \mod 2$. In other words,
$\abs{S} \equiv \abs{F} \mod 2$. This proves
Lemma~\ref{lem.dominating.heinrich-lemma-inv}.
\end{proof}

Here is a particular case of
Lemma~\ref{lem.dominating.heinrich-lemma-inv}:

\begin{corollary} \label{cor.dominating.heinrich-lemma-inv0}
Let $S$ be a finite set. Let $\sigma : S \to S$ be a map such that
$\sigma \circ \sigma = \id_S$. Assume that each $i \in S$
satisfies $\sigma\tup{i} \neq i$. Then,
$\abs{S}$ is even.
\end{corollary}

\begin{proof}[Proof of Corollary~\ref{cor.dominating.heinrich-lemma-inv0}.]
Let $F = \set{ i \in S \mid \sigma\tup{i} = i }$.
Lemma~\ref{lem.dominating.heinrich-lemma-inv} yields
$\abs{F} \equiv \abs{S} \mod 2$.

But each $i \in S$ satisfies $\sigma\tup{i} \neq i$. In other
words, no $i \in S$ satisfies $\sigma\tup{i} = i$. In other
words, $\set{ i \in S \mid \sigma\tup{i} = i } = \varnothing$.
Thus, $F = \set{ i \in S \mid \sigma\tup{i} = i }
= \varnothing$. Hence,
$\abs{F} = \abs{\varnothing} = 0$. But from
$\abs{F} \equiv \abs{S} \mod 2$, we obtain
$\abs{S} \equiv \abs{F} = 0 \mod 2$. In other words, $\abs{S}$
is even. Hence, Corollary~\ref{cor.dominating.heinrich-lemma-inv0}
is proven.
\end{proof}

Now, it is time to get rid of part \textbf{(a)} of
Theorem~\ref{thm.dominating.heinrich}:

\begin{lemma} \label{lem.dominating.heinrich-lemma.a}
Let $\alpha$ and $\beta$ be defined as in
Theorem~\ref{thm.dominating.heinrich}. Then, $\alpha$ and $\beta$ are
even.
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lem.dominating.heinrich-lemma.a}.]
Let us first show that $\beta$ is even.

Indeed, let $S$ be the set of all detached pairs $\tup{A, B}$ for
which both numbers $\abs{A}$ and $\abs{B}$ are odd.
Then, the definition of $\beta$ can be rewritten as
$\beta = \abs{S}$. Clearly, $S$ is a finite set.

For every $\tup{A, B} \in S$, we have
$\tup{B, A} \in S$\ \ \ \ \footnote{\textit{Proof.} Let
$\tup{A, B} \in S$. Thus, $\tup{A, B}$ is a detached pair for which
both numbers $\abs{A}$ and $\abs{B}$ are odd (by the definition of
$S$). Hence, $A$ and $B$ are two subsets of $V$ (since $\tup{A, B}$ is
a detached pair).
But Lemma~\ref{lem.dominating.heinrich-lemma3} shows that $\tup{A, B}$
is a detached pair if and only if $\tup{B, A}$ is a detached pair.
Hence, $\tup{B, A}$ is a detached pair (since $\tup{A, B}$ is a
detached pair). Furthermore, both numbers $\abs{B}$ and $\abs{A}$ are
odd. Thus, $\tup{B, A}$ is a detached pair for which both numbers
$\abs{B}$ and $\abs{A}$ are odd. In other words, $\tup{B, A} \in S$
(by the definition of $S$), qed.}. Hence, we can define a map
$\sigma : S \to S$ by setting
\[
\sigma\tup{A, B} = \tup{B, A}
\qquad \text{for all } \tup{A, B} \in S .
\]
Consider this $\sigma$. It is easy to see that
$\sigma \circ \sigma = \id_S$\ \ \ \ \footnote{\textit{Proof.}
For every $\tup{A, B} \in S$, we have
\begin{align*}
\tup{\sigma \circ \sigma} \tup{A, B}
&= \sigma \tup{\underbrace{\sigma \tup{A, B}}_{= \tup{B, A}}}
= \sigma \tup{B, A} = \tup{A, B}
\end{align*}
(by the definition of $\sigma$). In other words, the map
$\sigma \circ \sigma$ sends each $\tup{A, B} \in S$ to itself. Hence,
$\sigma \circ \sigma = \id_S$.}.

Each $i \in S$ satisfies
$\sigma\tup{i} \neq i$\ \ \ \ \footnote{\text{Proof.}
Let $i \in S$. We must prove that $\sigma\tup{i} \neq i$.

We have $i \in S$.
In other words, $i$ is a detached pair $\tup{A, B}$ for
which both numbers $\abs{A}$ and $\abs{B}$ are odd (by the
definition of $S$). Consider
this $\tup{A, B}$. The number $\abs{B}$ is odd and is a nonnegative
integer. Hence, this number $\abs{B}$ is positive (because any odd
nonnegative integer is positive). Thus, the set
$B$ is nonempty. In other words, $B \neq \varnothing$.

But $\tup{A, B}$ is a detached pair. In other
words, $A$ and $B$ are two disjoint subsets of $V$ having the
property that there exists no edge $ab \in E$ satisfying $a \in A$
and $b \in B$ (by the definition of a ``detached pair'').
Now, $A \cap B = \varnothing$ (since $A$ and $B$ are disjoint).
If we had $A = B$, then we would have
$\underbrace{A}_{=B} \cap B = B \cap B = B \neq \varnothing$,
which would contradict $A \cap B = \varnothing$. Hence, we cannot
have $A = B$. Thus, $A \neq B$.

If we had $\tup{A, B} = \tup{B, A}$, then we would have
$A = B$, which would contradict $A \neq B$. Hence, we cannot have
$\tup{A, B} = \tup{B, A}$. Therefore, $\tup{A, B} \neq
\tup{B, A}$.

But $i = \tup{A, B}$, and thus $\sigma\tup{i} = \sigma\tup{A, B}
= \tup{B, A}$ (by the definition of $\sigma$).
Now, $i = \tup{A, B} \neq \tup{B, A} = \sigma\tup{i}$. In other
words, $\sigma\tup{i} \neq i$, qed.}.
Hence, Corollary~\ref{cor.dominating.heinrich-lemma-inv0}
shows that $\abs{S}$ is even. In other words, $\beta$ is even
(since $\beta = \abs{S}$).

The same argument (with the obvious changes\footnote{For example,
``odd'' has to be replaced by ``even and positive''.}) shows that
$\alpha$ is even. This completes the proof of
Lemma~\ref{lem.dominating.heinrich-lemma.a}.
\end{proof}

We will spend the rest of this section manipulating sums in various
artful ways. These manipulations will culminate in a proof
of Theorem~\ref{thm.dominating.heinrich} \textbf{(b)}; they also
provide examples of techniques that are useful throughout
mathematics.

\begin{lemma} \label{lem.dominating.heinrich-lemma-2sum}
We have
\[
\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing}}
  \tup{\tup{-1}^{\abs{A}} + \tup{-1}^{\abs{B}}}
= 2
\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing}}
  \tup{-1}^{\abs{A}} .
\]
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lem.dominating.heinrich-lemma-2sum}.]
First of all,
\begin{align}
& \sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing}}
  \tup{\tup{-1}^{\abs{A}} + \tup{-1}^{\abs{B}}}
\nonumber \\
& =
\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing}}
  \tup{-1}^{\abs{A}}
+
\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing}}
  \tup{-1}^{\abs{B}}
.
\label{pf.lem.dominating.heinrich-lemma-2sum.1}
\end{align}
Let us now rewrite the second sum on the right hand side of
this equation:
\begin{align*}
& \sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing}}
  \tup{-1}^{\abs{B}} \\
& =
\sum_{\substack{\tup{B, A} \text{ is a detached pair}; \\
                B \neq \varnothing; \  A \neq \varnothing}}
  \tup{-1}^{\abs{A}} \\
& \qquad
\left( \text{here, we have renamed the summation index }
         \tup{A, B} \text{ as } \tup{B, A} \right) \\
& =
\sum_{\substack{\tup{B, A} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing}}
  \tup{-1}^{\abs{A}}
=
\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing}}
  \tup{-1}^{\abs{A}}
\end{align*}
(because the condition
``$\tup{B, A}$ is a detached pair'' under the summation
sign is equivalent to the condition ``$\tup{A, B}$ is a
detached pair'' (by Lemma~\ref{lem.dominating.heinrich-lemma3})).
Hence, \eqref{pf.lem.dominating.heinrich-lemma-2sum.1} becomes
\begin{align*}
&
\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing}}
  \tup{\tup{-1}^{\abs{A}} + \tup{-1}^{\abs{B}}} \\
& =
\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing}}
  \tup{-1}^{\abs{A}}
+
\underbrace{
\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing}}
  \tup{-1}^{\abs{B}}
}_{
=
\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing}}
  \tup{-1}^{\abs{A}}
} \\
& =
\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing}}
  \tup{-1}^{\abs{A}}
+
\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing}}
  \tup{-1}^{\abs{A}} \\
& = 2
\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing}}
  \tup{-1}^{\abs{A}}
.
\end{align*}
This proves
Lemma~\ref{lem.dominating.heinrich-lemma-2sum}.
\end{proof}

\begin{lemma} \label{lem.dominating.heinrich-lemma-2sum2}
Let $n = \abs{V}$. Assume that $n > 0$. Let $\delta$
be the number of dominating sets of $G$. Then,
\[
\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing}}
  \tup{-1}^{\abs{A}}
= \delta - \tup{2^n - 1}.
\]
\end{lemma}

\begin{proof}[Proof of
Lemma~\ref{lem.dominating.heinrich-lemma-2sum2}.]
We first observe that the set $V$ has $2^{\abs{V}}$ subsets. In other
words, the set $V$ has $2^n$ subsets (since $\abs{V} = n$). Thus, the
sum $\sum_{B \subseteq V} 1$ has $2^n$ addends. Since each of these
addends equals $1$, we conclude that this sum equals $2^n \cdot 1
= 2^n$. In other words,
\begin{equation}
\sum_{B \subseteq V} 1 = 2^n .
\label{pf.lem.dominating.heinrich-lemma-2sum2.1}
\end{equation}

On the other hand, each subset $B$ of $V$ is either dominating or not.
Hence,
\begin{align*}
& \sum_{B \subseteq V} \ive{B \text{ is dominating}}  \\
&=
\sum_{\substack{B \subseteq V; \\ B \text{ is dominating}}}
    \underbrace{\ive{B \text{ is dominating}}}_{\substack{
                 = 1 \\ \text{(since } B \text{ is dominating)}}}
+ \sum_{\substack{B \subseteq V; \\ B \text{ is not dominating}}}
    \underbrace{\ive{B \text{ is dominating}}}_{\substack{
                 = 0 \\ \text{(since } B \text{ is not dominating)}}}
 \\
&=
\sum_{\substack{B \subseteq V; \\ B \text{ is dominating}}}
    1
+ \underbrace{
  \sum_{\substack{B \subseteq V; \\ B \text{ is not dominating}}}
    0}_{=0}
=
\sum_{\substack{B \subseteq V; \\ B \text{ is dominating}}} 1 .
\end{align*}

But the number of dominating sets of $G$ is $\delta$. In other words,
the number of dominating subsets $B \subseteq V$ is $\delta$. Hence,
the sum
$\sum_{\substack{B \subseteq V; \\ B \text{ is dominating}}} 1$
has $\delta$ addends. Since each of these addends equals $1$, we thus
see that this sum equals $\delta \cdot 1 = \delta$. In other words,
\[
\sum_{\substack{B \subseteq V; \\ B \text{ is dominating}}} 1
= \delta .
\]
Hence,
\begin{equation}
\sum_{B \subseteq V} \ive{B \text{ is dominating}}
= \sum_{\substack{B \subseteq V; \\ B \text{ is dominating}}} 1
= \delta .
\label{pf.lem.dominating.heinrich-lemma-2sum2.2}
\end{equation}

The subset $\varnothing$ of $V$ is not
dominating\footnote{\textit{Proof.} Assume the contrary. Thus, the
subset $\varnothing$ of $V$ is dominating.

The set $V$ is nonempty (since $\abs{V} = n > 0$). Thus, there exists
some $q \in V$. Consider this $q$. But the set $\varnothing$ is
dominating. In other words, for every vertex
$v \in \verts{G} \setminus \varnothing$, at least one neighbor of
$v$ belongs to $\varnothing$. Applying this to $v = q$, we conclude
that at least one neighbor of $q$ belongs to $\varnothing$ (since
$q \in V = \verts{G} = \verts{G} \setminus \varnothing$). Hence, at
least some object belongs to $\varnothing$. In other words,
$\varnothing$ is nonempty. This is absurd. This contradiction shows
that our assumption is false, qed.}. Hence,
$\ive{\varnothing \text{ is dominating}} = 0$.

Each detached pair $\tup{A, B}$ consists of two subsets $A$ and $B$
of $V$ (by the definition of a ``detached pair''). Hence,
\begin{align}
& \sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing}}
  \tup{-1}^{\abs{A}} \nonumber \\
&=
\sum_{\substack{B \subseteq V; \\ B \neq \varnothing}}
\underbrace{
\sum_{\substack{A \subseteq V; \\ A \neq \varnothing; \\
                \tup{A, B} \text{ is a detached pair}}}
\tup{-1}^{\abs{A}}
}_{\substack{= \ive{B \text{ is dominating}} - 1 \\
        \text{(by Corollary~\ref{cor.dominating.heinrich-lemma2c})}}}
 \nonumber \\
&= \sum_{\substack{B \subseteq V; \\ B \neq \varnothing}}
\tup{\ive{B \text{ is dominating}} - 1} .
\label{pf.lem.dominating.heinrich-lemma-2sum2.3}
\end{align}

But the sum
$\sum_{B \subseteq V} \tup{\ive{B \text{ is dominating}} - 1}$ has an
addend for $B = \varnothing$. If we split off this addend, we find
\begin{align*}
& \sum_{B \subseteq V} \tup{\ive{B \text{ is dominating}} - 1} \\
& = \tup{\underbrace{\ive{\varnothing \text{ is dominating}}}_{=0} - 1}
+ \sum_{\substack{B \subseteq V; \\ B \neq \varnothing}}
\tup{\ive{B \text{ is dominating}} - 1} \\
&= -1
+ \sum_{\substack{B \subseteq V; \\ B \neq \varnothing}}
\tup{\ive{B \text{ is dominating}} - 1} .
\end{align*}
Thus,
\begin{align*}
\sum_{\substack{B \subseteq V; \\ B \neq \varnothing}}
\tup{\ive{B \text{ is dominating}} - 1}
&= \underbrace{
\sum_{B \subseteq V} \tup{\ive{B \text{ is dominating}} - 1}
}_{= \sum_{B \subseteq V} \ive{B \text{ is dominating}}
- \sum_{B \subseteq V} 1}
+ 1 \\
&= \underbrace{\sum_{B \subseteq V} \ive{B \text{ is dominating}}}_{
        \substack{= \delta \\
       \text{(by \eqref{pf.lem.dominating.heinrich-lemma-2sum2.2})}}}
- \underbrace{\sum_{B \subseteq V} 1}_{\substack{= 2^n \\
       \text{(by \eqref{pf.lem.dominating.heinrich-lemma-2sum2.1})}}}
+ 1
= \delta - 2^n + 1.
\end{align*}
Thus, \eqref{pf.lem.dominating.heinrich-lemma-2sum2.3} becomes
\begin{align*}
& \sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing}}
  \tup{-1}^{\abs{A}} \\
&= \sum_{\substack{B \subseteq V; \\ B \neq \varnothing}}
\tup{\ive{B \text{ is dominating}} - 1} = \delta - 2^n + 1
= \delta - \tup{2^n - 1} .
\end{align*}
This proves Lemma~\ref{lem.dominating.heinrich-lemma-2sum2}.
\end{proof}

Let us now finally step to the proof of
Theorem~\ref{thm.dominating.heinrich}:

\begin{proof}[Proof of Theorem~\ref{thm.dominating.heinrich}.]
\textbf{(a)} Theorem~\ref{thm.dominating.heinrich} \textbf{(a)} is
precisely Lemma~\ref{lem.dominating.heinrich-lemma.a}, which has
already been proven.

\textbf{(b)} Let $\delta$ be the number of dominating sets of $G$.

We shall compute the sum
$\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing}}
  \tup{\tup{-1}^{\abs{A}} + \tup{-1}^{\abs{B}}}$
in two different ways, and then compare the results.

The first way relies on
Lemma~\ref{lem.dominating.heinrich-lemma-2sum} and on
Lemma~\ref{lem.dominating.heinrich-lemma-2sum2}:
From Lemma~\ref{lem.dominating.heinrich-lemma-2sum}, we obtain
\begin{align}
\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing}}
  \tup{\tup{-1}^{\abs{A}} + \tup{-1}^{\abs{B}}}
&= 2
\underbrace{
\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing}}
  \tup{-1}^{\abs{A}}
}_{\substack{= \delta - \tup{2^n - 1} \\
        \text{(by Lemma~\ref{lem.dominating.heinrich-lemma-2sum2})}}}
\nonumber \\
&= 2 \tup{\delta - \tup{2^n - 1}} .
\label{pf.thm.dominating.heinrich.1}
\end{align}

Now, let us try another way. We observe that each detached pair
$\tup{A, B}$ satisfies one and only one of the following four
conditions:
\begin{enumerate}
\item The number $\abs{A}$ is even, and the number $\abs{B}$ is even.
\item The number $\abs{A}$ is even, and the number $\abs{B}$ is odd.
\item The number $\abs{A}$ is odd, and the number $\abs{B}$ is even.
\item The number $\abs{A}$ is odd, and the number $\abs{B}$ is odd.
\end{enumerate}
Accordingly, the sum on the left hand side of
\eqref{pf.thm.dominating.heinrich.1} can be split into four smaller
sums:
\begin{align*}
& \sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing}}
  \tup{\tup{-1}^{\abs{A}} + \tup{-1}^{\abs{B}}}
  \\
&= \sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing ; \\
                \abs{A} \text{ is even};
                \ \abs{B} \text{ is even}}}
  \tup{\underbrace{\tup{-1}^{\abs{A}}}_{\substack{= 1 \\
               \text{(since } \abs{A} \text{ is even)}}}
       + \underbrace{\tup{-1}^{\abs{B}}}_{\substack{= 1 \\
               \text{(since } \abs{B} \text{ is even)}}}} \\
& \qquad +  \sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing ; \\
                \abs{A} \text{ is even};
                \ \abs{B} \text{ is odd}}}
  \tup{\underbrace{\tup{-1}^{\abs{A}}}_{\substack{= 1 \\
               \text{(since } \abs{A} \text{ is even)}}}
       + \underbrace{\tup{-1}^{\abs{B}}}_{\substack{= -1 \\
               \text{(since } \abs{B} \text{ is odd)}}}} \\
& \qquad +  \sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing ; \\
                \abs{A} \text{ is odd};
                \ \abs{B} \text{ is even}}}
  \tup{\underbrace{\tup{-1}^{\abs{A}}}_{\substack{= -1 \\
               \text{(since } \abs{A} \text{ is odd)}}}
       + \underbrace{\tup{-1}^{\abs{B}}}_{\substack{= 1 \\
               \text{(since } \abs{B} \text{ is even)}}}} \\
& \qquad +  \sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing ; \\
                \abs{A} \text{ is odd};
                \ \abs{B} \text{ is odd}}}
  \tup{\underbrace{\tup{-1}^{\abs{A}}}_{\substack{= -1 \\
               \text{(since } \abs{A} \text{ is odd)}}}
       + \underbrace{\tup{-1}^{\abs{B}}}_{\substack{= -1 \\
               \text{(since } \abs{B} \text{ is odd)}}}} \\
&= \sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing ; \\
                \abs{A} \text{ is even};
                \ \abs{B} \text{ is even}}}
   \underbrace{\tup{1 + 1}}_{= 2}
+ \sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing ; \\
                \abs{A} \text{ is even};
                \ \abs{B} \text{ is odd}}}
   \underbrace{\tup{1 + \tup{-1}}}_{= 0} \\
&\qquad + \sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing ; \\
                \abs{A} \text{ is odd};
                \ \abs{B} \text{ is even}}}
   \underbrace{\tup{\tup{-1} + 1}}_{= 0}
+ \sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing ; \\
                \abs{A} \text{ is odd};
                \ \abs{B} \text{ is odd}}}
   \underbrace{\tup{\tup{-1} + \tup{-1}}}_{= -2} \\
&= \sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing ; \\
                \abs{A} \text{ is even};
                \ \abs{B} \text{ is even}}}
   2
+ \underbrace{\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing ; \\
                \abs{A} \text{ is even};
                \ \abs{B} \text{ is odd}}}
   0}_{= 0} \\
&\qquad + \underbrace{
   \sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing ; \\
                \abs{A} \text{ is odd};
                \ \abs{B} \text{ is even}}}
   0}_{= 0}
+ \sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing ; \\
                \abs{A} \text{ is odd};
                \ \abs{B} \text{ is odd}}}
   \tup{-2}
\end{align*}
\begin{align}
&= \sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing ; \\
                \abs{A} \text{ is even};
                \ \abs{B} \text{ is even}}}
   2
+ \sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing ; \\
                \abs{A} \text{ is odd};
                \ \abs{B} \text{ is odd}}}
   \tup{-2} .
\label{pf.thm.dominating.heinrich.3}
\end{align}

We shall now analyze the two sums on the right hand side of
\eqref{pf.thm.dominating.heinrich.3} more closely.
Let us begin with the first sum. For any subset $A$ of $V$, the
condition ``$A \neq \varnothing$'' is equivalent to the condition
``$\abs{A}$ is positive'' (because clearly, the finite set $A$ is
distinct from $\varnothing$ if and only if its size $\abs{A}$ is
positive). Likewise, for any subset $B$ of $V$, the condition
``$B \neq \varnothing$'' is equivalent to the condition
``$\abs{B}$ is positive''. Combining the preceding two sentences, we
conclude that the conditions ``$A \neq \varnothing$'' and
``$B \neq \varnothing$'' under the summation sign
$\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing ; \\
                \abs{A} \text{ is even};
                \ \abs{B} \text{ is even}}}$
are equivalent to the conditions ``$\abs{A}$ is positive'' and
``$\abs{B}$ is positive'', respectively. Hence, we can rewrite this
summation sign as
$\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                \abs{A} \text{ is positive} ; 
                \ \abs{B} \text{ is positive} ; \\
                \abs{A} \text{ is even};
                \ \abs{B} \text{ is even}}}$.
Thus,
\begin{align}
\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing ; \\
                \abs{A} \text{ is even};
                \ \abs{B} \text{ is even}}}
   2
&= \sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                \abs{A} \text{ is positive} ; 
                \ \abs{B} \text{ is positive} ; \\
                \abs{A} \text{ is even};
                \ \abs{B} \text{ is even}}} 2
= \sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                \abs{A} \text{ is even and positive} ; \\ 
                \abs{B} \text{ is even and positive}}} 2
\nonumber \\
&= \sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                \text{both numbers } \abs{A} \text{ and } \abs{B}
                \text{ are even and positive}}} 2 .
\label{pf.thm.dominating.heinrich.4a1}
\end{align}
But the sum $\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                \text{both numbers } \abs{A} \text{ and } \abs{B}
                \text{ are even and positive}}} 2$
has exactly $\alpha$ terms (since the number of all detached pairs
$\tup{A, B}$ for which both numbers $\abs{A}$ and $\abs{B}$ are even
and positive is $\alpha$), and thus equals $\alpha \cdot 2$ (since
each of its $\alpha$ terms equals $2$). Thus,
$\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                \text{both numbers } \abs{A} \text{ and } \abs{B}
                \text{ are even and positive}}} 2
= \alpha \cdot 2 = 2 \alpha$. Hence,
\eqref{pf.thm.dominating.heinrich.4a1} becomes
\begin{align}
\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing ; \\
                \abs{A} \text{ is even};
                \ \abs{B} \text{ is even}}}
   2
&= \sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                \text{both numbers } \abs{A} \text{ and } \abs{B}
                \text{ are even and positive}}} 2
= 2 \alpha .
\label{pf.thm.dominating.heinrich.4a12}
\end{align}

Let us now attack the second sum. We have
\begin{align}
\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing ; \\
                \abs{A} \text{ is odd};
                \ \abs{B} \text{ is odd}}}
   \tup{-2}
&= \sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                \text{both numbers } \abs{A} \text{ and } \abs{B}
                \text{ are odd and positive}}} \tup{-2} .
\label{pf.thm.dominating.heinrich.4b1}
\end{align}
(Indeed, this can be proven in the same way as we proved
\eqref{pf.thm.dominating.heinrich.4a1}, except that we must replace
every word ``even'' by ``odd'', and every appearance of $2$ by $-2$.)

But if $\tup{A, B}$ is a detached pair, then the condition
``both numbers $\abs{A}$ and $\abs{B}$ are odd and positive'' is
equivalent to the simpler condition
``both numbers $\abs{A}$ and $\abs{B}$ are
odd''\footnote{\textit{Proof.} Let $\tup{A, B}$ be a detached pair.
Thus, $A$ and $B$ are two subsets of $V$ (by the definition of
``detached pair''), and thus are two finite sets. Hence, $\abs{A}$ and
$\abs{B}$ are nonnegative integers. Therefore, if $\abs{A}$ and
$\abs{B}$ are odd, then $\abs{A}$ and $\abs{B}$ are automatically
positive (because if a nonnegative integer is odd, then it is
automatically positive). Therefore, the condition
``both numbers $\abs{A}$ and $\abs{B}$ are odd and positive'' is
equivalent to the simpler condition
``both numbers $\abs{A}$ and $\abs{B}$ are odd''. Qed.}. Hence, the
summation sign
$\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                \text{both numbers } \abs{A} \text{ and } \abs{B}
                \text{ are odd and positive}}} \tup{-2}$
can be rewritten as
$\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                \text{both numbers } \abs{A} \text{ and } \abs{B}
                \text{ are odd}}} \tup{-2}$.
Thus,
\[
\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                \text{both numbers } \abs{A} \text{ and } \abs{B}
                \text{ are odd and positive}}} \tup{-2}
= \sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                \text{both numbers } \abs{A} \text{ and } \abs{B}
                \text{ are odd}}} \tup{-2} .
\]
But the sum $\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                \text{both numbers } \abs{A} \text{ and } \abs{B}
                \text{ are odd}}} \tup{-2}$
has exactly $\beta$ terms (since the number of all detached pairs
$\tup{A, B}$ for which both numbers $\abs{A}$ and $\abs{B}$ are odd
is $\beta$), and thus equals $\beta \cdot \tup{-2}$ (since
each of its $\beta$ terms equals $-2$). Thus, \newline
$\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                \text{both numbers } \abs{A} \text{ and } \abs{B}
                \text{ are odd}}} \tup{-2}
= \beta \cdot \tup{-2} = -2 \beta$. Hence,
\eqref{pf.thm.dominating.heinrich.4b1} becomes
\begin{align}
\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing ; \\
                \abs{A} \text{ is odd};
                \ \abs{B} \text{ is odd}}}
   \tup{-2}
&= \sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                \text{both numbers } \abs{A} \text{ and } \abs{B}
                \text{ are odd and positive}}} \tup{-2} \nonumber \\
&= \sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                \text{both numbers } \abs{A} \text{ and } \abs{B}
                \text{ are odd}}} \tup{-2}
= -2 \beta .
\label{pf.thm.dominating.heinrich.4b12}
\end{align}

Now, \eqref{pf.thm.dominating.heinrich.3} becomes

\begin{align*}
& \sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing}}
  \tup{\tup{-1}^{\abs{A}} + \tup{-1}^{\abs{B}}}
  \\
&= \underbrace{\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing ; \\
                \abs{A} \text{ is even};
                \ \abs{B} \text{ is even}}}
   2}_{\substack{ = 2 \alpha \\
        \text{(by \eqref{pf.thm.dominating.heinrich.4a12})}}}
+ \underbrace{\sum_{\substack{\tup{A, B} \text{ is a detached pair}; \\
                A \neq \varnothing; \  B \neq \varnothing ; \\
                \abs{A} \text{ is odd};
                \ \abs{B} \text{ is odd}}}
   \tup{-2}}_{\substack{ = - 2 \beta \\
        \text{(by \eqref{pf.thm.dominating.heinrich.4b12})}}} \\
&= 2 \alpha + \tup{-2 \beta} = 2 \tup{\alpha - \beta} .
\end{align*}

Comparing this with \eqref{pf.thm.dominating.heinrich.1}, we find
\[
2 \tup{\delta - \tup{2^n - 1}} = 2 \tup{\alpha - \beta} .
\]
Cancelling $2$ from this equality, we obtain
$\delta - \tup{2^n - 1} = \alpha - \beta$. Solving this equation for
$\delta$, we find $\delta = 2^n - 1 + \alpha - \beta$.

Now, recall that $\delta$ is the number of dominating sets of $G$.
Hence, the number of dominating sets of $G$ is
$\delta = 2^n - 1 + \alpha - \beta$. This proves
Theorem~\ref{thm.dominating.heinrich} \textbf{(b)}.
\end{proof}

\subsection{\label{subsect.dominating.part1}Distances in graphs;
first proof of Proposition~\ref{prop.dominating.|V|/2}}

\begin{todo}
 Write this proof up. Note that it has several steps: First,
 splitting into connected components, and then (WLOG assuming $G$
 to be connected) the actual argument using distances.
\end{todo}

\subsection{\label{subsect.dominating.part2}Independent sets;
second proof of Proposition~\ref{prop.dominating.|V|/2}}

Now, let us prove Proposition~\ref{prop.dominating.|V|/2} using the
concept of \textit{independent sets}. This concept (which is
actually more fundamental than that of dominating sets) is easily
defined:

\begin{definition} \label{def.dominating.indset}
Let $G = \tup{V, E}$ be a simple graph. A subset $S$ of $V$ is
said to be \textit{independent} (with respect to $G$) if and only
if no two distinct elements of $S$ are adjacent.

An \textit{independent set} of $G$ means a subset of $V$ that
is independent (with respect to $G$).
\end{definition}

Independent sets are often called \textit{stable sets}.

If $G = \tup{V, E}$ is a simple graph, then each subset $S$ of
$V$ that has size $\abs{S} \leq 1$ is automatically independent
(since it has no two distinct elements). A subset $S$ of $V$ that
has size $\abs{S} = 2$ is independent if and only if its two
elements are non-adjacent. A subset $S$ of $V$ that has size $3$
is the same as an anti-triangle of $G$, as defined in
Exercise~\ref{exa.simple.R33.two} above.
Here is a concrete example:

\begin{example}
Let $V$, $E$ and $G$ be as in Example~\ref{exa.simple.R33}
\textbf{(c)}. Then, the subset $\set{2, 4, 6}$ of $V$ is
independent (since no two distinct elements of this subset are
adjacent), whereas the subset $\set{1, 3, 5}$ is not
independent (since the two distinct elements $1$ and $3$ of
this subset are adjacent).
\end{example}

\begin{todo}
explain that independent sets are induced subgraphs isomorphic
to the empty graph.
\end{todo}

The following property of independent set is almost trivial:

\begin{lemma} \label{lem.dominating.indset.addv}
Let $G = \tup{V, E}$ be a simple graph. Let $S$ be an independent set
of $G$. Let $v \in V$. Assume that no neighbor of $v$ belongs to $S$.
Then, $S \cup \set{v}$ also is an independent set of $G$.
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lem.dominating.indset.addv}.]
We have $\verts{G} = V$ (since $G = \tup{V, E}$). Furthermore, recall
that $S$ is an independent set of $G$. In other words,
the subset $S$ of $V$ is independent. In other words, no two
distinct elements of $S$ are adjacent (by the definition of
``independent'').

Clearly, $S \cup \set{v}$ is a subset of $V$ (since $S \subseteq V$
and $v \in V$).

Assume (for the sake of contradiction) that some two distinct elements
of $S \cup \set{v}$ are adjacent. Fix two such elements, and denote
them by $p$ and $q$. Hence, $p$ and $q$ are two distinct elements of
$S \cup \set{v}$, and are adjacent.
We have $p \neq q$ (since $p$ and $q$ are distinct). Hence,
at least one of the elements $p$ and $q$ must be distinct from $v$
(because otherwise, both $p$ and $q$ would be equal to $v$, which
would yield that $p = q$, but this would contradict $p \neq q$). In
other words, $p$ is distinct from $v$, or $q$ is distinct from $v$
(or both). Hence, we WLOG assume that $p$ is distinct from $v$ (since
otherwise, we can simply switch $p$ with $q$). Then, $p \neq v$.
Combining this with $p \in S \cup \set{v}$, we obtain
$p \in \tup{S \cup \set{v}} \setminus \set{v} \subseteq S$. Hence,
$q = v$ \ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Thus,
$q \neq v$. Combining this with $q \in S \cup \set{v}$, we obtain
$q \in \tup{S \cup \set{v}} \setminus \set{v} \subseteq S$. Now, $p$
and $q$ are two distinct elements of $S$ (since $p \in S$, $q \in S$
and $p \neq q$), and are adjacent. Hence, there are two distinct
elements of $S$ that are adjacent (namely, $p$ and $q$). This
contradicts the fact that no two distinct elements of $S$ are
adjacent. This contradiction proves that our assumption was wrong,
qed.}. Now, $p$ is a neighbor of $q$ (since $p$ and $q$ are adjacent).
In other words, $p$ is a neighbor of $v$ (since $q = v$). Hence,
at least one neighbor of $v$ belongs to $S$ (namely, the neighbor
$p$). This contradicts the fact that no neighbor of $v$ belongs to
$S$.

This contradiction proves that our assumption (that some two distinct
elements of $S \cup \set{v}$ are adjacent) was false. Hence, no two
distinct elements of $S \cup \set{v}$ are adjacent. In other words,
the subset $S \cup \set{v}$ of $V$ is independent (by the definition
of ``independent''). In other words, $S \cup \set{v}$ is an
independent set of $G$. This proves
Lemma~\ref{lem.dominating.indset.addv}.
\end{proof}

We shall now consider two specific classes of independent sets:
the \textit{maximum} and the \textit{maximal} ones.
These are not the same, and the contrast between them is rather
important to understand. Let us define the two classes:

\begin{definition} \label{def.dominating.indset.max}
Let $G = \tup{V, E}$ be a simple graph.

\textbf{(a)} A \textit{maximum independent set} of $G$ means
a subset of $V$ that is independent and has the highest possible
size among all independent sets. In other words, a
\textit{maximum independent set} of $G$ means a subset $S$ of
$V$ that is independent and that has the property that every
independent subset $T$ of $V$ satisfies $\abs{T} \leq \abs{S}$.

\textbf{(b)} A \textit{maximal independent set} of $G$ means
a subset of $V$ that is independent and that cannot be written
as a proper subset of any independent subset of $V$. In other
words, a \textit{maximal independent set} of $G$ means a subset
of $V$ that is independent and that has the property that no
independent subset $T$ of $V$ satisfies $S \subsetneq T$.
\end{definition}

Thus, roughly speaking, a maximum independent set is an
independent set having maximum size (among the independent sets),
whereas a maximal independent set is an independent set that
cannot be extended to a larger independent set by adding new
elements.

Notice that a simple graph often will have several
maximum independent sets (after all, the requirement to have
maximum size does not uniquely determine the set; it merely
uniquely determines the size), and also several maximal
independent sets.

It is easy to see that each maximum independent set of a
simple graph $G$ is automatically a maximal independent set
of $G$ as well. However, the opposite is not true, as the
following example shows:

\begin{example}
Let $V$, $E$ and $G$ be as in Example~\ref{exa.simple.R33}
\textbf{(c)}. Then, the subset $\set{2, 4, 6}$ of $V$ is
independent, and is a maximum independent set of $G$
(indeed, it is easy to check that each independent set of $G$
has size $\leq 3$). The subset $\set{1, 5}$ is also
independent, and is a \textbf{maximal} independent set of $G$
(indeed, it is easy to check that no independent subset $T$
of $V$ satisfies $\set{1, 5} \subsetneq T$, because each of
the remaining vertices $2, 3, 4, 6$ of $G$ is connected to
at least one of $1$ and $5$ and therefore cannot be in an
independent set together with $1$ and $5$), but not a
\textbf{maximum} independent set of $G$ (since the independent
set $\set{2, 4, 6}$ has greater size).
\end{example}

You might wonder how to compute a maximum independent set
or a maximal independent set of a simple graph $G = \tup{V, E}$.
Obviously, one way to do so is to catalogue all subsets of $V$,
then check which of them are independent, and then check which
of them are maximum independent sets and which are maximal
independent sets. This algorithm, while theoretically doable,
is massively inefficient, since the number of subsets of $V$
grows exponentially with $\abs{V}$. Are there fast algorithms?
This is a more interesting question, and the answers for
\textbf{maximum} and for \textbf{maximal} independent sets
are completely different:
\begin{itemize}
 \item There are, most likely, no quick algorithms to find
       a \textbf{maximum} independent set of a simple graph
       $G = \tup{V, E}$. Indeed, this problem is NP-hard. There
       are better algorithms than checking all subsets of $V$,
       but no algorithm of polynomial time in $\abs{V}$ is
       possible (unless $P = NP$).
 \item Finding a \textbf{maximal} independent set of a simple
       graph $G = \tup{V, E}$ is easy: Start with the empty
       set (which is clearly independent), and keep adding new
       elements to it while keeping the subset
       independent\footnote{It is easy to check whether
       adding a given new vertex
       $v$ to a given independent set $S$ keeps the set
       independent (in fact, we just need to check that $v$
       is not adjacent to any of the elements of $S$).},
       until it is no longer possible.
       Once it is no longer possible, your independent subset
       is a maximal independent set. This is a polynomial-time
       algorithm. We shall formalize this algorithm (and prove its
       correctness) further below (in
       Proposition~\ref{prop.dominating.indset.alg-correct}).
\end{itemize}

Now, let us show a simple yet crucial fact about maximal independent
sets:

\begin{lemma} \label{lem.dominating.indset.max}
Let $G = \tup{V, E}$ be a simple graph. Let $S$ be an independent
subset of $V$. Then, $S$ is a maximal independent set of $G$ if and
only if $S$ is dominating.
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lem.dominating.indset.max}.]
We have $\verts{G} = V$ (since $G = \tup{V, E}$). Furthermore, recall
that the subset $S$ of $V$ is independent. In other words, no two
distinct elements of $S$ are adjacent (by the definition of
``independent'').

Now, let us make the following two claims:

\begin{statement}
\textit{Claim 1:} If $S$ is a maximal independent set of $G$, then
$S$ is dominating.

\textit{Claim 2:} If $S$ is dominating, then $S$ is a maximal
independent set of $G$.
\end{statement}

\textit{Proof of Claim 1:} Assume that $S$ is a maximal independent
set of $G$. We must prove that $S$ is dominating.

We know that $S$ is a maximal independent set of $G$. In other words,
$S$ is a subset of $V$ that is independent and that cannot be written
as a proper subset of any independent subset of $V$.

Let $v \in \verts{G} \setminus S$ be a vertex. Thus, $v \in \verts{G}$
and $v \notin S$. From $S \subseteq V$ and $v \in \verts{G} = V$, we
conclude that $S \cup \set{v}$ is a subset of $V$.
This subset $S \cup \set{v}$ of $V$ is not
independent\footnote{\textit{Proof.} Assume the contrary. Thus, the
subset $S \cup \set{v}$ of $V$ is independent. But $S$ is a proper
subset of $S \cup \set{v}$ (since $v \notin S$). Hence, $S$ can be
written as a proper subset of some independent subset of $V$
(namely, of the independent subset $S \cup \set{v}$). This contradicts
the fact that $S$ cannot be written as a proper subset of any
independent subset of $V$. This contradiction proves that our
assumption was wrong, qed.}. Hence, at least one neighbor of $v$
belongs to $S$\ \ \ \ \footnote{\textit{Proof.} Assume the contrary.
Thus, no neighbor of $v$ belongs to $S$. Hence,
Lemma~\ref{lem.dominating.indset.addv} shows that $S \cup \set{v}$
also is an independent set of $G$ (since $v \in \verts{G} = V$). In
other words, the subset $S \cup \set{v}$ of $V$ is independent.
This contradicts the fact that the subset $S \cup \set{v}$ of $V$ is
not independent. This contradiction proves that our assumption was
wrong, qed.}.

Now, forget that we have fixed $v$. We thus have shown that for
every vertex $v \in \verts{G} \setminus S$, at least one neighbor of
$v$ belongs to $S$. In other words, the subset $S$ of $V$ is
dominating (by the definition of ``dominating''). This proves Claim 1.

\textit{Proof of Claim 2:} Assume that $S$ is dominating. We must
prove that $S$ is a maximal independent set of $G$.

Assume (for the sake of contradiction) that $S$ can be written as a
proper subset of some independent subset of $V$. Let $I$ be this
independent subset. Thus, $S$ is a proper subset of $I$. In
particular, $S \subseteq I$. But $I \setminus S \neq \varnothing$
(since $S$ is a \textbf{proper} subset of $I$). Hence, there exists
some $i \in I \setminus S$. Consider such an $i$. Combining
$i \in I \setminus S \subseteq I \subseteq V = \verts{G}$ with
$i \notin S$ (since $i \in I \setminus S$), we obtain
$i \in \verts{G} \setminus S$.

The subset $I$ of $V$ is independent. In other words, no two distinct
elements of $I$ are adjacent (by the definition of ``independent'').

But the subset $S$ of $V$ is dominating. In other words, for
every vertex $v \in \verts{G} \setminus S$, at least one neighbor of
$v$ belongs to $S$ (by the definition of ``dominating''). Applying
this to $v = i$, we find that at least one neighbor of $i$ belongs to
$S$ (since $i \in \verts{G} \setminus S$). Let $j$ be this neighbor.
Thus, $j \in S$. Also, the vertices $i$ and $j$ are adjacent (since
$j$ is a neighbor of $i$). Hence, the vertices $i$ and $j$ are
distinct. Also, $i$ and $j$ are elements of $I$ (since $i \in I$ and
$j \in S \subseteq I$). Thus, there exist two distinct elements of $I$
that are adjacent (namely, $i$ and $j$). This contradicts the fact
that no two distinct elements of $I$ are adjacent. This contradiction
proves that our assumption (that $S$ can be written as a proper subset
of some independent subset of $V$) is wrong. Hence, $S$ cannot be
written as a proper subset of any independent subset of $V$. Thus,
$S$ is a subset of $V$ that is independent and that cannot be written
as a proper subset of any independent subset of $V$.
In other words, $S$ is a maximal independent set of $G$ (by the
definition of maximal independent sets). This proves Claim 2.

Combining Claim 1 and Claim 2, we conclude that $S$ is a maximal
independent set of $G$ if and only if $S$ is dominating. This proves
Lemma~\ref{lem.dominating.indset.max}.
\end{proof}

Let us now rigorously state the algorithm for finding a maximal
independent set:

\begin{algorithm} \label{alg.dominating.indset}
\textbf{Input:} a simple graph $G = \tup{V, E}$.

\textbf{Output:} a maximal independent set $S$ of $G$.

\begin{enumerate}
\item Define a subset $S$ of $V$ by $S = \varnothing$.
\item \textbf{While} there exists some $v \in V \setminus S$ such that
      no neighbor of $v$ belongs to $S$,
      \textbf{do} the following:
      \begin{itemize}
      \item Choose one such $v$, and add this $v$ to $S$. (Clearly,
      the new $S$ is still a subset of $V$.)
      \end{itemize}
\item Output the subset $S$ of $V$.
\end{enumerate}
\end{algorithm}

(Note that in each iteration of the while-loop in Step 2 of
Algorithm~\ref{alg.dominating.indset}, we only add one $v$ to $S$,
even if there exist several candidates. Then, we go back to the
beginning of the while-loop, and we check again whether there exists
some $v \in V \setminus S$ such that no neighbor of $v$ belongs to
$S$.)

\begin{proposition} \label{prop.dominating.indset.alg-correct}
Algorithm~\ref{alg.dominating.indset} always terminates, and the
subset $S$ that it outputs is indeed a maximal independent set of $G$.
\end{proposition}

We shall show the proof of
Proposition~\ref{prop.dominating.indset.alg-correct} in all detail
due to it being a neat introductory example of reasoning about
algorithms; nevertheless, the proof is almost evident.

\begin{proof}[Proof of
Proposition~\ref{prop.dominating.indset.alg-correct}.]
We will prove the following claims:

\begin{statement}
\textit{Claim 1:} During each iteration of the while-loop in
Algorithm~\ref{alg.dominating.indset}, the size $\abs{S}$ increases
by $1$.

\textit{Claim 2:} Algorithm~\ref{alg.dominating.indset} always
terminates.

\textit{Claim 3:} Consider one single iteration of the while-loop
in Algorithm~\ref{alg.dominating.indset}. If the subset $S$ is
independent before this iteration, then $S$ is also independent after
the iteration.

\textit{Claim 4:} The subset $S$ stays independent throughout
the execution of Algorithm~\ref{alg.dominating.indset}.

\textit{Claim 5:} The subset $S$ that is outputted by
Algorithm~\ref{alg.dominating.indset}
is a maximal independent set of $G$.
\end{statement}

Clearly, Claim 2 and Claim 5 combined will yield
Proposition~\ref{prop.dominating.indset.alg-correct} (once they are
proven). The other three claims are merely auxiliary results.

Let us now prove the five claims:

\textit{Proof of Claim 1.} Consider one
single iteration of the while-loop in
Algorithm~\ref{alg.dominating.indset}. Let $S_{\operatorname{old}}$ be
the value of $S$ at the beginning of this iteration, and let
$S_{\operatorname{new}}$ be the value of $S$ at the end of this
iteration. Thus, $S_{\operatorname{new}}$ is obtained from
$S_{\operatorname{old}}$ by adding the element $v$ that was chosen
during this particular iteration of the while-loop (because what
happens to $S$ during an iteration of the while-loop is that the
element $v$ is being added to $S$). In other words, we have
$S_{\operatorname{new}} = S_{\operatorname{old}} \cup \set{v}$.
But remember how the element $v$ was chosen at the beginning of the
iteration: It was chosen to be an element of $V \setminus S$ such that
no neighbor of $v$ belongs to $S$. Thus, $v$ is an element of
$V \setminus S_{\operatorname{old}}$ such that no neighbor of $v$
belongs to $S_{\operatorname{old}}$. In particular, $v$ is an element
of $V \setminus S_{\operatorname{old}}$. Hence,
$v \notin S_{\operatorname{old}}$. But from
$S_{\operatorname{new}} = S_{\operatorname{old}} \cup \set{v}$, we
obtain
$\abs{S_{\operatorname{new}}}
= \abs{S_{\operatorname{old}} \cup \set{v}}
= \abs{S_{\operatorname{old}}} + 1$ (since
$v \notin S_{\operatorname{old}}$). In other words, the size $\abs{S}$
has increased by $1$ during our iteration.

We thus have shown that during each iteration of the while-loop in
Algorithm~\ref{alg.dominating.indset}, the size $\abs{S}$ increases
by $1$. This proves Claim 1.

\textit{Proof of Claim 2.} Let us show that the while-loop
in Algorithm~\ref{alg.dominating.indset} (more precisely, in its
Step 2) cannot go on forever.

Throughout the execution of the algorithm, $S$ remains a subset of
$V$. Hence, $\abs{S}$ remains a nonnegative integer smaller or equal
to $\abs{V}$ throughout the execution of the algorithm. Therefore,
$\abs{S}$ cannot keep increasing by $1$ forever (since it would
eventually surpass $\abs{V}$ this way). But Claim 1 shows that
$\abs{S}$ increases by $1$ during each iteration of the while-loop in
Algorithm~\ref{alg.dominating.indset}. Therefore, the while-loop in
Algorithm~\ref{alg.dominating.indset} cannot go on forever (since
$\abs{S}$ cannot keep increasing by $1$ forever). Hence,
Algorithm~\ref{alg.dominating.indset} must eventually terminate (since
the only part of Algorithm~\ref{alg.dominating.indset} that could
possibly go on forever is the while-loop). This proves Claim 2.

\textit{Proof of Claim 3.} Let $\mathcal{I}$ be a single iteration of
the while-loop in Algorithm~\ref{alg.dominating.indset}. We must show
that if the subset $S$ is independent before the iteration
$\mathcal{I}$, then $S$ is also independent after the iteration
$\mathcal{I}$.

Let
$S_{\operatorname{old}}$ be the value of $S$ at the beginning of this
iteration $\mathcal{I}$, and let $S_{\operatorname{new}}$ be the value
of $S$ at the end of this iteration $\mathcal{I}$.

Assume that the subset $S_{\operatorname{old}}$ of $V$ is independent.
In other words, $S_{\operatorname{old}}$ is an independent set of $G$.
Now, $S_{\operatorname{new}}$ is obtained from
$S_{\operatorname{old}}$ by adding the element $v$ that was chosen
during this particular iteration of the while-loop (because what
happens to $S$ during an iteration of the while-loop is that the
element $v$ is being added to $S$). In other words, we have
$S_{\operatorname{new}} = S_{\operatorname{old}} \cup \set{v}$.

The element $v$ was chosen to be an element of $V \setminus S$ such
that no neighbor of $v$ belongs to $S$ at the beginning of the
iteration $\mathcal{I}$. Hence, $v$ is an element of
$V \setminus S_{\operatorname{old}}$ such that no neighbor of $v$
belongs to $S_{\operatorname{old}}$. Thus, we have
$v \in V \setminus S_{\operatorname{old}} \subseteq V$, and no
neighbor of $v$ belongs to $S_{\operatorname{old}}$. Hence,
Lemma~\ref{lem.dominating.indset.addv} (applied to
$S_{\operatorname{old}}$ instead of $S$) shows that
$S_{\operatorname{old}} \cup \set{v}$ also is an independent set of
$G$. In other words, $S_{\operatorname{new}}$ is an independent set of
$G$ (since
$S_{\operatorname{new}} = S_{\operatorname{old}} \cup \set{v}$).
In other words, the subset $S_{\operatorname{new}}$ of $V$ is
independent.

Now, forget our assumption that the subset $S_{\operatorname{old}}$ of
$V$ is independent. We thus have shown that if
the subset $S_{\operatorname{old}}$ of $V$ is independent, then
the subset $S_{\operatorname{new}}$ of $V$ is independent. In other
words, if the subset $S$ is independent before the iteration
$\mathcal{I}$, then $S$ is also independent after the iteration
$\mathcal{I}$. This proves Claim 3.

\textit{Proof of Claim 4.} The empty set $\varnothing$ is clearly
independent. Hence, the subset $S$ of $V$ is independent when it is
first defined in Algorithm~\ref{alg.dominating.indset} (because it is
first defined to be the empty set $\varnothing$). Then, throughout the
while-loop in Algorithm~\ref{alg.dominating.indset}, it remains
independent (because of Claim 3). Therefore, $S$ stays independent
throughout Algorithm~\ref{alg.dominating.indset}. This proves Claim 4.

\textit{Proof of Claim 5.} Consider the subset $S$ that is outputted
by Algorithm~\ref{alg.dominating.indset}. This subset $S$ is
independent (by Claim 4), i.e., is an independent set of $G$.

Notice that $G = \tup{V, E}$, hence $\verts{G} = V$. We have
$S \subseteq V = \verts{G}$.

Furthermore, looking back at Algorithm~\ref{alg.dominating.indset}, we
see that the subset $S$ gets outputted immediately after the
while-loop is escaped. Thus, the condition of the while-loop cannot be
satisfied for this subset $S$ (because otherwise, the while-loop would
not be escaped at this point). In other words, there exists
\textbf{no} $v \in V \setminus S$ such that no neighbor of $v$ belongs
to $S$. In other words, for every $v \in V \setminus S$, at least one
neighbor of $v$ belongs to $S$. In other words, for every
$v \in \verts{G} \setminus S$, at least one neighbor of $v$ belongs to
$S$ (because $V = \verts{G}$). In other words, the subset $S$ of
$\verts{G}$ is dominating (by the definition of ``dominating'').
But Lemma~\ref{lem.dominating.indset.max} shows that $S$ is a maximal
independent set of $G$ if and only if $S$ is dominating. Thus, $S$ is
a maximal independent set of $G$ (since $S$ is dominating). This proves
Claim 5.

Now, all five claims are proven.
Proposition~\ref{prop.dominating.indset.alg-correct} follows from
Claim 2 and Claim 5.
\end{proof}

\begin{lemma} \label{lem.dominating.indset.max.compl}
Let $G = \tup{V, E}$ be a simple graph that has no isolated vertices.
Let $S$ be an independent set of $G$.
Then, the subset $V \setminus S$ of $V$ is dominating.
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lem.dominating.indset.max.compl}.]
We have $G = \tup{V, E}$, thus $\verts{G} = V$.
But $S$ is an independent set of $G$, hence an independent subset of
$V$. In other words, no two distinct elements of $S$ are adjacent
(by the definition of ``independent'').

Let $v \in \verts{G} \setminus \tup{V \setminus S}$ be a vertex.
Thus,
$v \in \underbrace{\verts{G}}_{= V} \setminus \tup{V \setminus S}
= V \setminus \tup{V \setminus S} = S$ (since $S \subseteq V$).
Furthermore, the vertex $v$ of $G$ has a
neighbor\footnote{\textit{Proof.} Assume the contrary. Thus, the
vertex $v$ has no neighbors. Hence, the vertex $v$ of $G$ is
isolated (because the vertex $v$ of $G$ is isolated if and only if
$v$ has no neighbors). Therefore, the graph $G$ has at least one
isolated vertex. This contradicts the fact that $G$ has no isolated
vertices. This contradiction proves that our assumption was false,
qed.}. Fix such a neighbor, and denote it by $u$. Thus, $u$ and $v$
are adjacent (since $u$ is a neighbor of $v$).
We have $u \notin S$\ \ \ \ \footnote{\textit{Proof.} Assume the
contrary. Thus, $u \in S$. Now, $u$ and $v$ are two elements
of $S$ (since $u \in S$ and $v \in S$) and are distinct (since $u$ and
$v$ are adjacent). Hence, some two distinct elements of $S$ are
adjacent (namely, $u$ and $v$). This contradicts the fact that no two
distinct elements of $S$ are adjacent. This contradiction proves that
our assumption was false, qed.}. Hence, $u \in V \setminus S$ (since
$u \in V$ and $u \notin S$). Recall also that $u$ is a neighbor of
$v$. Thus, at least one neighbor of $v$ belongs to $V \setminus S$
(namely, the neighbor $u$).

Now, forget that we fixed $v$. We thus have shown that for every
vertex $v \in \verts{G} \setminus \tup{V \setminus S}$, at least one
neighbor of $v$ belongs to $V \setminus S$. In other words, the subset
$V \setminus S$ of $V$ is dominating (by the definition of
``dominating''). This proves
Lemma~\ref{lem.dominating.indset.max.compl}.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop.dominating.|V|/2}.]
\textbf{(a)} There exists a maximal independent set of
$G$\ \ \ \ \footnote{\textit{First proof.} Such a set can be obtained
by Algorithm~\ref{alg.dominating.indset} (because
Proposition~\ref{prop.dominating.indset.alg-correct} shows that this
algorithm outputs a maximal independent set of $G$).

\textit{Second proof.} We have the following two observations:
\begin{itemize}
\item There exists at least one independent set of $G$.
(In fact, the subset $\varnothing$ of $V$ is clearly independent.)
\item There are only finitely many independent sets of $G$. (In fact,
each independent set of $G$ is a subset of $V$, and there are only
finitely many subsets of $V$.)
\end{itemize}
From these two observations, we can conclude that there exists an
independent set of $G$ having maximum size. In other words, there
exists a maximum independent set of $G$. But since each maximum
independent set of $G$ is a maximal independent set of $G$ (this is
easy to show), we can thus conclude that there exists a maximal
independent set of $G$.}. Consider such a set, and denote it by $S$.
Lemma~\ref{lem.dominating.indset.max} shows that $S$ is a maximal
independent set of $G$ if and only if $S$ is dominating. Thus,
$S$ is dominating (since $S$ is a maximal
independent set of $G$).
But Lemma~\ref{lem.dominating.indset.max.compl} shows that
the subset $V \setminus S$ of $V$ is dominating. Clearly, the sets
$S$ and $V \setminus S$ are disjoint subsets of $V$. Furthermore,
$S \cup \tup{V \setminus S} = V$ (since $S \subseteq V$). Hence,
there exist two disjoint dominating subsets $A$ and $B$
of $V$ such that $A \cup B = V$ (namely, $A = S$ and
$B = V \setminus S$). This proves
Proposition~\ref{prop.dominating.|V|/2} \textbf{(a)}.

\textbf{(b)} Proposition~\ref{prop.dominating.|V|/2} \textbf{(a)}
shows that there exist two disjoint dominating subsets $A$ and $B$
of $V$ such that $A \cup B = V$. Consider such $A$ and $B$.
We WLOG assume that $\abs{A} \geq \abs{B}$ (since otherwise, we can
simply switch $A$ with $B$). But since the sets $A$ and $B$ are
disjoint, we have $\abs{A \cup B} = \abs{A} + \abs{B}$. Now,
from $V = A \cup B$, we obtain
\[
\abs{V} = \abs{A \cup B}
= \underbrace{\abs{A}}_{\geq \abs{B}} + \abs{B}
\geq \abs{B} + \abs{B}
= 2 \abs{B} ,
\]
so that $\abs{B} \leq \abs{V} / 2$. In other words, the subset $B$ of
$V$ has size $\leq \abs{V} / 2$. Hence, there exists a dominating
subset of $V$ having size $\leq \abs{V}/2$ (namely, the subset $B$).
This proves Proposition~\ref{prop.dominating.|V|/2} \textbf{(b)}.
\end{proof}

\section{\label{sect.hamilton}Hamiltonian paths}

\begin{todo}
Continue from here...
\end{todo}

[...]

[to be continued]

\begin{thebibliography}{9999999999}                                                                                       %

\bibitem[Aigner95]{Aigner95}Martin Aigner,
\textit{Tur\'an's Graph Theorem}, American Mathematical Monthly
\textbf{102}, No. 9 (Nov., 1995), pp. 808--816. \newline
\url{http://www.math.caltech.edu/~2014-15/1term/ma121a/Aigner - Turans Graph Theorem.pdf}

\bibitem[AigZie]{AigZie}Martin Aigner, G\"{u}nter M. Ziegler,
\textit{Proofs from the Book}, 4th edition, Springer 2010.

\bibitem[AoPS-ISL]{AoPS-ISL}Art of Problem Solving (forum),
\textit{IMO Shortlist} (collection of threads),
\newline
\url{http://www.artofproblemsolving.com/community/c3223_imo_shortlist}

% \bibitem[Artin10]{Artin10}Michael Artin, \textit{Algebra}, 2nd edition,
% Pearson 2010.

\bibitem[Bahran15]{Bahran15}Cihan Bahran,
\textit{Solutions to Math 5707 Spring 2015 homework}.
\newline \url{http://www-users.math.umn.edu/~bahra004/5707.html}

\bibitem[Balakr97]{Balakr97}V. K. Balakrishnan,
\textit{Schaum's Outline of Theory and Problems of Graph Theory},
McGraw-Hill 1997.

% \bibitem[BarSch73]{BarSch73}Hans Schneider, George Phillip Barker,
% \textit{Matrices and Linear Algebra}, 2nd edition, Dover 1973.

\bibitem[BehCha71]{BehCha71}Mehdi Behzad, Gary Chartrand,
\textit{Introduction to the Theory of Graphs},
Allyn \& Bacon, 1971.

\bibitem[BenWil12]{BenWil12}Edward A. Bender and S. Gill Williamson,
\textit{Foundations of Combinatorics with Applications}.
\newline \url{http://cseweb.ucsd.edu/~gill/FoundCombSite/}

\bibitem[BeChZh15]{BeChZh15}Arthur Benjamin, Gary Chartrand,
Ping Zhang,
\textit{The fascinating world of graph theory},
Princeton University Press 2015.

\bibitem[Berge91]{Berge91}Claude Berge,
\textit{Graphs}, 3rd edition, North-Holland 1991.

\bibitem[Bogomoln]{cut-the-knot}Alexander Bogomolny,
\textit{Cut the Knot} (website devoted to educational applets on
various mathematical subjects),
\url{http://www.cut-the-knot.org/Curriculum/index.shtml#combinatorics} .

\bibitem[Bollob79]{Bollob79}B\'ela Bollob\'as,
\textit{Graph Theory: An Introductory Course},
Graduate Texts in Mathematics \#63, 1st edition, Springer 1971.

\bibitem[Bollob98]{Bollob98}B\'ela Bollob\'as,
\textit{Modern Graph Theory},
Graduate Texts in Mathematics \#184, 1st edition, Springer 1998.

\bibitem[Bona11]{Bona11}
Mikl\'os B\'ona,
\textit{A Walk Through Combinatorics:
An Introduction to Enumeration and Graph Theory},
3rd edition, World Scientific 2011.

\bibitem[BonMur76]{BonMur76}
J. A. Bondy and U. S. R. Murty, \textit{Graph theory with Applications},
North-Holland 1976.
\newline \url{https://www.iro.umontreal.ca/~hahn/IFT3545/GTWA.pdf}.

\bibitem[BonMur08]{BonMur08}
J. A. Bondy and U. S. R. Murty, \textit{Graph theory}, Graduate Texts
in Mathematics \#244, Springer 2008.

\bibitem[BonTho77]{BonTho77}
J. A. Bondy, C. Thomassen,
\textit{A short proof of Meyniel's theorem},
Discrete Mathematics 19 (1977), pp. 195--197.
\newline \url{https://doi.org/10.1016/0012-365X(77)90034-6}

\bibitem[Brouwe09]{Brouwe09}
Andries E. Brouwer,
\textit{The number of dominating sets of a finite graph is odd},
\url{http://www.win.tue.nl/~aeb/preprints/domin2.pdf} .

% \bibitem[Camero08]{Camero08}Peter J. Cameron, \textit{Notes on Linear
% Algebra}, version 5 Sep 2008.\newline\url{http://www.maths.qmul.ac.uk/~pjc/notes/linalg.pdf}

\bibitem[ChaLes15]{ChaLes15}
Gary Chartrand, Linda Lesniak, Ping Zhang,
\textit{Graphs \& Digraphs}, 6th edition, CRC Press 2016.

\bibitem[Choo16]{Choo16}David Choo,
\textit{4 proofs to Mantel's Theorem},
\url{http://davinchoo.com/post/mantel/} .

\bibitem[Conrad]{Conrad-sign}Keith Conrad, \textit{Sign of permutations},
\newline\url{http://www.math.uconn.edu/~kconrad/blurbs/grouptheory/sign.pdf} .

\bibitem[Day16]{Day-proofs}Martin V. Day,
\textit{An Introduction to Proofs and the Mathematical Vernacular},
7 December 2016.
\newline\url{https://www.math.vt.edu/people/day/ProofsBook/IPaMV.pdf} .

% \bibitem[deBoor]{deBoor}Carl de Boor, \textit{An empty exercise}.
% \url{ftp://ftp.cs.wisc.edu/Approx/empty.pdf} .

\bibitem[Dieste16]{Dieste16}Reinhard Diestel, \textit{Graph Theory},
Graduate Texts in Mathematics \#173, 5th edition, Springer 2016.
\newline \url{http://diestel-graph-theory.com/basic.html} .

\bibitem[FriFri98]{FriFri98}
Rudolf Fritsch, Gerda Fritsch,
\textit{The Four-Color Theorem},
Springer 1998.

\bibitem[Gessel79]{Gessel-Vand}Ira Gessel, \textit{Tournaments and
Vandermonde's Determinant}, Journal of Graph Theory, Vol. 3 (1979), pp. 305--307.

\bibitem[GrRoSp90]{GrRoSp90}Ronald L. Graham, Bruce L. Rothschild,
Joel H. Spencer, \textit{Ramsey Theory}, 2nd edition,
Wiley 1990.

\bibitem[Griffi15]{Griffi15}Christopher Griffin,
\textit{Graph Theory: Penn State Math 485 Lecture Notes},
version 1.4.3 (9 Dec 2017),
\newline\url{http://www.personal.psu.edu/cxg286/Math485.pdf} .

\bibitem[Grinbe16]{detnotes}Darij Grinberg, \textit{Notes on the combinatorial
fundamentals of algebra}, 10 January 2019.
\newline\url{http://www.cip.ifi.lmu.de/~grinberg/primes2015/sols.pdf}

\bibitem[Guicha16]{Guicha16}David Guichard,
\textit{An Introduction to Combinatorics and Graph Theory},
\newline\url{https://www.whitman.edu/mathematics/cgt_online/cgt.pdf} .

\bibitem[Hammac15]{Hammac15}
Richard Hammack,
\textit{Book of Proof},
3rd edition, Ingram 2018.
\newline\url{http://www.people.vcu.edu/~rhammack/BookOfProof/}

\bibitem[Harary69]{Harary69}
Frank Harary,
\textit{Graph theory},
Addison-Wesley 1969.
\newline\url{http://www.dtic.mil/dtic/tr/fulltext/u2/705364.pdf}

\bibitem[HarPal73]{HarPal73}
Frank Harary, Edgar M. Palmer,
\textit{Graphical enumeration},
Academic Press 1973.

\bibitem[Harju14]{Harju14}
Tero Harju,
\textit{Lecture notes on Graph Theory},
24 April 2014.
\newline\url{http://users.utu.fi/harju/graphtheory/graphtheory.pdf}

\bibitem[HaHiMo08]{HaHiMo08}
John M. Harris, Jeffry L. Hirst, Michael J. Mossinghoff,
\textit{Combinatorics and Graph Theory}, Undergraduate Texts in
Mathematics, Springer 2008.

% \bibitem[Heffer16]{Heffer16}Jim Hefferon, \textit{Linear Algebra},
% 2016.\newline\url{http://joshua.smcvt.edu/linearalgebra/}

\bibitem[HeiTit17]{HeiTit17}
Irene Heinrich, Peter Tittmann,
\textit{Counting Dominating Sets of Graphs},
\arxiv{1701.03453v1}.

\bibitem[Jukna11]{Jukna11}Stasys Jukna,
\textit{Extremal Combinatorics}, 2nd edition, Springer 2011.
An early draft is available at
\url{http://www.thi.informatik.uni-frankfurt.de/~jukna/EC_Book_2nd/} .

\bibitem[Jungni13]{Jungni13}
Dieter Jungnickel,
\textit{Graphs, Networks and Algorithms},
4th edition, Springer 2013.

% \bibitem[OlvSha06]{OlvSha06}Peter J. Olver, Chehrzad Shakiban, \textit{Applied
% Linear Algebra}, Prentice Hall, 2006.\newline See also
% \url{http://www.math.umn.edu/~olver/ala.html} for corrections.

\bibitem[KelTro15]{KelTro15}Mitchel T. Keller, William T. Trotter,
\textit{Applied Combinatorics},
version 26 May 2015.
\newline \url{https://people.math.gatech.edu/~trotter/book.pdf}

\bibitem[Klarre17]{Klarre17}Erica Klarreich,
\textit{Complexity Theory Problem Strikes Back},
Quanta Magazine, 5 January 2017.
\newline \url{https://www.quantamagazine.org/20170105-graph-isomorphism-retraction/}

\bibitem[Knuth97]{Knuth-TAoCP1}
Donald E. Knuth,
\textit{The Art of Computer Programming, Volume 1: Fundamental
Algorithms},
3rd edition, Addison-Wesley 1997.

\bibitem[Knuth95]{Knuth95} Donald E. Knuth,
\textit{Overlapping Pfaffians},
Electron. J. Combin. 3 (1996), no. 2, \#R5.
Also available as arXiv preprint \arxiv{math/9503234v1}.

% \bibitem[Kowals16]{Kowals16}Emmanuel Kowalski, \textit{Linear Algebra},
% version 15 Sep 2016.\newline\url{https://people.math.ethz.ch/~kowalski/script-la.pdf}

\bibitem[LaNaSc16]{LaNaSc16}Isaiah Lankham, Bruno Nachtergaele, Anne
Schilling, \textit{Linear Algebra As an Introduction to Abstract Mathematics},
2016.\newline\url{https://www.math.ucdavis.edu/~anne/linear_algebra/mat67_course_notes.pdf}

\bibitem[LeLeMe16]{LeLeMe16}
Eric Lehman, F. Thomson Leighton, Albert R. Meyer,
\textit{Mathematics for Computer Science},
revised  Wednesday 25th May, 2017,
\newline\url{https://courses.csail.mit.edu/6.042/spring17/mcs.pdf} .

\bibitem[LoPeVe03]{LoPeVe03}L\'aszl\'o Lov\'asz, J\'ozsef Pelik\'an,
Katalin Vesztergombi,
\textit{Discrete Mathematics: Elementary and Beyound},
Springer 2003.

\bibitem[Martin16]{Martin16}
Jeremy L. Martin,
\textit{Math 725, Spring 2016 Lecture Notes},
\newline\url{https://www.math.ku.edu/~jmartin/courses/math725-S16/}

% \bibitem[m.se709196]{m.se709196}Daniela Diaz and others, \textit{Definition of
% General Associativity for binary operations}, math.stackexchange question
% \#709196 .\newline\url{http://math.stackexchange.com/q/709196}

\bibitem[Oggier14]{Oggier14}
Fr\'ed\'erique Oggier,
\textit{Discrete Mathematics},
21 November 2014.
\newline\url{http://www1.spms.ntu.edu.sg/~frederique/DiscreteMathws.pdf}

\bibitem[Ore74]{Ore74}
\O ystein Ore, \textit{Theory of graphs},
AMS Colloquium Publications \#XXXVIII,
4th printing, AMS 1974.

\bibitem[Ore90]{Ore90}
\O ystein Ore, \textit{Graphs and their uses}, revised and updated by
Robin J. Wilson,
Anneli Lax New Mathematical Library \#34, MAA 1990.

\bibitem[Overbe74]{Overbe74}Maria Overbeck-Larisch,
\textit{Hamiltonian paths in oriented graphs},
Journal of Combinatorial Theory, Series B,
Volume 21, Issue 1, August 1976, pp. 76--80.
\newline \url{https://doi.org/10.1016/0095-8956(76)90030-7}

\bibitem[Petrov15]{Petrov15}Fedor Petrov,
\textit{mathoverflow post \#198679 (Flooding a cycle digraph via
chip-firing: $n^{k-1} + n^{k-2} + \cdots + 1$ bound (a Norway 1998-99
problem generalized))},
MathOverflow,
\newline \url{http://mathoverflow.net/q/198679}

\bibitem[PoTaWo83]{PoTaWo83}George P\'olya, Robert E. Tarjan,
Donald R. Woods,
\textit{Notes on Introductory Combinatorics},
Birkh\"auser 1983.
\newline See
\url{http://i.stanford.edu/pub/cstr/reports/cs/tr/79/732/CS-TR-79-732.pdf}
for a preliminary version.

\bibitem[Pretzel]{Pretzel}Oliver Pretzel,
\textit{On reorienting graphs by pushing down maximal vertices},
Order, 1986, Volume 3, Issue 2, pp. 135--153.

\bibitem[Radzis21]{Radzis21}
Stanis\l aw Radziszowski,
\textit{Small Ramsey Numbers},
The Electronic Journal of Combinatorics,
Dynamic Surveys \#21. \newline
\url{https://doi.org/10.37236/21}

\bibitem[RaWiRa]{RaWi-Ramsey}RationalWiki,
\textit{Ramsey theory},
\url{http://rationalwiki.org/wiki/Ramsey_theory} .

\bibitem[RotSot92]{RotSot92}Alvin E. Roth, Marilda A. Oliveira
Sotomayor, \textit{Two-sided matching: A study in game-theoretic
modeling and analysis}, Cambridge University Press 1992.

\bibitem[Ruohon13]{Ruohon13}Keijo Ruohonen, \textit{Graph theory},
2013.\newline
\url{https://web.archive.org/web/20200205235637/http://math.tut.fi/~ruohonen/GT_English.pdf} .

\bibitem[Stanle12]{Stanley-EC1}Richard P. Stanley, \textit{Enumerative
Combinatorics, Volume 1}, 2nd edition, CUP 2012.\newline See
\url{http://math.mit.edu/~rstan/ec/ec1/} for a preliminary version.

\bibitem[Stanle13]{Stanley13}
Richard P. Stanley,
\textit{Algebraic Combinatorics:
Walks, Trees, Tableaux, and More},
Springer 2013.
\newline See \url{http://www-math.mit.edu/~rstan/algcomb/} for errata
and a downloadable draft of the book.

\bibitem[Strick13]{Strick13}
Neil P. Strickland,
\textit{MAS201 Linear Mathematics for Applications}.
\newline \url{https://neil-strickland.staff.shef.ac.uk/courses/MAS201/}

\bibitem[ThuSwa92]{ThuSwa92}
K. Thulasiraman, M. N. S. Swamy,
\textit{Graphs: Theory and Algorithms},
Wiley 1992.

\bibitem[West01]{West01}Douglas B. West,
\textit{Introduction to Graph Theory}, 2nd edition, Pearson 2001.
\newline See \url{http://www.math.illinois.edu/~dwest/igt/}
for updates and corrections.

\bibitem[Wilson96]{Wilson96}Robin J. Wilson,
\textit{Introduction to Graph Theory}, 4th edition,
Addison Wesley 1996. \newline
See \url{https://archive.org/details/IntroductionToGraphTheory} for
the 3rd edition.

\end{thebibliography}


\end{document}